[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Master Machine Learning with scikit-learn",
    "section": "",
    "text": "About this book\nThis is a practical guide to help you transition from Machine Learning novice to skilled Machine Learning practitioner.\nThroughout the book, you‚Äôll learn the best practices for proper Machine Learning and how to apply those practices to your own Machine Learning problems.\nBy the end of this book, you‚Äôll be more confident when tackling new Machine Learning problems because you‚Äôll understand what steps you need to take, why you need to take them, and how to correctly execute those steps using scikit-learn.\nYou‚Äôll know what problems you might run into, and you‚Äôll know exactly how to solve them.\nBecause you‚Äôre learning a better way to work in scikit-learn, your code will be easier to write and to read, and you‚Äôll get better Machine Learning results faster than before!",
    "crumbs": [
      "About this book"
    ]
  },
  {
    "objectID": "index.html#about-the-author",
    "href": "index.html#about-the-author",
    "title": "Master Machine Learning with scikit-learn",
    "section": "About the author",
    "text": "About the author\nHi! My name is Kevin Markham. I‚Äôm the founder of Data School, an online school for learning Data Science with Python.\nI‚Äôve been teaching Machine Learning in the classroom and online for more than 10 years, and I‚Äôm passionate about teaching people who are new to the field, regardless of their educational and professional backgrounds.\nThis book was adapted from my 7.5-hour video course of the same name, and is a compilation of nearly everything I know about effective Machine Learning with scikit-learn!\nPersonally, I have a degree in Computer Engineering from Vanderbilt University and I live in Asheville, North Carolina.",
    "crumbs": [
      "About this book"
    ]
  },
  {
    "objectID": "index.html#prerequisite-skills",
    "href": "index.html#prerequisite-skills",
    "title": "Master Machine Learning with scikit-learn",
    "section": "Prerequisite skills",
    "text": "Prerequisite skills\nThis is an intermediate-level book about scikit-learn, though it also includes many advanced topics.\nYou‚Äôre ready for this book if you can use scikit-learn to solve simple classification or regression problems, including loading a dataset, defining the features and target, training and evaluating a model, and making predictions with new data.\nIf you‚Äôre brand new to scikit-learn, I recommend first taking my free introductory course, Introduction to Machine Learning with scikit-learn. Once you‚Äôve completed lessons 1 through 7, you‚Äôll know the Machine Learning and scikit-learn fundamentals that you‚Äôll need for this book.\nIf you‚Äôve used scikit-learn before but you just need a refresher, there‚Äôs no need to take my introductory course because I‚Äôll be reviewing the Machine Learning workflow in chapter 2.",
    "crumbs": [
      "About this book"
    ]
  },
  {
    "objectID": "index.html#book-outline",
    "href": "index.html#book-outline",
    "title": "Master Machine Learning with scikit-learn",
    "section": "Book outline",
    "text": "Book outline\nIn chapter 1, I‚Äôll give you an overview of the book and help you to get set up. Then in chapter 2, we‚Äôll move on to a review of the Machine Learning workflow in order to establish a foundation for the rest of the book.\nIn chapters 3 through 9, we‚Äôll explore how to handle common issues such as categorical features, text data, and missing values, and also how to integrate those steps into an efficient workflow. Then in chapter 10, we‚Äôll cover how to properly evaluate and tune your entire workflow for maximum performance.\nIn chapters 11 through 16, we‚Äôll walk through a variety of advanced techniques that can help to further improve your model‚Äôs performance, including ensembling, feature selection, feature standardization, and feature engineering. In chapters 17 through 19, we‚Äôll dive deep into two common issues you‚Äôll run into during real-world Machine Learning, namely high-cardinality categorical features and class imbalance.\nFinally, in chapter 20, I‚Äôll end the book with my advice for how you can continue to make progress with your Machine Learning education and skill development!",
    "crumbs": [
      "About this book"
    ]
  },
  {
    "objectID": "index.html#topics-covered",
    "href": "index.html#topics-covered",
    "title": "Master Machine Learning with scikit-learn",
    "section": "Topics covered",
    "text": "Topics covered\n\nReview of the basic Machine Learning workflow\nEncoding categorical features\nEncoding text data\nHandling missing values\nPreparing complex datasets\nCreating an efficient workflow for preprocessing and model building\nTuning your workflow for maximum performance\nAvoiding data leakage\nProper model evaluation\nAutomatic feature selection\nFeature standardization\nFeature engineering using custom transformers\nLinear and non-linear models\nModel ensembling\nModel persistence\nHandling high-cardinality categorical features\nHandling class imbalance",
    "crumbs": [
      "About this book"
    ]
  },
  {
    "objectID": "index.html#functions-and-classes-covered",
    "href": "index.html#functions-and-classes-covered",
    "title": "Master Machine Learning with scikit-learn",
    "section": "Functions and classes covered",
    "text": "Functions and classes covered\n\nWorkflow composition: Pipeline, ColumnTransformer, make_pipeline, make_column_transformer, make_column_selector, make_union\nCategorical encoding: OneHotEncoder, OrdinalEncoder\nNumerical encoding: KBinsDiscretizer\nText encoding: CountVectorizer\nMissing value imputation: SimpleImputer, KNNImputer, IterativeImputer, MissingIndicator\nModel building: LogisticRegression, RandomForestClassifier, ExtraTreesClassifier\nModel ensembling: VotingClassifier\nModel selection: StratifiedKFold, cross_val_score, train_test_split\nModel evaluation: accuracy_score, classification_report, confusion_matrix, roc_auc_score, average_precision_score, plot_confusion_matrix, plot_roc_curve, plot_precision_recall_curve\nHyperparameter tuning: GridSearchCV, RandomizedSearchCV\nFeature selection: RFE, SelectPercentile, SelectFromModel, chi2\nFeature standardization: StandardScaler, MaxAbsScaler\nFeature engineering: FunctionTransformer, PolynomialFeatures\nConfiguration: set_config\nModel persistence: joblib, pickle, cloudpickle (these are external libraries)",
    "crumbs": [
      "About this book"
    ]
  },
  {
    "objectID": "index.html#how-to-support-this-book",
    "href": "index.html#how-to-support-this-book",
    "title": "Master Machine Learning with scikit-learn",
    "section": "How to support this book",
    "text": "How to support this book\nIf you appreciate reading this book for free and want to give back, here are a few options for how you can help:\n\nPurchase a physical or digital copy of the book (coming soon!)\nPurchase one of my online courses\nTell a friend about the book\nShare the book on social media\n\nThank you so much! üôè",
    "crumbs": [
      "About this book"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Master Machine Learning with scikit-learn",
    "section": "License",
    "text": "License\nThe content of this book is protected by copyright, and may not be copied or reproduced without the author‚Äôs permission.",
    "crumbs": [
      "About this book"
    ]
  },
  {
    "objectID": "ch01.html",
    "href": "ch01.html",
    "title": "1¬† Introduction",
    "section": "",
    "text": "1.1 Book overview\nHello and welcome to Master Machine Learning with scikit-learn. In this book, you‚Äôre going to learn how to build an effective Machine Learning workflow using the latest scikit-learn techniques so that you can solve almost any supervised Machine Learning problem.\nIf you have just a bit of experience with scikit-learn, you‚Äôve probably spent most of your time building models using artificially clean training data. But in this book, you‚Äôll learn how to build models using more complex datasets. We‚Äôll cover:\n‚Ä¶and so much more!\nThroughout the book, you‚Äôll learn the best practices for proper Machine Learning and how to apply those practices to your own Machine Learning problems.\nBy the end of this book, you‚Äôll be more confident when tackling new Machine Learning problems because you‚Äôll understand what steps you need to take, why you need to take them, and how to correctly execute those steps using scikit-learn.\nYou‚Äôll know what problems you might run into, and you‚Äôll know exactly how to solve them.\nBecause you‚Äôre learning a better way to work in scikit-learn, your code will be easier to write and to read, and you‚Äôll get better Machine Learning results faster than before!",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "ch01.html#book-overview",
    "href": "ch01.html#book-overview",
    "title": "1¬† Introduction",
    "section": "",
    "text": "How to handle common scenarios such as missing values, text data, categorical data, and class imbalance\nHow to build a reusable workflow that starts with a pandas DataFrame and ends with a trained scikit-learn model\nHow to integrate feature engineering, selection, and standardization into your workflow\nHow to avoid data leakage so that you can correctly estimate model performance\nHow to tune your entire workflow for maximum performance\n\n\n\n\n\n\n\n\nHigh-level topics:\n\nHandling missing values, text data, categorical data, and class imbalance\nBuilding a reusable workflow\nFeature engineering, selection, and standardization\nAvoiding data leakage\nTuning your entire workflow\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow you will benefit from this book:\n\nKnowledge of best practices\nConfidence when tackling new ML problems\nAbility to anticipate and solve problems\nImproved code quality\nBetter, faster results",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "ch01.html#scikit-learn-vs-deep-learning",
    "href": "ch01.html#scikit-learn-vs-deep-learning",
    "title": "1¬† Introduction",
    "section": "1.2 scikit-learn vs Deep Learning",
    "text": "1.2 scikit-learn vs Deep Learning\nIf you want to solve any Machine Learning problem using Python, then I always recommend starting with scikit-learn. Here‚Äôs why:\n\nIt provides a consistent interface to a huge number of Machine Learning models\nIt offers many options and tuning parameters but uses sensible defaults\nIt includes a rich set of functionality to support the entire Machine Learning workflow\nIt has exceptional documentation\nAnd there is an active community of researchers and developers who continue to improve and support the library\n\n\n\n\n\n\n\nBenefits of scikit-learn:\n\nConsistent interface to many models\nMany tuning parameters (but sensible defaults)\nWorkflow-related functionality\nExceptional documentation\nActive community support\n\n\n\n\nIn fact, scikit-learn was the most popular Machine Learning tool in Kaggle‚Äôs most recent ‚ÄúState of Machine Learning‚Äù report, with more than 80% of data scientists using it.\nFor many Machine Learning problems, scikit-learn will be the only library you need. However, there are some specialized problems for which a deep learning library such as TensorFlow, PyTorch, or Keras will provide superior results. That being said, deep learning does have some significant drawbacks:\n\nDeep learning requires more computational resources\nDeep learning libraries have a higher learning curve\nAnd deep learning models are less interpretable than non-deep learning models\n\n\n\n\n\n\n\nDrawbacks of deep learning:\n\nMore computational resources\nHigher learning curve\nLess interpretable models\n\n\n\n\nIn other words, I only recommend using deep learning if you already know that you need it to solve your particular problem. But for the majority of Machine Learning problems, you are likely to get similar results using scikit-learn, and you will get those results much faster and easier with scikit-learn.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "ch01.html#prerequisite-skills",
    "href": "ch01.html#prerequisite-skills",
    "title": "1¬† Introduction",
    "section": "1.3 Prerequisite skills",
    "text": "1.3 Prerequisite skills\nThis is an intermediate level book about scikit-learn, though it also includes many advanced topics.\nYou‚Äôre ready for this book if you can use scikit-learn to solve simple classification or regression problems, including loading a dataset, defining the features and target, training and evaluating a model, and making predictions with new data.\n\n\n\n\n\n\nscikit-learn prerequisites:\n\nLoading a dataset\nDefining the features and target\nTraining and evaluating a model\nMaking predictions with new data\n\n\n\n\nIf you‚Äôre brand new to scikit-learn, I recommend first taking my free introductory course, Introduction to Machine Learning with scikit-learn. Once you‚Äôve completed lessons 1 through 7, you‚Äôll know the Machine Learning and scikit-learn fundamentals that you‚Äôll need for this book.\n\n\n\n\n\n\nNew to scikit-learn?\n\nEnroll in Introduction to Machine Learning with scikit-learn (free course)\nComplete lessons 1 through 7\n\n\n\n\nIf you‚Äôve used scikit-learn before but you just need a refresher, there‚Äôs no need to take my introductory course because I‚Äôll be reviewing the Machine Learning workflow in the next chapter.\nFinally, I should note that we will perform a few basic pandas operations in the book, including reading a CSV file and selecting columns from a DataFrame. However, it‚Äôs not a problem at all if you‚Äôre new to pandas because I will explain that code as we go.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "ch01.html#setup-and-software-versions",
    "href": "ch01.html#setup-and-software-versions",
    "title": "1¬† Introduction",
    "section": "1.4 Setup and software versions",
    "text": "1.4 Setup and software versions\nTo follow along with this book, you‚Äôll need to have access to scikit-learn and pandas, which are both open source Python libraries. The easiest way to install scikit-learn and pandas (and their dependencies) is to download the Anaconda distribution of Python.\nFor other installation options, visit the scikit-learn and pandas websites.\n\n\n\n\n\n\nHow to install scikit-learn and pandas:\n\nOption 1: Install together\n\nAnaconda distribution\n\nOption 2: Install separately\n\nscikit-learn\npandas\n\n\n\n\n\nBecause we‚Äôll be using some of the latest scikit-learn functionality, it‚Äôs important that you are running a modern version of scikit-learn. To check your version, just open your code editor and run this code. Note that there are two underscores before and after the word ‚Äúversion‚Äù.\n\nimport sklearn\nsklearn.__version__\n\n'0.23.2'\n\n\nNote that this book uses version 0.23.2. To follow along with the book, it‚Äôs important that you are using at least version 0.20.2. There are some scikit-learn features that I‚Äôll use in the book that were released in 0.21 or 0.22 or 0.23, but none of those features are essential to the core content of the book. Throughout the book, I‚Äôll specifically mention any features that require scikit-learn 0.21 or above.\n\n\n\n\n\n\nscikit-learn version:\n\nBook version: 0.23.2\nMinimum version: 0.20.2\n\n\n\n\nYou are also welcome to use a later version of scikit-learn, such as 0.24, 1.0, or beyond. Throughout the book, I‚Äôll mention any changes to scikit-learn in those versions that are relevant to the book.\nRegarding the pandas library, this book uses version 1.2.4, but any version should work just fine.\n\nimport pandas\npandas.__version__\n\n'1.2.4'\n\n\nOne final option for following along with the book, especially if you aren‚Äôt able to install Python packages on your local machine, is to use Google Colab. Colab provides you with an interface similar to the Jupyter Notebook. It‚Äôs free and runs entirely in your browser, though it does require that you have a Google account.\n\n\n\n\n\n\nUsing Google Colab with the book:\n\nSimilar to the Jupyter Notebook\nRuns in your browser\nFree (but requires a Google account)",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "ch01.html#book-outline",
    "href": "ch01.html#book-outline",
    "title": "1¬† Introduction",
    "section": "1.5 Book outline",
    "text": "1.5 Book outline\nLet‚Äôs talk about what we‚Äôre going to cover in each chapter.\n\nIn chapter 1, which is this chapter, we‚Äôre getting you ready for the book.\nIn chapter 2, we‚Äôll walk through the basic Machine Learning workflow, from loading a dataset to building a model to making predictions.\nIn chapter 3, we‚Äôll focus on one of the most important data preprocessing steps, which is the encoding of categorical features.\nIn chapter 4, we‚Äôll see how to use ColumnTransformer and Pipeline to make our workflow more powerful and efficient.\nIn chapter 5, we‚Äôll review the workflow we‚Äôve built so far to make sure you understand the key concepts before we start adding additional complexity.\nIn chapter 6, we‚Äôll learn how to create features from unstructured text data.\nIn chapter 7, we‚Äôll discuss missing values and explore a few different ways to handle them.\nIn chapter 8, we‚Äôll see what problems arise when we expand the size of our dataset, and then we‚Äôll figure out how to handle those problems.\nIn chapter 9, we‚Äôll review our workflow again and discuss how it‚Äôs helping us to prevent data leakage.\nIn chapter 10, we‚Äôll take a deep dive into how to efficiently tune our Pipeline for maximum performance.\nIn chapter 11, we‚Äôll try out a non-linear model called ‚Äúrandom forests‚Äù and figure out how to tune it without overextending our computing resources.\nIn chapter 12, we‚Äôll learn how to ensemble our different models two different ways and how to tune the ensemble for even better performance.\nIn chapter 13, we‚Äôll discuss the benefits of feature selection and then try out a handful of different automated methods for selecting features.\nIn chapter 14, we‚Äôll experiment with standardizing our features to see if that improves our model performance.\nIn chapter 15, we‚Äôll create a variety of new features within our Pipeline and discuss why you might want to do all of your feature engineering using scikit-learn rather than pandas.\nIn chapter 16, we‚Äôll do one final review of the workflow that we created throughout the book.\nIn chapter 17, we‚Äôll experiment with different ways of handling categorical features with lots of unique values.\nIn chapter 18, we‚Äôll thoroughly explore the problem of class imbalance and the processes you can use to work around it.\nIn chapter 19, we‚Äôll walk through my complete workflow for handling class imbalance so that you can see a demonstration of the best practices.\nAnd finally, in chapter 20, we‚Äôll discuss how you can keep learning and improving your skills on your own.\n\n\n\n\n\n\n\nChapters:\n\nIntroduction\nReview of the Machine Learning workflow\nEncoding categorical features\nImproving your workflow with ColumnTransformer and Pipeline\nWorkflow review #1\nEncoding text data\nHandling missing values\nFixing common workflow problems\nWorkflow review #2\nEvaluating and tuning a Pipeline\nComparing linear and non-linear models\nEnsembling multiple models\nFeature selection\nFeature standardization\nFeature engineering with custom transformers\nWorkflow review #3\nHigh-cardinality categorical features\nClass imbalance\nClass imbalance walkthrough\nGoing further\n\n\n\n\nI recommend reading the chapters in order, because each chapter builds on the material from previous chapters.\nAlso, you might have noticed that most of the chapters end with a series of lessons marked ‚ÄúQ&A‚Äù. These lessons answer common questions that may have come up in your mind while reading the rest of the lessons in that chapter. Thus, the Q&A lessons should help you to understand the core material from each chapter in greater depth.\n\n\n\n\n\n\nLesson types:\n\nCore lessons\nQ&A lessons\n\n\n\n\nFinally, I wanted to mention that this book won‚Äôt be focusing on high-level algorithm selection, such as whether you should use a logistic regression model or a random forests model for your particular problem. That‚Äôs because I‚Äôve found that the workflow is more important and will have a greater impact on your overall Machine Learning results than your ability to pick between algorithms.\nIn fact, once you‚Äôve mastered the workflow, you can iterate through different algorithms quickly even if you don‚Äôt deeply understand them. And even if you did understand all of the algorithms, it‚Äôs hard to know in advance which one will work best for a given problem, which is why it‚Äôs so important to build a reusable workflow that enables you to switch between algorithms easily.\nThe bottom line is that understanding the algorithms is still useful, but the workflow is even more important, and so that‚Äôs the focus of this book.\n\n\n\n\n\n\nWhy not focus on algorithms?\n\nWorkflow will have a greater impact on your results\nReusable workflow enables you to try many different algorithms\nHard to know (in advance) which algorithm will work best",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "ch01.html#datasets",
    "href": "ch01.html#datasets",
    "title": "1¬† Introduction",
    "section": "1.6 Datasets",
    "text": "1.6 Datasets\nWe‚Äôll be using three different datasets in the book:\n\nThe first is the famous Titanic dataset.\nThe second is a dataset of US census data.\nThe third is a dataset of mammography scans used to detect the presence of cancer.\n\n\n\n\n\n\n\nDatasets:\n\nTitanic\nUS census\nMammography scans\n\n\n\n\nI chose these datasets for three main reasons:\nFirst, I wanted to use small-to-medium size datasets that could be read via URL so that you aren‚Äôt required to download any files for this book, which is more convenient for everyone.\nSecond, using smaller datasets means that computationally intense operations such as grid search won‚Äôt take many hours to run. This will save you a lot of time, especially if you‚Äôre working through the book without the latest and greatest hardware.\nThird, I believe you‚Äôll actually understand the lessons better because we‚Äôre not using huge datasets. From years of teaching experience, I‚Äôve found that you will never master complex processes until you understand the simpler building blocks in great detail.\nIn this book, we‚Äôre going to be examining the inputs and outputs from different workflow steps in detail, and that‚Äôs not practical when you have thousands of features or hundreds of thousands of samples.\nIn fact, we‚Äôre going to be spending the next few chapters with just 10 rows of data, which might sound crazy, but is actually the ideal way to truly understand how the different scikit-learn components work together.\n\n\n\n\n\n\nWhy use smaller datasets?\n\nEasier and faster access to files\nReduced computational time\nGreater understanding of the lessons\n\n\n\n\nThe great news is that once you master the workflow you‚Äôll learn in this book, you‚Äôll be able to handle datasets of almost any size without changing a single step. In other words, the knowledge you gain here will 100% transfer to far more complex datasets, and yet you will still understand in detail what is actually going on inside your code.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "ch01.html#meet-the-author",
    "href": "ch01.html#meet-the-author",
    "title": "1¬† Introduction",
    "section": "1.7 Meet the author",
    "text": "1.7 Meet the author\nFinally, before we wrap up this chapter, I just want to introduce myself. My name is Kevin Markham and I‚Äôm the author of this book. I‚Äôm the founder of Data School, an online school for learning Data Science with Python.\nI‚Äôve been teaching Machine Learning in the classroom and online for more than 10 years, and I‚Äôm passionate about teaching people who are new to the field, regardless of their educational and professional backgrounds.\nPersonally, I have a degree in Computer Engineering from Vanderbilt University and I live in Asheville, North Carolina.\n\n\n\n\n\n\nAbout me:\n\nFounder of Data School\nTeaching Machine Learning for 10+ years\nPassionate about teaching people who are new to the field\nDegree in Computer Engineering\nLives in Asheville, North Carolina",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "ch02.html",
    "href": "ch02.html",
    "title": "2¬† Review of the Machine Learning workflow",
    "section": "",
    "text": "2.1 Loading and exploring a dataset\nIn this chapter, we‚Äôre going to do a quick review of the basic Machine Learning workflow, from loading and exploring a dataset to building and evaluating a model, and then finally using that model to making predictions.\nWe‚Äôll start by importing the pandas library and then using its read_csv function to read a dataset directly from a URL.\nWe specified that only the first 10 rows should be read in. We‚Äôll work with the entire dataset later in the book, but by starting with just 10 rows, we can more easily examine the input and output of each step.\nimport pandas as pd\ndf = pd.read_csv('http://bit.ly/MLtrain', nrows=10)\nLet‚Äôs print out these 10 rows.\nThis is the famous Titanic dataset. There‚Äôs a lot we can learn by working with this dataset because it contains a diversity of feature types, lots of missing values, and other problems we‚Äôll need to solve.\nThe Titanic ship sank in 1912, and each row in this dataset represents one passenger. Some of the passengers survived, represented by a 1 in the Survived column, whereas others did not, represented by a 0.\ndf\n\n\n\n\n\n\n\n\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n5\n0\n3\nMoran, Mr. James\nmale\nNaN\n0\n0\n330877\n8.4583\nNaN\nQ\n\n\n6\n0\n1\nMcCarthy, Mr. Timothy J\nmale\n54.0\n0\n0\n17463\n51.8625\nE46\nS\n\n\n7\n0\n3\nPalsson, Master. Gosta Leonard\nmale\n2.0\n3\n1\n349909\n21.0750\nNaN\nS\n\n\n8\n1\n3\nJohnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)\nfemale\n27.0\n0\n2\n347742\n11.1333\nNaN\nS\n\n\n9\n1\n2\nNasser, Mrs. Nicholas (Adele Achem)\nfemale\n14.0\n1\n0\n237736\n30.0708\nNaN\nC\nLet‚Äôs review some basic terminology:\nNext, let‚Äôs select some features. There are many valid methods for feature selection, such as:\nTo start, we‚Äôre going to use intuition and start with Parch and Fare as our features:\nAs you can see above, both Parch and Fare are numeric features.\nLet‚Äôs now define our X and our y:\nX = df[['Parch', 'Fare']]\nX\n\n\n\n\n\n\n\n\nParch\nFare\n\n\n\n\n0\n0\n7.2500\n\n\n1\n0\n71.2833\n\n\n2\n0\n7.9250\n\n\n3\n0\n53.1000\n\n\n4\n0\n8.0500\n\n\n5\n0\n8.4583\n\n\n6\n0\n51.8625\n\n\n7\n1\n21.0750\n\n\n8\n2\n11.1333\n\n\n9\n0\n30.0708\ny = df['Survived']\ny\n\n0    0\n1    1\n2    1\n3    1\n4    0\n5    0\n6    0\n7    0\n8    1\n9    1\nName: Survived, dtype: int64\nObject shape is important in scikit-learn, and so we‚Äôll check the shapes of both X and y:\nBy convention, you use an uppercase X because it has 2 dimensions, and you use a lowercase y because it has 1 dimension.\nX.shape\n\n(10, 2)\ny.shape\n\n(10,)",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Review of the Machine Learning workflow</span>"
    ]
  },
  {
    "objectID": "ch02.html#loading-and-exploring-a-dataset",
    "href": "ch02.html#loading-and-exploring-a-dataset",
    "title": "2¬† Review of the Machine Learning workflow",
    "section": "",
    "text": "The Survived column is the target column, which is what we are trying to predict.\nThe target column is categorical, and thus this is a classification problem. More specifically, it‚Äôs called binary classification because there are only two classes, 0 and 1.\nAll other columns are possible features, meaning inputs to the model.\nEach row is known as a sample or observation, and represents a passenger. At the moment, we have 10 samples.\nAnd this is known as our training data because we know the actual target values.\n\n\n\n\n\n\n\nMachine Learning terminology:\n\nTarget: Goal of prediction\nClassification: Problem with a categorical target\nFeature: Input to the model (column)\nSample: Single observation (row)\nTraining data: Data with known target values\n\n\n\n\n\n\nHuman intuition, meaning what makes sense as a feature\nDomain knowledge, meaning what you know because of your expertise in that field\nData exploration, meaning relationships in the data that you discover\nAnd finally, there are automated methods that we‚Äôll cover later in the book.\n\n\n\n\n\n\n\nFeature selection methods:\n\nHuman intuition\nDomain knowledge\nData exploration\nAutomated methods\n\n\n\n\n\n\nParch is the number of parents or children aboard with that passenger. Our intuition is that families may have gotten priority access to lifeboats, and thus Parch may be predictive of who survived, which is our target.\nFare is the amount the passenger paid. Our intuition is that people who paid more may have gotten priority access to lifeboats, and again it may be predictive of our target.\n\n\n\n\n\n\n\n\nCurrently selected features:\n\nParch: Number of parents or children aboard with that passenger\nFare: Amount the passenger paid\n\n\n\n\n\n\nX is the feature matrix, and we‚Äôll use double brackets to select those two columns as a DataFrame.\ny is the target, and we‚Äôll use single brackets to select this column as a Series.\n\n\n\n\n\nX is a pandas DataFrame with 2 columns, thus it has 2 dimensions.\ny is a pandas Series, thus it has 1 dimension.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Review of the Machine Learning workflow</span>"
    ]
  },
  {
    "objectID": "ch02.html#building-and-evaluating-a-model",
    "href": "ch02.html#building-and-evaluating-a-model",
    "title": "2¬† Review of the Machine Learning workflow",
    "section": "2.2 Building and evaluating a model",
    "text": "2.2 Building and evaluating a model\nNow that we‚Äôve defined X and y, our next step is to build and evaluate a model.\nTo start, we‚Äôre going to use logistic regression as our model. It‚Äôs a good default choice for classification problems because it‚Äôs both fast and interpretable.\nWe import it from the linear_model module, and then we create an instance called logreg. This is our model object.\nThe default solver for logistic regression has changed between different scikit-learn versions, but in this book I‚Äôm going to set the solver to liblinear. I‚Äôm specifying the solver explicitly and setting a value for random_state so that if you run the same code at home, you will most likely get the same results as me.\n\nfrom sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression(solver='liblinear', random_state=1)\n\nLet‚Äôs now talk about model evaluation. The goal of model evaluation is to simulate how a model will perform on future data so that we can choose between models today. To do model evaluation, we need both an evaluation procedure and an evaluation metric.\nThe procedure we will use is K-fold cross-validation. Another option is to use train/test split, but cross-validation is generally superior because it gives a lower variance estimate of model performance.\nThe metric we will use is classification accuracy. There are many other classification metrics we could have chosen, but accuracy is suitable for this problem for two reasons:\n\nFirst, there is not significant class imbalance.\nAnd second, predicting the positive class correctly is just as important to us as predicting the negative class correctly.\n\nThat being said, I will cover other classification metrics in the chapters on class imbalance.\n\n\n\n\n\n\nRequirements for model evaluation:\n\nProcedure: K-fold cross-validation\nMetric: Classification accuracy\n\n\n\n\nWith such a small dataset, we‚Äôre going to use 3-fold cross-validation, rather than 5 or 10 folds which is more typical. Let me briefly review what happens during 3-fold cross-validation:\n\nThe rows are split into 3 subsets, which we‚Äôll call A, B, and C.\nFirst, A and B together become the training set, and C becomes the testing set. The model is trained on the training set, the trained model makes predictions for the testing set, and those predictions are evaluated.\nNext, A and C together become the training set, and B becomes the testing set. Again, the model is trained, it makes predictions, and the predictions are evaluated.\nFinally, B and C together become the training set, and A becomes the testing set. The training, predicting, and evaluation process happens one final time.\nBecause the evaluation process occurred 3 times, it returns 3 scores, and we will usually take the mean of those scores.\n\n\n\n\n\n\n\nSteps of 3-fold cross-validation:\n\nSplit rows into 3 subsets (A, B, C)\nA & B is training set, C is testing set\n\nTrain model on training set\nMake predictions on testing set\nEvaluate predictions\n\nRepeat with A & C as training set, B as testing set\nRepeat with B & C as training set, A as testing set\nCalculate the mean of the scores\n\n\n\n\nLet‚Äôs go ahead and use cross-validation to evaluate our model:\n\nFirst we import the cross_val_score function from the model_selection module.\nThen we pass it the model object, X and y, the number of cross-validation folds, and the evaluation metric. Although the default metric for classification problems is accuracy, I recommend specifying it explicitly so that there‚Äôs no ambiguity.\nWhen we run cross_val_score, it does the dataset splitting, training, predicting, and evaluation. 3 accuracy scores are returned, and the mean of those scores is 69%.\n\n\nfrom sklearn.model_selection import cross_val_score\ncross_val_score(logreg, X, y, cv=3, scoring='accuracy').mean()\n\n0.6944444444444443\n\n\nIf you received a different result, that‚Äôs not a problem. The results can vary based on your scikit-learn version due to changes in the default parameters, algorithm changes, bug fixes, and so on.\nUnfortunately, we can‚Äôt take these results seriously because the dataset is so small. There‚Äôs actually no reliable evaluation procedure when your training data only contains 10 rows, but I did want to demonstrate it anyway to emphasize that model evaluation is a normal part of the Machine Learning workflow.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Review of the Machine Learning workflow</span>"
    ]
  },
  {
    "objectID": "ch02.html#using-the-model-to-make-predictions",
    "href": "ch02.html#using-the-model-to-make-predictions",
    "title": "2¬† Review of the Machine Learning workflow",
    "section": "2.3 Using the model to make predictions",
    "text": "2.3 Using the model to make predictions\nAt this point in the workflow, we would typically try making changes in order to achieve a better accuracy, such as:\n\nTuning the the model‚Äôs hyperparameters\nAdding or removing features\nOr trying a different classification model other than logistic regression.\n\n\n\n\n\n\n\nWays to improve the model:\n\nHyperparameter tuning\nAdding or removing features\nTrying a different model\n\n\n\n\nWe‚Äôll cover these topics in detail later in the book, but for now, let‚Äôs assume that we‚Äôre happy with the model as-is. Thus, our next steps are to train the model and then use it to make predictions on new data.\nWe use the model‚Äôs fit method, which instructs the model to try to learn the relationship between X and y.\n\nlogreg.fit(X, y)\n\nLogisticRegression(random_state=1, solver='liblinear')\n\n\nThere are four important points I want to note here:\n\nFirst, you should train your model on the entire dataset before using it to make predictions, otherwise you are throwing away valuable training data. In truth, we do have more than 10 rows, but for now we are considering our entire training dataset to be these 10 rows.\nSecond, the model object is modified in-place when you run the fit method, and so there‚Äôs no need to overwrite the logreg object using an assignment statement.\nThird, scikit-learn understands how to work with pandas objects, and so we can pass X and y directly to the fit method.\nAnd finally, if you‚Äôre using scikit-learn 0.23 or later, you will only see the parameters that have changed from the defaults when you print or fit a model. That‚Äôs why it only displays the random_state and solver parameters, whereas in previous versions of scikit-learn, all model parameters would have been displayed.\n\n\n\n\n\n\n\nImportant points about model fitting:\n\nTrain your model on the entire dataset before making predictions\nAssignment statement is unnecessary\nPassing pandas objects is fine\nOnly prints parameters that have changed (version 0.23 or later)\n\n\n\n\nNow, let‚Äôs read in a new dataset for which we don‚Äôt know the target values. Again, we are only going to keep the first 10 rows.\nYou‚Äôll notice that it has the same columns as the df DataFrame, except that there‚Äôs no Survived column, which is the column that we‚Äôre going to predict.\n\ndf_new = pd.read_csv('http://bit.ly/MLnewdata', nrows=10)\ndf_new\n\n\n\n\n\n\n\n\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n3\nKelly, Mr. James\nmale\n34.5\n0\n0\n330911\n7.8292\nNaN\nQ\n\n\n1\n3\nWilkes, Mrs. James (Ellen Needs)\nfemale\n47.0\n1\n0\n363272\n7.0000\nNaN\nS\n\n\n2\n2\nMyles, Mr. Thomas Francis\nmale\n62.0\n0\n0\n240276\n9.6875\nNaN\nQ\n\n\n3\n3\nWirz, Mr. Albert\nmale\n27.0\n0\n0\n315154\n8.6625\nNaN\nS\n\n\n4\n3\nHirvonen, Mrs. Alexander (Helga E Lindqvist)\nfemale\n22.0\n1\n1\n3101298\n12.2875\nNaN\nS\n\n\n5\n3\nSvensson, Mr. Johan Cervin\nmale\n14.0\n0\n0\n7538\n9.2250\nNaN\nS\n\n\n6\n3\nConnolly, Miss. Kate\nfemale\n30.0\n0\n0\n330972\n7.6292\nNaN\nQ\n\n\n7\n2\nCaldwell, Mr. Albert Francis\nmale\n26.0\n1\n1\n248738\n29.0000\nNaN\nS\n\n\n8\n3\nAbrahim, Mrs. Joseph (Sophie Halaut Easu)\nfemale\n18.0\n0\n0\n2657\n7.2292\nNaN\nC\n\n\n9\n3\nDavies, Mr. John Samuel\nmale\n21.0\n2\n0\nA/4 48871\n24.1500\nNaN\nS\n\n\n\n\n\n\n\nBefore we make predictions, we have to define X_new. It has to have the same columns as X, and those columns have to be in the same order.\n\nX_new = df_new[['Parch', 'Fare']]\nX_new\n\n\n\n\n\n\n\n\nParch\nFare\n\n\n\n\n0\n0\n7.8292\n\n\n1\n0\n7.0000\n\n\n2\n0\n9.6875\n\n\n3\n0\n8.6625\n\n\n4\n1\n12.2875\n\n\n5\n0\n9.2250\n\n\n6\n0\n7.6292\n\n\n7\n1\n29.0000\n\n\n8\n0\n7.2292\n\n\n9\n0\n24.1500\n\n\n\n\n\n\n\nFinally, we‚Äôll use the trained model to make predictions by passing X_new to the predict method, which outputs a NumPy array. There are 10 predictions because it makes 1 prediction for each sample in X_new.\nThe predictions are in the same order as the samples in X_new, meaning the first prediction is for the first row in X_new, the second prediction is for the second row in X_new, and so on.\n\nlogreg.predict(X_new)\n\narray([0, 0, 0, 0, 1, 0, 0, 1, 0, 1])\n\n\nNote that we can‚Äôt actually evaluate the accuracy of these predictions because we don‚Äôt know the true target values for the samples in X_new.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Review of the Machine Learning workflow</span>"
    ]
  },
  {
    "objectID": "ch02.html#qa-how-do-i-adapt-this-workflow-to-a-regression-problem",
    "href": "ch02.html#qa-how-do-i-adapt-this-workflow-to-a-regression-problem",
    "title": "2¬† Review of the Machine Learning workflow",
    "section": "2.4 Q&A: How do I adapt this workflow to a regression problem?",
    "text": "2.4 Q&A: How do I adapt this workflow to a regression problem?\nIn this book, we‚Äôre going to be focusing on classification problems, which means that the target you‚Äôre trying to predict is categorical. The other main type of prediction problem is regression, in which your target value is continuous.\nIf you‚Äôre planning to work on a regression problem, the good news is that the workflow I‚Äôm teaching will work just as well to solve classification or regression problems. There are only two changes you will need to make to adapt this workflow for regression:\n\nFirst, you will need to choose a different Machine Learning model. For example, you might choose linear regression instead of logistic regression, since linear regression predicts continuous values whereas logistic regression predicts class values.\nSecond, you will need to choose a different model evaluation metric. For example, you might choose mean squared error instead of accuracy, since mean squared error is appropriate for continuous values whereas accuracy is only appropriate for categorical values.\n\n\n\n\n\n\n\nAdapting this workflow for regression:\n\nChoose a different model\nChoose a different evaluation metric",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Review of the Machine Learning workflow</span>"
    ]
  },
  {
    "objectID": "ch02.html#qa-how-do-i-adapt-this-workflow-to-a-multiclass-problem",
    "href": "ch02.html#qa-how-do-i-adapt-this-workflow-to-a-multiclass-problem",
    "title": "2¬† Review of the Machine Learning workflow",
    "section": "2.5 Q&A: How do I adapt this workflow to a multiclass problem?",
    "text": "2.5 Q&A: How do I adapt this workflow to a multiclass problem?\nIn this chapter, we worked on a binary classification problem, which means there are only two possible output classes.\nMulticlass problems are ones in which there are more than two output classes. The classic example of this is the iris dataset, in which each iris plant can be classified as one of three possible species.\nThankfully, in scikit-learn, all classifiers automatically handle multiclass problems with no changes to the workflow. It automatically detects the number of classes from the data, thus you don‚Äôt even have to inform scikit-learn that you‚Äôre working on a multiclass problem.\n\n\n\n\n\n\nTypes of classification problems:\n\nBinary: Two output classes\nMulticlass: More than two output classes\n\n\n\n\nSo how do classifiers handle multiclass problems?\n\nMany classifiers are inherently multiclass, meaning that they work exactly the same regardless of the number of classes.\nFor classifiers that only work in the binary case, they can be extended to the multiclass case using the so-called ‚Äúone-vs-one‚Äù or ‚Äúone-vs-rest‚Äù strategies, in which multiple models are fit and the results are combined. You can research more about these strategies if you‚Äôre interested, but the key point is that this is handled for you automatically by scikit-learn without you needing to do anything special.\n\n\n\n\n\n\n\nHow classifiers handle multiclass problems:\n\nMany are inherently multiclass\nOthers can be extended using ‚Äúone-vs-one‚Äù or ‚Äúone-vs-rest‚Äù strategies",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Review of the Machine Learning workflow</span>"
    ]
  },
  {
    "objectID": "ch02.html#qa-why-should-i-select-a-series-for-the-target",
    "href": "ch02.html#qa-why-should-i-select-a-series-for-the-target",
    "title": "2¬† Review of the Machine Learning workflow",
    "section": "2.6 Q&A: Why should I select a Series for the target?",
    "text": "2.6 Q&A: Why should I select a Series for the target?\nFrom our DataFrame, there are two ways you could imagine selecting the target variable of Survived:\n\nThe first way is as a pandas Series, which we can do using a single set of brackets.\nThe second way is as a pandas DataFrame with one column, which we can do using two sets of brackets.\n\n\ndf['Survived']\n\n0    0\n1    1\n2    1\n3    1\n4    0\n5    0\n6    0\n7    0\n8    1\n9    1\nName: Survived, dtype: int64\n\n\n\ndf[['Survived']]\n\n\n\n\n\n\n\n\nSurvived\n\n\n\n\n0\n0\n\n\n1\n1\n\n\n2\n1\n\n\n3\n1\n\n\n4\n0\n\n\n5\n0\n\n\n6\n0\n\n\n7\n0\n\n\n8\n1\n\n\n9\n1\n\n\n\n\n\n\n\nThese two objects look similar, but they actually have different shapes. The Series is a one-dimensional object, while the DataFrame is a two-dimensional object.\n\ndf['Survived'].shape\n\n(10,)\n\n\n\ndf[['Survived']].shape\n\n(10, 1)\n\n\nThe difference between these two objects is more clear if you convert them to NumPy arrays using the to_numpy method.\n\ndf['Survived'].to_numpy()\n\narray([0, 1, 1, 1, 0, 0, 0, 0, 1, 1])\n\n\n\ndf[['Survived']].to_numpy()\n\narray([[0],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1]])\n\n\nNow that you‚Äôve seen that these two objects are different, the question is: Why does it matter which object you use for the target?\nTo answer this question, I have to briefly explain multilabel classification. A multilabel classification problem is one in which each sample can simultaneously have more than one label. The classic example of this is classifying the topic of a document. For example, a document might be about politics, religion, or law, or it might fit into multiple topics at once.\nThis is different from multiclass classification because multiclass only allows a sample to have a single label, whereas multilabel allows a single sample to have multiple labels.\n\n\n\n\n\n\nMultilabel vs multiclass problems:\n\nMultilabel: Each sample can have more than one label\nMulticlass: Each sample can have one label\n\n\n\n\nAnyway, scikit-learn supports multilabel classification by allowing you to represent the target as a two-dimensional object. For example, if you had 10 documents and there were 3 possible labels, you would actually use a 10 by 3 DataFrame as your y value.\nThat is all to say that using a two-dimensional DataFrame as your y value signals to scikit-learn that you are working on a multilabel problem. Our classification problem is not multilabel, and thus we use a one-dimensional Series as our y value to signal that we are working on a single-label problem.\n\n\n\n\n\n\nMultilabel vs multiclass targets:\n\nMultilabel: 2-dimensional y (DataFrame)\nMulticlass: 1-dimensional y (Series)",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Review of the Machine Learning workflow</span>"
    ]
  },
  {
    "objectID": "ch02.html#qa-how-do-i-add-the-models-predictions-to-a-dataframe",
    "href": "ch02.html#qa-how-do-i-add-the-models-predictions-to-a-dataframe",
    "title": "2¬† Review of the Machine Learning workflow",
    "section": "2.7 Q&A: How do I add the model‚Äôs predictions to a DataFrame?",
    "text": "2.7 Q&A: How do I add the model‚Äôs predictions to a DataFrame?\nLet‚Äôs say that you wanted to match up the 10 predictions output by the model with the 10 rows of the X_new DataFrame so that you can see the predictions next to the features.\nTo do this, you would first convert the predictions from a NumPy array to a pandas Series. Note that we are setting the Series index to match the index of X_new, and we‚Äôre giving the Series a name.\n\npredictions = pd.Series(logreg.predict(X_new), index=X_new.index,\n                        name='Prediction')\n\nThen, you use the concat function to concatenate the X_new DataFrame and the predictions Series along the 'columns' axis, which outputs a DataFrame. Note that the name of the Series became the name of that DataFrame column.\n\npd.concat([X_new, predictions], axis='columns')\n\n\n\n\n\n\n\n\nParch\nFare\nPrediction\n\n\n\n\n0\n0\n7.8292\n0\n\n\n1\n0\n7.0000\n0\n\n\n2\n0\n9.6875\n0\n\n\n3\n0\n8.6625\n0\n\n\n4\n1\n12.2875\n1\n\n\n5\n0\n9.2250\n0\n\n\n6\n0\n7.6292\n0\n\n\n7\n1\n29.0000\n1\n\n\n8\n0\n7.2292\n0\n\n\n9\n0\n24.1500\n1",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Review of the Machine Learning workflow</span>"
    ]
  },
  {
    "objectID": "ch02.html#qa-how-do-i-determine-the-confidence-level-of-each-prediction",
    "href": "ch02.html#qa-how-do-i-determine-the-confidence-level-of-each-prediction",
    "title": "2¬† Review of the Machine Learning workflow",
    "section": "2.8 Q&A: How do I determine the confidence level of each prediction?",
    "text": "2.8 Q&A: How do I determine the confidence level of each prediction?\nFor some classification problems, you are only interested in the predicted class labels.\n\nlogreg.predict(X_new)\n\narray([0, 0, 0, 0, 1, 0, 0, 1, 0, 1])\n\n\nHowever, sometimes it‚Äôs useful to output the predicted probabilities of class membership using the predict_proba method.\nThe output array has 10 rows because the model made predictions for 10 samples, and it has 2 columns because there are 2 possible classes. The left column represents class 0, and the right column represents class 1.\n\nlogreg.predict_proba(X_new)\n\narray([[0.57804075, 0.42195925],\n       [0.58275546, 0.41724454],\n       [0.56742414, 0.43257586],\n       [0.57328835, 0.42671165],\n       [0.48357081, 0.51642919],\n       [0.57007262, 0.42992738],\n       [0.57917926, 0.42082074],\n       [0.38795132, 0.61204868],\n       [0.58145374, 0.41854626],\n       [0.48342837, 0.51657163]])\n\n\n\n\n\n\n\n\nArray of predicted probabilities:\n\nOne row for each sample\nOne column for each class\n\n\n\n\nLet‚Äôs talk about how to interpret this array, using the first row as an example:\n\nThe model calculated a likelihood of 58% that the first sample in X_new was class 0 and a 42% likelihood that it was class 1.\nBecause class 0 had a higher likelihood, the model predicted class 0 for this sample.\nAnd as you might imagine, the values in every row will always add up to 1.\n\nIf you need just the second column, meaning the predicted probabilities of class 1, then you can extract it using NumPy‚Äôs slicing notation. The : means select all rows, and the 1 means select the column in the 1 position, which is the second column.\n\nlogreg.predict_proba(X_new)[:, 1]\n\narray([0.42195925, 0.41724454, 0.43257586, 0.42671165, 0.51642919,\n       0.42992738, 0.42082074, 0.61204868, 0.41854626, 0.51657163])\n\n\nSome classifiers, such as logistic regression, are known as well-calibrated classifiers, which means that their predicted probabilities can be directly interpreted as the model‚Äôs confidence level in that prediction. So for example, it‚Äôs more confident that the 8th sample is class 1 than it is that the 10th sample is class 1.\nKnowing these confidence levels can be useful if you‚Äôre most interested in the samples with the highest predicted probabilities. For example, if you were trying to predict who might be interested in purchasing a specific product, you might focus all of your marketing budget on reaching those customers with the highest predicted probabilities of purchase.\nKeep in mind that there are other classifiers which are not as well-calibrated, such as Naive Bayes. In those cases, it‚Äôs less appropriate to interpret their predicted probabilities as confidence levels. Thus if you know you‚Äôre going to be interested in these confidence levels, it‚Äôs best to use a well-calibrated classifier like logistic regression.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Review of the Machine Learning workflow</span>"
    ]
  },
  {
    "objectID": "ch02.html#qa-how-do-i-check-the-accuracy-of-the-models-predictions",
    "href": "ch02.html#qa-how-do-i-check-the-accuracy-of-the-models-predictions",
    "title": "2¬† Review of the Machine Learning workflow",
    "section": "2.9 Q&A: How do I check the accuracy of the model‚Äôs predictions?",
    "text": "2.9 Q&A: How do I check the accuracy of the model‚Äôs predictions?\nAt the end of this chapter, we made predictions for the 10 samples in X_new, though we couldn‚Äôt check the accuracy of these predictions because we don‚Äôt know the true target values for those samples. That will sometimes be the case in the real world.\nFor example, if you built a model to predict what medical conditions someone might develop based on their genetic information, you may not ever find out whether the model‚Äôs predictions were correct, either because that data is not being collected or because that data is protected by privacy laws.\nIn other cases, you can actually check the accuracy of your predictions. For example, if you built a model to predict the outcome of all US Supreme Court cases, you would make those predictions before those cases were decided, and then you could check the model‚Äôs accuracy once the court‚Äôs rulings were publicly announced.\n\n\n\n\n\n\nChecking model accuracy:\n\nNot possible: Target value is unknown or is private data\nPossible: Target value is known\n\n\n\n\nIdeally, the actual accuracy of your model will be close to the accuracy that you estimated using your training data during model evaluation. If it‚Äôs not close, that could indicate a problem with your model evaluation procedure, or it could indicate that there are some important differences between your training data and the new data.\nIn all cases, you can incorporate this new data into your training data since you know the true target values, which should help the model to make better predictions in the future.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Review of the Machine Learning workflow</span>"
    ]
  },
  {
    "objectID": "ch02.html#qa-what-do-the-solver-and-random_state-parameters-do",
    "href": "ch02.html#qa-what-do-the-solver-and-random_state-parameters-do",
    "title": "2¬† Review of the Machine Learning workflow",
    "section": "2.10 Q&A: What do the ‚Äúsolver‚Äù and ‚Äúrandom_state‚Äù parameters do?",
    "text": "2.10 Q&A: What do the ‚Äúsolver‚Äù and ‚Äúrandom_state‚Äù parameters do?\nIn this chapter, when creating the model object, I set the logistic regression‚Äôs solver to liblinear, and I set the random_state to 1. I set these values so that if you ran the same code at home, you would most likely get the same results as me. In this lesson, I‚Äôll explain what these two parameters actually do.\n\nlogreg = LogisticRegression(solver='liblinear', random_state=1)\n\nThe solver is the algorithm used to solve the optimization problem of calculating the logistic regression‚Äôs coefficients. In other words, given the features and the target, the solver figures out the coefficients. The solvers have different strengths and weaknesses, different properties, and ultimately may come up with different results. Here‚Äôs a comparison chart from the scikit-learn documentation.\n\nI recommend reviewing this chart and reading the documentation to decide which solver to use for your particular problem, but ultimately it‚Äôs fine to just try each one and see what happens.\nliblinear used to be the default solver for logistic regression, but in scikit-learn version 0.22, they changed the default to lbfgs instead. I‚Äôm having all of us use liblinear in this book so that we will tend to get the same results, regardless of scikit-learn version. Keep in mind that we still aren‚Äôt guaranteed to get the exact same results, because with each new version, bugs are fixed and other algorithm parameters are sometimes changed.\n\n\n\n\n\n\nDefault solver for logistic regression:\n\nBefore version 0.22: liblinear\nStarting in version 0.22: lbfgs\n\n\n\n\nOne final note about the solver is that if you ever get a convergence warning when using logistic regression, the best solution is usually just to use a different solver.\nNext, let‚Äôs talk about the random_state parameter. It happens that there is some randomness involved in three of the solvers, including liblinear. That means you may get different results each time you fit the model. By setting the random_state, you ensure that your model will output the same results every time.\nMore generally, any time you‚Äôre running a scikit-learn function that involves a random process, I recommend setting the random_state parameter to any integer. That allows your code to be reproducible, both by you and others. Keep in mind that the only way to know whether a given function involves a random process is by reading the documentation.\n\n\n\n\n\n\nAdvice for random_state:\n\nSet random_state to any integer when a random process is involved\nAllows your code to be reproducible",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Review of the Machine Learning workflow</span>"
    ]
  },
  {
    "objectID": "ch02.html#qa-how-do-i-show-all-of-the-model-parameters",
    "href": "ch02.html#qa-how-do-i-show-all-of-the-model-parameters",
    "title": "2¬† Review of the Machine Learning workflow",
    "section": "2.11 Q&A: How do I show all of the model parameters?",
    "text": "2.11 Q&A: How do I show all of the model parameters?\nStarting in scikit-learn version 0.23, when you print any estimator (such as a model, a transformer, or a pipeline), it only shows you the parameters that are not set to their default values. For example, when we print out the logreg model object, it only shows the random_state and solver parameters because we set those explicitly.\n\nlogreg\n\nLogisticRegression(random_state=1, solver='liblinear')\n\n\nHowever, you can still see all parameters by running the get_params method.\n\nlogreg.get_params()\n\n{'C': 1.0,\n 'class_weight': None,\n 'dual': False,\n 'fit_intercept': True,\n 'intercept_scaling': 1,\n 'l1_ratio': None,\n 'max_iter': 100,\n 'multi_class': 'auto',\n 'n_jobs': None,\n 'penalty': 'l2',\n 'random_state': 1,\n 'solver': 'liblinear',\n 'tol': 0.0001,\n 'verbose': 0,\n 'warm_start': False}\n\n\nIf you like, you can restore the behavior from previous scikit-learn versions by importing the set_config function and then setting the print_changed_only parameter to False.\n\nfrom sklearn import set_config\nset_config(print_changed_only=False)\n\nNow, all parameters will be printed, regardless of whether you‚Äôve changed them.\n\nlogreg\n\nLogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n                   multi_class='auto', n_jobs=None, penalty='l2',\n                   random_state=1, solver='liblinear', tol=0.0001, verbose=0,\n                   warm_start=False)\n\n\nI prefer the new behavior, so I‚Äôm going to set print_changed_only back to True.\n\nset_config(print_changed_only=True)",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Review of the Machine Learning workflow</span>"
    ]
  },
  {
    "objectID": "ch02.html#qa-should-i-shuffle-the-samples-when-using-cross-validation",
    "href": "ch02.html#qa-should-i-shuffle-the-samples-when-using-cross-validation",
    "title": "2¬† Review of the Machine Learning workflow",
    "section": "2.12 Q&A: Should I shuffle the samples when using cross-validation?",
    "text": "2.12 Q&A: Should I shuffle the samples when using cross-validation?\nWhen I ran cross_val_score earlier in this chapter, I passed an integer, 3 in this case, that specified the number of cross-validation folds.\n\ncross_val_score(logreg, X, y, cv=3, scoring='accuracy')\n\narray([0.75      , 0.66666667, 0.66666667])\n\n\nThis code shows you what happens ‚Äúunder the hood‚Äù when you specify cv=3 for a classification problem. I‚Äôm going to walk through this code so you understand what‚Äôs happening and you can modify it when needed.\nFirst, you‚Äôll notice that we‚Äôre importing a class called StratifiedKFold. It‚Äôs known as a cross-validation splitter, which means that its role is to split datasets. We create an instance of this class and pass it a 3 so that it will create 3 folds. And then we can pass this instance to cross_val_score instead of an integer.\n\nfrom sklearn.model_selection import StratifiedKFold\nkf = StratifiedKFold(3)\ncross_val_score(logreg, X, y, cv=kf, scoring='accuracy')\n\narray([0.75      , 0.66666667, 0.66666667])\n\n\nIt‚Äôs called StratifiedKFold because it uses stratified sampling to ensure that the class proportions are approximately equal in each fold. For example, if 40% of the passengers in the dataset survived, then stratified sampling ensures that about 40% of each fold is survived passengers.\nIn other words, it ensures that each fold is representative of the entire dataset. Stratified sampling is desirable because it produces more reliable cross-validation scores, and again, scikit-learn will do this for you by default.\n\n\n\n\n\n\nStratified sampling:\n\nEnsures that each fold is representative of the dataset\nProduces more reliable cross-validation scores\n\n\n\n\nAnother good thing to know about StratifiedKFold is that by default, it does not shuffle the samples before splitting. Thus, there is nothing random about this process, and as such you will get the same results every time you run cross_val_score.\nIn most cases, it doesn‚Äôt matter whether you shuffle the samples before splitting. However, if the order of the samples in your dataset is not arbitrary, then it‚Äôs important to randomly shuffle the samples when cross-validating.\nFor example, you could imagine that if your dataset was sorted by one of the features, then some folds would only have high values of that feature and other folds would only have low values of that feature, which could result in unreliable cross-validation scores.\nIf you do need to shuffle the samples, you simply modify the cross-validation splitter by setting shuffle to True, and then pass that splitter object to cross_val_score. Note that because you are introducing randomness into the process by shuffling, you should also set a random_state to ensure reproducibility.\n\nkf = StratifiedKFold(3, shuffle=True, random_state=1)\ncross_val_score(logreg, X, y, cv=kf, scoring='accuracy')\n\narray([0.75      , 0.33333333, 0.66666667])\n\n\nIn summary, if you have a classification problem and the samples are in an arbitrary order, you can just pass an integer to the cv parameter of cross_val_score, and it will use stratified sampling without shuffling.\nIf your samples are not in an arbitrary order, you should use StratifiedKFold as your splitter and set shuffle to True, and then pass the splitter object to the cv parameter of cross_val_score, as I did above.\n\n\n\n\n\n\nWhen to shuffle your samples:\n\nSamples in arbitrary order: Shuffling not needed\nSamples are ordered: Shuffling needed\n\n\n\n\nFinally, it‚Äôs worth mentioning that if you‚Äôre working on a regression problem instead, and you need to shuffle the samples, you should use the KFold class instead of the StratifiedKFold class, because stratified sampling does not apply to regression problems.\n\n\n\n\n\n\nHow to shuffle your samples:\n\nClassification: StratifiedKFold\nRegression: KFold",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Review of the Machine Learning workflow</span>"
    ]
  },
  {
    "objectID": "ch03.html",
    "href": "ch03.html",
    "title": "3¬† Encoding categorical features",
    "section": "",
    "text": "3.1 Introduction to one-hot encoding\nIn this chapter, we‚Äôre going to focus on one of the most important data preprocessing steps, which is the encoding of categorical features.\nLet‚Äôs take a look at our Titanic DataFrame. In the last chapter, the only features we used were Parch and Fare. In this chapter, we want to add Embarked and Sex as additional features, in case they improve our model.\ndf\n\n\n\n\n\n\n\n\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n5\n0\n3\nMoran, Mr. James\nmale\nNaN\n0\n0\n330877\n8.4583\nNaN\nQ\n\n\n6\n0\n1\nMcCarthy, Mr. Timothy J\nmale\n54.0\n0\n0\n17463\n51.8625\nE46\nS\n\n\n7\n0\n3\nPalsson, Master. Gosta Leonard\nmale\n2.0\n3\n1\n349909\n21.0750\nNaN\nS\n\n\n8\n1\n3\nJohnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)\nfemale\n27.0\n0\n2\n347742\n11.1333\nNaN\nS\n\n\n9\n1\n2\nNasser, Mrs. Nicholas (Adele Achem)\nfemale\n14.0\n1\n0\n237736\n30.0708\nNaN\nC\nAs a reminder, Parch is the number of parents or children aboard with that passenger, and Fare is the amount the passenger paid. Our first new feature, Embarked, is the port that each passenger embarked from, and the possible values are C, Q, or S. Our other new feature, Sex, is simply male or female.\nBoth Embarked and Sex are known as unordered categorical features because there are distinct categories and there‚Äôs no inherent logical ordering to the categories. This type of data is also known as nominal data.\nAll scikit-learn models expect features to be numeric, and so Embarked and Sex can‚Äôt actually be passed directly to a model. Instead, we‚Äôre going to encode them using a process called one-hot encoding, also known as dummy encoding.\nLet‚Äôs look at the code for one-hot encoding. First, we import the OneHotEncoder class from the preprocessing module. Then, we create an instance of it and set sparse to False.\nfrom sklearn.preprocessing import OneHotEncoder\nohe = OneHotEncoder(sparse=False)\nBy default, OneHotEncoder will output a sparse matrix, which is the most efficient and performant data structure for this type of data. By setting sparse to False, it will instead output a dense matrix, which is just the normal way of representing a matrix. This representation will allow us to examine the output so that we can understand the encoding scheme.\nNext, we‚Äôll encode the Embarked column by passing it to the fit_transform method of the OneHotEncoder. We‚Äôll talk about the fit_transform method in the next lesson, but for now I just want to highlight that we‚Äôll use double brackets around Embarked to pass it as a single-column DataFrame instead of using single brackets to pass it as a Series.\nThis is important because OneHotEncoder expects to receive a two-dimensional object (such as a DataFrame) since a one-dimensional object is considered ambiguous. A one-dimensional Series could be interpreted either as a single feature or a single sample, whereas our two-dimensional DataFrame signals to scikit-learn that this is indeed a single feature.\nRunning the fit_transform method outputs this 10 by 3 array. This is the encoded version of the Embarked column, and it is exactly what we will pass to the model instead of the strings C, Q, and S.\nohe.fit_transform(df[['Embarked']])\n\narray([[0., 0., 1.],\n       [1., 0., 0.],\n       [0., 0., 1.],\n       [0., 0., 1.],\n       [0., 0., 1.],\n       [0., 1., 0.],\n       [0., 0., 1.],\n       [0., 0., 1.],\n       [0., 0., 1.],\n       [1., 0., 0.]])\nLet‚Äôs talk about how we interpret this output. There are 3 columns because there were 3 unique values in Embarked. Each row contains a single 1, and the rest of the values in the row are 0. 100 means ‚ÄúC‚Äù, 010 means ‚ÄúQ‚Äù, and 001 means ‚ÄúS‚Äù, which you can confirm by comparing it to the Embarked column in our DataFrame.\nAs an aside, this is called one-hot encoding because in each row there is one ‚Äúhot‚Äù level, meaning one non-zero level.\nThis is also the same output you would get by using the get_dummies function in pandas, though we‚Äôll talk later in the book why it‚Äôs best to do all of your preprocessing in scikit-learn instead of pandas.\nLet‚Äôs now look at the categories_ attribute of the OneHotEncoder. You can think of it as the column header for our 10 by 3 array. In other words, the categories_ attribute tells you that the first column represents ‚ÄúC‚Äù, the second column represents ‚ÄúQ‚Äù, and the third column represents ‚ÄúS‚Äù. Because the categories are always in alphabetical order from left to right, I didn‚Äôt actually have to examine the categories_ attribute in order to know how to interpret it.\nAs an aside, you‚Äôll notice a lot of attributes in scikit-learn end in an _. This is scikit-learn‚Äôs convention for any attribute that is learned or estimated from the data during the fit step.\nohe.categories_\n\n[array(['C', 'Q', 'S'], dtype=object)]\nWe‚Äôve now seen how the OneHotEncoder encodes the Embarked feature. But why is this a reasonable way to encode a categorical feature?\nYou can think of it this way: OneHotEncoder creates a feature from each level so that the model can learn the relationship between each level and the target value. In this case, the model can learn the relationship between the target value of Survived and whether or not a passenger embarked at a given port.\nFor example, the model might learn from the first feature that passengers who embarked at C have a higher survival rate than passengers who didn‚Äôt embark at C. This is similar to how a model might learn from a numeric feature like Fare that passengers with a higher Fare have a higher survival rate than passengers with a lower Fare.\nAt this point, you might be wondering whether we could have instead encoded Embarked as a single numeric feature with the values 0, 1, and 2 representing C, Q, and S. The answer is that yes, we can do this, but it‚Äôs generally not a good idea to do this with unordered categories because it would imply an ordering that doesn‚Äôt inherently exist.\nTo see why it‚Äôs not a good idea, let‚Äôs pretend that passengers who embarked at C and S had high survival rates, and passengers who embarked at Q had low survival rates. There would be no way for a linear model like logistic regression to learn that relationship if Embarked is encoded as a single feature since a single feature can‚Äôt be assigned both a negative coefficient to represent the impact of Q (with respect to C) and a positive coefficient to represent the impact of S (with respect to Q).\nIn summary, encoding Embarked as a single feature would prohibit a linear model from learning a non-linear relationship in the data, which is why encoding it as multiple features is generally the better choice.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Encoding categorical features</span>"
    ]
  },
  {
    "objectID": "ch03.html#introduction-to-one-hot-encoding",
    "href": "ch03.html#introduction-to-one-hot-encoding",
    "title": "3¬† Encoding categorical features",
    "section": "",
    "text": "Currently selected features:\n\nParch: Number of parents or children aboard with that passenger\nFare: Amount the passenger paid\nEmbarked: Port the passenger embarked from\nSex: Male or female\n\n\n\n\n\n\n\n\n\n\n\nUnordered categorical data:\n\nContains distinct categories\nNo inherent logical ordering to the categories\nAlso called ‚Äúnominal data‚Äù\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMatrix representations:\n\nSparse: More efficient and performant\nDense: More readable\n\n\n\n\n\n\n\n\n\n\n\n\nWhy use double brackets?\n\nSingle brackets:\n\nOutputs a Series\nCould be interpreted as a single feature or a single sample\n\nDouble brackets:\n\nOutputs a single-column DataFrame\nInterpreted as a single feature\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOutput of OneHotEncoder:\n\nOne column for each unique value\nOne non-zero value in each row:\n\n1, 0, 0 means ‚ÄúC‚Äù\n0, 1, 0 means ‚ÄúQ‚Äù\n0, 0, 1 means ‚ÄúS‚Äù\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy use one-hot encoding?\n\nModel can learn the relationship between each level and the target value\nExample: Model might learn that ‚ÄúC‚Äù passengers have a higher survival rate than ‚Äúnot C‚Äù passengers\n\n\n\n\n\n\n\n\n\n\n\n\nWhy not encode as a single feature?\n\nPretend:\n\nC: high survival rate\nQ: low survival rate\nS: high survival rate\n\nSingle feature would need two coefficients:\n\nNegative coefficient for impact of Q (with respect to C)\nPositive coefficient for impact of S (with respect to Q)",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Encoding categorical features</span>"
    ]
  },
  {
    "objectID": "ch03.html#transformer-methods-fit-transform-fit_transform",
    "href": "ch03.html#transformer-methods-fit-transform-fit_transform",
    "title": "3¬† Encoding categorical features",
    "section": "3.2 Transformer methods: fit, transform, fit_transform",
    "text": "3.2 Transformer methods: fit, transform, fit_transform\nLet‚Äôs discuss the fit_transform method, since that‚Äôs the method we used with OneHotEncoder to encode the Embarked feature.\nOneHotEncoder is known as a transformer, meaning its role is to perform data transformations. Transformers usually have a fit method and always have a transform method. The fit method is when the transformer learns something, and the transform method is when it uses what it learned to do the data transformation.\n\n\n\n\n\n\nGeneric transformer methods:\n\nfit: Transformer learns something\ntransform: Transformer uses what it learned to do the data transformation\n\n\n\n\nUsing OneHotEncoder as an example, the fit method is when it learns the categories from the data in alphabetical order, and the transform method is when it creates the feature matrix using those categories.\n\n\n\n\n\n\nOneHotEncoder methods:\n\nfit: Learn the categories\ntransform: Create the feature matrix using those categories\n\n\n\n\nThe fit_transform method, which is what we used above, just combines those two steps into a single method call. You can actually do those steps as two separate calls of fit then transform, but the single method call of fit_transform is better because it‚Äôs more computationally efficient and also more readable in my opinion.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Encoding categorical features</span>"
    ]
  },
  {
    "objectID": "ch03.html#one-hot-encoding-of-multiple-features",
    "href": "ch03.html#one-hot-encoding-of-multiple-features",
    "title": "3¬† Encoding categorical features",
    "section": "3.3 One-hot encoding of multiple features",
    "text": "3.3 One-hot encoding of multiple features\nWe saw how to use OneHotEncoder to encode the Embarked column, but we actually need to encode both Embarked and Sex. Thankfully, OneHotEncoder can be applied to multiple features at once.\nTo do this, we simply pass a two-column DataFrame to the fit_transform method, whereas previously we had passed a one-column DataFrame. It outputs 5 columns, in which the first 3 columns represent Embarked and the last 2 columns represent Sex.\n\nohe.fit_transform(df[['Embarked', 'Sex']])\n\narray([[0., 0., 1., 0., 1.],\n       [1., 0., 0., 1., 0.],\n       [0., 0., 1., 1., 0.],\n       [0., 0., 1., 1., 0.],\n       [0., 0., 1., 0., 1.],\n       [0., 1., 0., 0., 1.],\n       [0., 0., 1., 0., 1.],\n       [0., 0., 1., 0., 1.],\n       [0., 0., 1., 1., 0.],\n       [1., 0., 0., 1., 0.]])\n\n\nLooking at the categories_ attribute, we first see the 3 categories that were learned from Embarked in alphabetical order, and then we see the 2 categories that were learned from Sex in alphabetical order. Thus, we know that a 10 in the last two columns means ‚Äúfemale‚Äù, and a 01 in the last two columns means ‚Äúmale‚Äù.\n\nohe.categories_\n\n[array(['C', 'Q', 'S'], dtype=object), array(['female', 'male'], dtype=object)]\n\n\nSo for example, the first sample in the output array is 00101, which means they embarked from S and they are male. The second sample in the array is 10010, which means they embarked from C and they are female. And so on.\n\n\n\n\n\n\nDecoding the output array:\n\nFirst three columns:\n\n1, 0, 0 means ‚ÄúC‚Äù\n0, 1, 0 means ‚ÄúQ‚Äù\n0, 0, 1 means ‚ÄúS‚Äù\n\nLast two columns:\n\n1, 0 means ‚Äúfemale‚Äù\n0, 1 means ‚Äúmale‚Äù\n\nExample:\n\n0, 0, 1, 0, 1 means ‚ÄúS, male‚Äù\n1, 0, 0, 1, 0 means ‚ÄúC, female‚Äù\n\n\n\n\n\nRecall that our goal in this chapter was to numerically encode Embarked and Sex so we could include them in our model along with Parch and Fare. How might we do that?\nOne idea would be to manually stack Parch and Fare side-by-side with the 5 columns output by OneHotEncoder, and then train the model using all 7 columns. However, we would need to repeat the same exact process of encoding and stacking with the new data, since if you train a model with 7 features, you need the same 7 features in the new data in order to make predictions.\n\n\n\n\n\n\nHow to manually add Embarked and Sex to the model:\n\nStack Parch and Fare side-by-side with OneHotEncoder output\nRepeat the same process with new data\n\n\n\n\nThis process would work, but it‚Äôs less than ideal, since repeating the same steps twice is both inefficient and error-prone. Not only that, but the complexity of this process will continue to increase as you preprocess additional features.\n\n\n\n\n\n\nProblems with a manual approach:\n\nRepeating steps is inefficient and error-prone\nComplexity will increase\n\n\n\n\nIn the next chapter, I‚Äôll introduce you to the ColumnTransformer and Pipeline classes. We‚Äôll use these two classes to accomplish our goal of adding Embarked and Sex to our model, but we‚Äôll do it in a way that is both reliable and efficient.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Encoding categorical features</span>"
    ]
  },
  {
    "objectID": "ch03.html#qa-when-should-i-use-transform-instead-of-fit_transform",
    "href": "ch03.html#qa-when-should-i-use-transform-instead-of-fit_transform",
    "title": "3¬† Encoding categorical features",
    "section": "3.4 Q&A: When should I use transform instead of fit_transform?",
    "text": "3.4 Q&A: When should I use transform instead of fit_transform?\nEarlier in this chapter, we used the fit_transform method of OneHotEncoder to encode two categorical features. In this lesson, I‚Äôll show you when it‚Äôs appropriate to just use the transform method instead of fit_transform. The example below will use OneHotEncoder, but the principles I‚Äôm teaching here apply the same way to all transformers.\nWe‚Äôll start by creating a DataFrame of training data with just 1 categorical feature.\n\ndemo_train = pd.DataFrame({'letter':['A', 'B', 'C', 'B']})\ndemo_train\n\n\n\n\n\n\n\n\nletter\n\n\n\n\n0\nA\n\n\n1\nB\n\n\n2\nC\n\n\n3\nB\n\n\n\n\n\n\n\nLet‚Äôs run fit_transform on the entire DataFrame.\n\nohe.fit_transform(demo_train)\n\narray([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.],\n       [0., 1., 0.]])\n\n\nRecall that fit_transform is really 2 steps. During the first step, which is fit, the OneHotEncoder learns the 3 categories. During the second step, which is transform, the OneHotEncoder creates the feature matrix using those categories. It outputs a 4 by 3 array, since there are 4 samples and 3 categories.\n\n\n\n\n\n\nExample of fit_transform on training data:\n\nfit: Learn 3 categories (A, B, C)\ntransform: Create feature matrix with 3 columns\n\n\n\n\nNow, we‚Äôll create a DataFrame of testing data. It contains the same feature, but that feature includes one less category.\n\ndemo_test = pd.DataFrame({'letter':['A', 'C', 'A']})\ndemo_test\n\n\n\n\n\n\n\n\nletter\n\n\n\n\n0\nA\n\n\n1\nC\n\n\n2\nA\n\n\n\n\n\n\n\nWhat would happen if we ran fit_transform on the testing data?\n\nohe.fit_transform(demo_test)\n\narray([[1., 0.],\n       [0., 1.],\n       [1., 0.]])\n\n\nThe output array only includes two columns, because the testing data only included two categories. The first column represents the A category, and the second column represents the C category.\nThis is problematic, because if we trained a model using the 3-column feature matrix, and then tried to make predictions on the 2-column feature matrix, it would error due to a shape mismatch. That makes sense because if you train a model such as logistic regression using 3 features, it will learn 3 coefficients, and it expects to use all 3 of those coefficients when making predictions.\n\n\n\n\n\n\nExample of fit_transform on testing data:\n\nfit: Learn 2 categories (A, C)\ntransform: Create feature matrix with 2 columns\n\n\n\n\nThe solution is to run fit_transform on the training data, and only run transform on the testing data. Let‚Äôs take a look at the output arrays.\nNotice that the categories are represented the same way in both arrays: the first column represents A, the second column represents B, and the third column represents C.\n\nohe.fit_transform(demo_train)\n\narray([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.],\n       [0., 1., 0.]])\n\n\n\nohe.transform(demo_test)\n\narray([[1., 0., 0.],\n       [0., 0., 1.],\n       [1., 0., 0.]])\n\n\nThis happened because we only ran the fit method once, on the training data, and the fit method is when the OneHotEncoder learns the categories.\nThen we ran the transform method twice, both on the training data and the testing data. Because we didn‚Äôt run the fit method on the testing data, the categories learned from the training data were applied to both the training and testing data. This is critically important because it means that both our training and testing feature matrices have 3 columns, and those 3 columns mean the same thing.\n\n\n\n\n\n\nCorrect process:\n\nRun fit_transform on training data:\n\nfit: Learn 3 categories (A, B, C)\ntransform: Create feature matrix with 3 columns\n\nRun transform on testing data:\n\ntransform: Create feature matrix with 3 columns\n\n\n\n\n\nIn summary, when using any transformer, you will always use the fit_transform method on the training data and only the transform method on the testing data.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Encoding categorical features</span>"
    ]
  },
  {
    "objectID": "ch03.html#sec-3-5",
    "href": "ch03.html#sec-3-5",
    "title": "3¬† Encoding categorical features",
    "section": "3.5 Q&A: What happens if the testing data includes a new category?",
    "text": "3.5 Q&A: What happens if the testing data includes a new category?\nIn the previous lesson, we created this example DataFrame of training data.\n\ndemo_train\n\n\n\n\n\n\n\n\nletter\n\n\n\n\n0\nA\n\n\n1\nB\n\n\n2\nC\n\n\n3\nB\n\n\n\n\n\n\n\nWhen we passed that DataFrame to fit_transform, the output array included three columns.\n\nohe.fit_transform(demo_train)\n\narray([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.],\n       [0., 1., 0.]])\n\n\nWe know from the categories_ attribute that those columns represent the categories A, B, and C.\n\nohe.categories_\n\n[array(['A', 'B', 'C'], dtype=object)]\n\n\nNow we‚Äôll create a new DataFrame of testing data that includes a category, D, which was not seen in the training data.\n\ndemo_test_unknown = pd.DataFrame({'letter':['A', 'C', 'D']})\ndemo_test_unknown\n\n\n\n\n\n\n\n\nletter\n\n\n\n\n0\nA\n\n\n1\nC\n\n\n2\nD\n\n\n\n\n\n\n\nIf you pass this new DataFrame to the transform method, it will throw an error because it doesn‚Äôt know how to represent the D category. It only knows how to represent the A, B, and C categories because those are the ones that were seen by the OneHotEncoder during the fit step.\n\nohe.transform(demo_test_unknown)\n\n\n\nValueError: Found unknown categories ['D'] in column 0 during transform\n\n\nThere are two possible solutions to this problem.\nThe first solution is to specify the categories manually to the OneHotEncoder when creating an instance.\n\nohe = OneHotEncoder(sparse=False, categories=[['A', 'B', 'C', 'D']])\n\nThen, when fit_transform is run on the training data, a column is reserved for each of the four categories.\n\nohe.fit_transform(demo_train)\n\narray([[1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [0., 0., 1., 0.],\n       [0., 1., 0., 0.]])\n\n\nAs a result, the transform on the testing data will no longer error.\n\nohe.transform(demo_test_unknown)\n\narray([[1., 0., 0., 0.],\n       [0., 0., 1., 0.],\n       [0., 0., 0., 1.]])\n\n\nHowever, specifying the categories manually is only a useful solution if you know all possible categories that might ever appear in your data. But in the real world, you don‚Äôt always know the full set of categories ahead of time.\nFor example, there might be rare categories that aren‚Äôt present in your set of samples, or new categories might be added in the future. For example, if one of your categorical features was medical billing codes, you could imagine that new billing codes are added over time.\n\n\n\n\n\n\nWhy you might not know all possible categories:\n\nRare categories aren‚Äôt present in your set of samples\nNew categories are added later\n\n\n\n\nIf you don‚Äôt know all possible categories, then the solution is to set the handle_unknown parameter of the OneHotEncoder to 'ignore', which overrides the default value of 'error'.\n\nohe = OneHotEncoder(sparse=False, handle_unknown='ignore')\n\nLet‚Äôs use the fit_transform method on our training data one more time. The output array includes three columns representing A, B, and C.\n\nohe.fit_transform(demo_train)\n\narray([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.],\n       [0., 1., 0.]])\n\n\nNow, when you use the transform method on the testing data, the third sample is encoded as all zeros because D is an unknown category.\n\nohe.transform(demo_test_unknown)\n\narray([[1., 0., 0.],\n       [0., 0., 1.],\n       [0., 0., 0.]])\n\n\nAlthough this might seem strange, this is actually quite a reasonable approach since you don‚Äôt have any information from the training data about the relationship between the D category and the target value.\nOne limitation of this approach, however, is that all unknown categories will be encoded the same way, which means that an E value in the testing data would also be encoded as all zeros.\nHere‚Äôs my overall advice:\n\nWhen starting a project, keep the handle_unknown parameter set to its default value of 'error' so that you will know if you are encountering new categories in your testing data.\nIf you do find that you‚Äôre encountering new categories, but you can determine the full set of categories through research, then define the categories manually when creating the OneHotEncoder instance.\nIf you can‚Äôt determine the full set of categories, then set the handle_unknown parameter to 'ignore'. However, you should retrain your model as soon as possible using data that includes those new categories.\n\n\n\n\n\n\n\nAdvice for OneHotEncoder:\n\nStart with handle_unknown set to 'error'\nIf possible, specify the categories manually\nIf necessary, set handle_unknown to 'ignore' and then retrain your model",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Encoding categorical features</span>"
    ]
  },
  {
    "objectID": "ch03.html#sec-3-6",
    "href": "ch03.html#sec-3-6",
    "title": "3¬† Encoding categorical features",
    "section": "3.6 Q&A: Should I drop one of the one-hot encoded categories?",
    "text": "3.6 Q&A: Should I drop one of the one-hot encoded categories?\nHere‚Äôs the example training data that we‚Äôve used in the past few lessons.\n\ndemo_train\n\n\n\n\n\n\n\n\nletter\n\n\n\n\n0\nA\n\n\n1\nB\n\n\n2\nC\n\n\n3\nB\n\n\n\n\n\n\n\nAnd here‚Äôs the default one-hot encoding of this DataFrame.\n\nohe.fit_transform(demo_train)\n\narray([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.],\n       [0., 1., 0.]])\n\n\nWhen one-hot encoding, it‚Äôs somewhat common to drop the first column of the output array because it contains redundant information and because it avoids collinearity between features.\n\n\n\n\n\n\nYou can drop the first column:\n\nContains redundant information\nAvoids collinearity between features\n\n\n\n\nIf you want to drop the first column, you can set the OneHotEncoder‚Äôs drop parameter to 'first', though this option only exists in scikit-learn version 0.21 and later. When you run the fit_transform, you can see that the output array contains 1 less column. However, the new encoding retains the same information, since each category is still represented by a unique code: 00 means ‚ÄúA‚Äù, 10 means ‚ÄúB‚Äù, and 01 means ‚ÄúC‚Äù.\n\nohe = OneHotEncoder(sparse=False, drop='first')\nohe.fit_transform(demo_train)\n\narray([[0., 0.],\n       [1., 0.],\n       [0., 1.],\n       [1., 0.]])\n\n\n\n\n\n\n\n\nDecoding the output array (after dropping the first column):\n\n0, 0 means ‚ÄúA‚Äù\n1, 0 means ‚ÄúB‚Äù\n0, 1 means ‚ÄúC‚Äù\n\n\n\n\nDropping the first column will work regardless of the number of categories, but you‚Äôre only ever allowed to drop a single column. And it doesn‚Äôt actually matter which column you drop, though the convention is to drop the first column.\nYou‚Äôve now seen that you can drop the first column, but the question is, should you drop the first column? Here‚Äôs my advice.\nIf you know that perfectly collinear features will cause problems, such as when feeding the resulting data into a neural network or an unregularized regression, then it‚Äôs a good idea to drop the first column. However, for most scikit-learn models, perfectly collinear features will not cause any problems, and thus dropping the first column will not benefit the model.\nThere are also some significant downsides to dropping the first column that you need to be aware of:\n\nDropping the first column is incompatible with ignoring unknown categories, which is the handle_unknown='ignore' option that we saw in the previous lesson, since the dropped category and unknown categories would both be encoded as all zeros. You are allowed to do this starting in scikit-learn 1.0, but I still don‚Äôt recommend it.\nDropping the first column can introduce bias into the model if you standardize your features, such as with StandardScaler, or if you use a regularized model, such as logistic regression, since the dropped category will be exempt from standardization and regularization.\n\n\n\n\n\n\n\nShould you drop the first column?\n\nAdvantages:\n\nUseful if perfectly collinear features will cause problems (does not apply to most models)\n\nDisadvantages:\n\nIncompatible with handle_unknown='ignore'\nIntroduces bias if you standardize features or use a regularized model\n\n\n\n\n\nIn summary, I recommend that you drop the first column only if you know that perfectly collinear features will cause problems, otherwise I don‚Äôt recommend dropping the first column.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Encoding categorical features</span>"
    ]
  },
  {
    "objectID": "ch03.html#qa-how-do-i-encode-an-ordinal-feature",
    "href": "ch03.html#qa-how-do-i-encode-an-ordinal-feature",
    "title": "3¬† Encoding categorical features",
    "section": "3.7 Q&A: How do I encode an ordinal feature?",
    "text": "3.7 Q&A: How do I encode an ordinal feature?\nThroughout this chapter, we used one-hot encoding to encode unordered categorical features, also known as nominal data. But how should you encode categorical features with an inherent logical ordering, also known as ordinal data? That‚Äôs the subject of this lesson.\n\n\n\n\n\n\nTypes of categorical data:\n\nUnordered (nominal data)\nOrdered (ordinal data)\n\n\n\n\nLet‚Äôs take a look at our Titanic DataFrame.\nPclass, which stands for passenger class, is an ordinal feature. Although it‚Äôs already numeric, the numbers 1, 2, and 3 represent the categories 1st class, 2nd class, and 3rd class. It‚Äôs considered ordinal data because there is a logical ordering to the categories.\nOur intuition is that there may be a relationship between Pclass values increasing and survival rate decreasing, because passengers in the lower-numbered classes may have gotten priority access to lifeboats.\n\ndf\n\n\n\n\n\n\n\n\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n5\n0\n3\nMoran, Mr. James\nmale\nNaN\n0\n0\n330877\n8.4583\nNaN\nQ\n\n\n6\n0\n1\nMcCarthy, Mr. Timothy J\nmale\n54.0\n0\n0\n17463\n51.8625\nE46\nS\n\n\n7\n0\n3\nPalsson, Master. Gosta Leonard\nmale\n2.0\n3\n1\n349909\n21.0750\nNaN\nS\n\n\n8\n1\n3\nJohnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)\nfemale\n27.0\n0\n2\n347742\n11.1333\nNaN\nS\n\n\n9\n1\n2\nNasser, Mrs. Nicholas (Adele Achem)\nfemale\n14.0\n1\n0\n237736\n30.0708\nNaN\nC\n\n\n\n\n\n\n\nThus if we were going to include Pclass in the model, we would keep the existing numeric encoding so that the model can learn the relationship between Pclass and Survived with a single feature. You could use one-hot encoding with Pclass instead, but the model wouldn‚Äôt be able to learn that relationship as effectively because that information would be spread out across three features.\n\n\n\n\n\n\nOptions for encoding Pclass:\n\nOrdinal encoding: Creates one feature\nOne-hot encoding: Creates three features\n\n\n\n\nLet‚Äôs create an example DataFrame to see how to handle ordinal features that are stored as strings.\nIn this DataFrame, we have two ordinal features, Class and Size.\n\ndf_ordinal = pd.DataFrame({'Class': ['third', 'first', 'second',\n                                     'third'],\n                           'Size': ['S', 'S', 'L', 'XL']})\ndf_ordinal\n\n\n\n\n\n\n\n\nClass\nSize\n\n\n\n\n0\nthird\nS\n\n\n1\nfirst\nS\n\n\n2\nsecond\nL\n\n\n3\nthird\nXL\n\n\n\n\n\n\n\nIf you have ordinal data, you should use the OrdinalEncoder class to do the encoding. First, you import it from the preprocessing module. Then, you create an instance of OrdinalEncoder, and when you do so, you define the logical order of the categories.\nWe pass a list of lists to the categories parameter, in which the first inner list is the categories for the Class feature, and the second inner list is the categories for the Size feature. I put the two lists in that order because that is the order in which I‚Äôll be passing the features to the fit_transform method.\nOne important note is that I included the ‚ÄúM‚Äù category for Size even though it wasn‚Äôt present in this DataFrame because I knew that it would occur in the dataset at some point.\n\nfrom sklearn.preprocessing import OrdinalEncoder\noe = OrdinalEncoder(categories=[['first', 'second', 'third'],\n                                ['S', 'M', 'L', 'XL']])\n\nNext, we pass the DataFrame to the OrdinalEncoder‚Äôs fit_transform method in order to do the encoding. You‚Äôll notice that each input feature became a single feature in the output array.\n\noe.fit_transform(df_ordinal)\n\narray([[2., 0.],\n       [0., 0.],\n       [1., 2.],\n       [2., 3.]])\n\n\nFor the Class feature, ‚Äúfirst‚Äù was encoded as 0, ‚Äúsecond‚Äù was encoded as 1, and ‚Äúthird‚Äù was encoded as 2.\nFor the Size feature, ‚ÄúS‚Äù was encoded as 0, ‚ÄúL‚Äù was encoded as 2, and ‚ÄúXL‚Äù was encoded as 3. And if ‚ÄúM‚Äù appears in the data at some point, it will be encoded as 1.\nAgain, we encoded each input feature as a single column so that the model can learn the relationship between the target and an increase or decrease in each feature.\n\n\n\n\n\n\nDecoding the output array:\n\nFirst column:\n\n0 means ‚Äúfirst‚Äù\n1 means ‚Äúsecond‚Äù\n2 means ‚Äúthird‚Äù\n\nSecond column:\n\n0 means ‚ÄúS‚Äù\n1 means ‚ÄúM‚Äù\n2 means ‚ÄúL‚Äù\n3 means ‚ÄúXL‚Äù\n\nExample:\n\n2, 0 means ‚Äúthird, S‚Äù\n\n\n\n\n\nLet‚Äôs briefly contrast this with the output you would get if you used OneHotEncoder with these same two features.\nOneHotEncoder would create 7 columns in the output array, since Class has 3 categories and Size has 4 categories. These 7 columns contain the same information as the 2 columns output by OrdinalEncoder, but the model would have a comparatively harder time learning from the 7 columns since the information is expressed in a less compact form.\n\nohe = OneHotEncoder(sparse=False,\n                    categories=[['first', 'second', 'third'],\n                                ['S', 'M', 'L', 'XL']])\nohe.fit_transform(df_ordinal)\n\narray([[0., 0., 1., 1., 0., 0., 0.],\n       [1., 0., 0., 1., 0., 0., 0.],\n       [0., 1., 0., 0., 0., 1., 0.],\n       [0., 0., 1., 0., 0., 0., 1.]])\n\n\nHere‚Äôs a summary of my advice on this topic:\n\nIf you have an ordinal feature that‚Äôs already encoded numerically, then leave it as-is.\nIf you have an ordinal feature that‚Äôs stored as strings, then encode it using OrdinalEncoder.\nIf you have a nominal feature, then encode it using OneHotEncoder.\n\n\n\n\n\n\n\nAdvice for encoding categorical data:\n\nOrdinal feature stored as numbers: Leave as-is\nOrdinal feature stored as strings: Use OrdinalEncoder\nNominal feature: Use OneHotEncoder\n\n\n\n\nIn chapter 17, we‚Äôll explore this topic further and see if there are cases in which you should diverge from this advice.\nOne final note about OrdinalEncoder is that unlike OneHotEncoder, it does not allow for new categories in the testing data that were not seen during training. However, that functionality is available beginning in scikit-learn version 0.24 using a handle_unknown parameter.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Encoding categorical features</span>"
    ]
  },
  {
    "objectID": "ch03.html#qa-whats-the-difference-between-ordinalencoder-and-labelencoder",
    "href": "ch03.html#qa-whats-the-difference-between-ordinalencoder-and-labelencoder",
    "title": "3¬† Encoding categorical features",
    "section": "3.8 Q&A: What‚Äôs the difference between OrdinalEncoder and LabelEncoder?",
    "text": "3.8 Q&A: What‚Äôs the difference between OrdinalEncoder and LabelEncoder?\nThere are many similarities between the OrdinalEncoder and LabelEncoder classes, so in this lesson I‚Äôll explain how they‚Äôre different and why you should be using OrdinalEncoder, not LabelEncoder.\nThe first main difference is that OrdinalEncoder allows you to define the order of the categories, whereas LabelEncoder does not. LabelEncoder simply uses the alphabetical order of the values you pass to it to determine which value to encode as 0, which value to encode as 1, and so on.\nThe second main difference is that OrdinalEncoder can be used to encode multiple features at once, whereas LabelEncoder can only encode one column of data at once.\n\n\n\n\n\n\n\n\n\n¬†\nOrdinalEncoder\nLabelEncoder\n\n\n\n\nCan you define the category order?\nYes\nNo\n\n\nCan you encode multiple features?\nYes\nNo\n\n\n\n\n\n\nBecause of these differences, OrdinalEncoder is much better suited than LabelEncoder for encoding ordinal features. And in fact, LabelEncoder is only intended for the encoding of class labels, hence its name.\nYou might be asking why LabelEncoder even exists, given its limitations. There are two reasons:\nFirst, in older versions of scikit-learn, some classification models were not able to handle string-based labels. LabelEncoder was used to encode those strings as integers so that they could be passed to the model. That limitation was removed a few years ago, and so all scikit-learn classifiers can now handle string-based labels. Therefore, you should never need to use LabelEncoder for encoding your class labels.\nSecond, also in order versions of scikit-learn, OneHotEncoder did not accept strings as input. Thus if you had categorical data stored as strings, you actually had to use LabelEncoder to encode the strings as integers before passing them to the OneHotEncoder. Again, that limitation was removed a few years ago, and so you can pass string-based categorical data directly to OneHotEncoder.\n\n\n\n\n\n\nOutdated uses for LabelEncoder:\n\nEncoding string-based labels for some classifiers\nEncoding string-based features for OneHotEncoder\n\n\n\n\nBecause of this legacy from older versions of scikit-learn, many people are familiar with LabelEncoder and thus use it to encode features. However, the best practice is to use OrdinalEncoder to encode ordinal features. In fact, it‚Äôs rare that you will ever need to use LabelEncoder, which is why I‚Äôm not using it in this book.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Encoding categorical features</span>"
    ]
  },
  {
    "objectID": "ch03.html#qa-should-i-encode-numeric-features-as-ordinal-features",
    "href": "ch03.html#qa-should-i-encode-numeric-features-as-ordinal-features",
    "title": "3¬† Encoding categorical features",
    "section": "3.9 Q&A: Should I encode numeric features as ordinal features?",
    "text": "3.9 Q&A: Should I encode numeric features as ordinal features?\nNormally, when you have a continuous numeric feature such as Fare, you pass that feature directly to your Machine Learning model.\n\ndf[['Fare']]\n\n\n\n\n\n\n\n\nFare\n\n\n\n\n0\n7.2500\n\n\n1\n71.2833\n\n\n2\n7.9250\n\n\n3\n53.1000\n\n\n4\n8.0500\n\n\n5\n8.4583\n\n\n6\n51.8625\n\n\n7\n21.0750\n\n\n8\n11.1333\n\n\n9\n30.0708\n\n\n\n\n\n\n\nHowever, one strategy that is sometimes used with numeric features is to ‚Äúdiscretize‚Äù or ‚Äúbin‚Äù them into categorical features. In scikit-learn, we can do this using KBinsDiscretizer.\n\nfrom sklearn.preprocessing import KBinsDiscretizer\n\nWhen creating an instance of KBinsDiscretizer, you define the number of bins, the binning strategy, and the method used for encoding the result.\n\nkb = KBinsDiscretizer(n_bins=3, strategy='quantile', encode='ordinal')\n\nHere‚Äôs the output when we pass the Fare feature to the fit_transform method.\nBecause we specified 3 bins, every sample has been assigned to bin 0 or 1 or 2. The smallest values were assigned to bin 0, the largest values were assigned to bin 2, and the values in between were assigned to bin 1. Thus, we‚Äôve taken a continuous numeric feature and encoded it as an ordinal feature, and this ordinal feature could be passed to the model in place of the numeric feature.\n\nkb.fit_transform(df[['Fare']])\n\narray([[0.],\n       [2.],\n       [0.],\n       [2.],\n       [0.],\n       [1.],\n       [2.],\n       [1.],\n       [1.],\n       [2.]])\n\n\nThe obvious follow-up question is: Should we discretize our numeric features? Theoretically, discretization can benefit linear models by helping them to learn non-linear trends. However, my general recommendation is to not use discretization, for three main reasons:\nFirst, discretization removes all nuance from the data, which makes it harder for a model to learn the actual trends that are present in the data.\nSecond, discretization reduces the variation in the data, which makes it easier to find trends that don‚Äôt actually exist.\nThird, any possible benefits of discretization are highly dependent on the parameters used with KBinsDiscretizer. Making those decisions by hand creates a risk of overfitting the training data, and making those decisions during a tuning process adds both complexity and processing time, and so neither of those options is particularly attractive to me.\n\n\n\n\n\n\nWhy not discretize numeric features?\n\nMakes it harder to learn the actual trends\nMakes it easier to discover non-existent trends\nMay result in overfitting",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Encoding categorical features</span>"
    ]
  },
  {
    "objectID": "ch04.html",
    "href": "ch04.html",
    "title": "4¬† Improving your workflow with ColumnTransformer and Pipeline",
    "section": "",
    "text": "4.1 Preprocessing features with ColumnTransformer\nIn the last chapter, our goal was to include two numeric features and two categorical features in our model. We saw how to numerically encode the categorical features using OneHotEncoder, but we lacked an efficient process for stacking those encoded features next to the numerical features, and we lacked an efficient way to apply this same preprocessing to our new data.\nIn this chapter, we‚Äôre going to solve both of those problems using the ColumnTransformer and Pipeline classes:\nTo start, we‚Äôll create a Python list of the four columns we‚Äôve been working with, and use that to create our X object.\ncols = ['Parch', 'Fare', 'Embarked', 'Sex']\nX = df[cols]\nX\n\n\n\n\n\n\n\n\nParch\nFare\nEmbarked\nSex\n\n\n\n\n0\n0\n7.2500\nS\nmale\n\n\n1\n0\n71.2833\nC\nfemale\n\n\n2\n0\n7.9250\nS\nfemale\n\n\n3\n0\n53.1000\nS\nfemale\n\n\n4\n0\n8.0500\nS\nmale\n\n\n5\n0\n8.4583\nQ\nmale\n\n\n6\n0\n51.8625\nS\nmale\n\n\n7\n1\n21.0750\nS\nmale\n\n\n8\n2\n11.1333\nS\nfemale\n\n\n9\n0\n30.0708\nC\nfemale\nWe‚Äôre still going to be one-hot encoding the Embarked and Sex columns, so we‚Äôll create an instance of OneHotEncoder. We‚Äôre using the default options for OneHotEncoder, which means it will output a sparse matrix, but that‚Äôs fine because we‚Äôre not going to examine the output directly.\nohe = OneHotEncoder()\nNow it‚Äôs time to create our first ColumnTransformer, which will take care of any data transformations that we specify. We‚Äôll start by importing the make_column_transformer function from the compose module.\nIn general, you use make_column_transformer by passing it one or more tuples, and each tuple should have two elements:\nIn our case, we‚Äôll pass it a single tuple in which the first element is our OneHotEncoder object and the second element is a list of the two columns we want to one-hot encode.\nAfter all tuples, we‚Äôll set the remainder parameter to 'drop', which means that all columns which are not explicitly mentioned in the ColumnTransformer should be dropped. 'drop' is actually the default value for remainder, but I‚Äôm including it here just for clarity.\nNote that I could have defined the ColumnTransformer on a single line, but I prefer breaking the lines in this way for readability.\nWhen we run this code, the make_column_transformer code returns a ColumnTransformer object, which we‚Äôll save as ct.\nfrom sklearn.compose import make_column_transformer\nct = make_column_transformer(\n    (ohe, ['Embarked', 'Sex']),\n    remainder='drop')\nNext, we‚Äôll perform the transformation by passing X, which is our four-column DataFrame, to the fit_transform method of the ct object. It outputs a 10 by 5 array that represents the one-hot encoding of the Embarked and Sex columns. The first three columns represent Embarked and the other two columns represent Sex, and they‚Äôre in that order because that‚Äôs the order in which they were listed in the ColumnTransformer.\nNote that even though the Parch and Fare columns are part of X, they‚Äôre excluded from the output array because we told the ColumnTransformer to drop all unspecified columns.\nct.fit_transform(X)\n\narray([[0., 0., 1., 0., 1.],\n       [1., 0., 0., 1., 0.],\n       [0., 0., 1., 1., 0.],\n       [0., 0., 1., 1., 0.],\n       [0., 0., 1., 0., 1.],\n       [0., 1., 0., 0., 1.],\n       [0., 0., 1., 0., 1.],\n       [0., 0., 1., 0., 1.],\n       [0., 0., 1., 1., 0.],\n       [1., 0., 0., 1., 0.]])\nThis is nice, but our actual goal was to create a matrix that includes the Parch and Fare columns alongside the encoded versions of Embarked and Sex. To accomplish that, we‚Äôll simply change the value of remainder from 'drop' to 'passthrough'. This means that all columns which are not mentioned in the ColumnTransformer should be passed through to the output unmodified. In other words, include the Parch and Fare columns in the output, but don‚Äôt transform them in any way.\nct = make_column_transformer(\n    (ohe, ['Embarked', 'Sex']),\n    remainder='passthrough')\nWhen we run the fit_transform method this time, it outputs a 10 by 7 array. The first five columns represent the encoded Embarked and Sex columns, and the sixth and seventh columns are the Parch and Fare columns. The column order is based on the order in which the columns were listed in the ColumnTransformer, followed by any you pass through.\nct.fit_transform(X)\n\narray([[ 0.    ,  0.    ,  1.    ,  0.    ,  1.    ,  0.    ,  7.25  ],\n       [ 1.    ,  0.    ,  0.    ,  1.    ,  0.    ,  0.    , 71.2833],\n       [ 0.    ,  0.    ,  1.    ,  1.    ,  0.    ,  0.    ,  7.925 ],\n       [ 0.    ,  0.    ,  1.    ,  1.    ,  0.    ,  0.    , 53.1   ],\n       [ 0.    ,  0.    ,  1.    ,  0.    ,  1.    ,  0.    ,  8.05  ],\n       [ 0.    ,  1.    ,  0.    ,  0.    ,  1.    ,  0.    ,  8.4583],\n       [ 0.    ,  0.    ,  1.    ,  0.    ,  1.    ,  0.    , 51.8625],\n       [ 0.    ,  0.    ,  1.    ,  0.    ,  1.    ,  1.    , 21.075 ],\n       [ 0.    ,  0.    ,  1.    ,  1.    ,  0.    ,  2.    , 11.1333],\n       [ 1.    ,  0.    ,  0.    ,  1.    ,  0.    ,  0.    , 30.0708]])\nWe were able to figure out on our own what each column represents, but you can also use the ColumnTransformer‚Äôs get_feature_names method to confirm the meanings of these 7 features. The x0 simply means feature 0 that was passed to the OneHotEncoder, and the x1 means feature 1.\nct.get_feature_names()\n\n['onehotencoder__x0_C',\n 'onehotencoder__x0_Q',\n 'onehotencoder__x0_S',\n 'onehotencoder__x1_female',\n 'onehotencoder__x1_male',\n 'Parch',\n 'Fare']\nBefore we move on, I have two quick asides about the get_feature_names method:\nTo wrap up this lesson, I want to show you one other way to specify this same ColumnTransformer.\nAs I mentioned before, make_column_transformer accepts tuples, and the first element of each tuple is usually a transformer object (like our ohe object). However, the first element of the tuple can also be the special strings 'drop' or 'passthrough', which tells the ColumnTransformer to drop or pass through specific columns.\nSo, we‚Äôre going to add a second tuple in which the transformer is the string 'passthrough', and we want to apply this passthrough transformer to the columns Parch and Fare. This ColumnTransformer will do the exact same thing as the previous one, but I actually prefer this notation any time I have a small number of passthrough columns, since it reminds me of which columns I‚Äôm passing through.\nIt‚Äôs still important to remember that the default value for the remainder parameter is 'drop', which means that any unspecified columns will be dropped, though we don‚Äôt have any unspecified columns in this case.\nct = make_column_transformer(\n    (ohe, ['Embarked', 'Sex']),\n    ('passthrough', ['Parch', 'Fare']))\nWe‚Äôll run the fit_transform method one more time, and you can see that it outputs the same 7 columns as before. And to be clear, this is the feature matrix that we will pass to our model.\nct.fit_transform(X)\n\narray([[ 0.    ,  0.    ,  1.    ,  0.    ,  1.    ,  0.    ,  7.25  ],\n       [ 1.    ,  0.    ,  0.    ,  1.    ,  0.    ,  0.    , 71.2833],\n       [ 0.    ,  0.    ,  1.    ,  1.    ,  0.    ,  0.    ,  7.925 ],\n       [ 0.    ,  0.    ,  1.    ,  1.    ,  0.    ,  0.    , 53.1   ],\n       [ 0.    ,  0.    ,  1.    ,  0.    ,  1.    ,  0.    ,  8.05  ],\n       [ 0.    ,  1.    ,  0.    ,  0.    ,  1.    ,  0.    ,  8.4583],\n       [ 0.    ,  0.    ,  1.    ,  0.    ,  1.    ,  0.    , 51.8625],\n       [ 0.    ,  0.    ,  1.    ,  0.    ,  1.    ,  1.    , 21.075 ],\n       [ 0.    ,  0.    ,  1.    ,  1.    ,  0.    ,  2.    , 11.1333],\n       [ 1.    ,  0.    ,  0.    ,  1.    ,  0.    ,  0.    , 30.0708]])",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Improving your workflow with ColumnTransformer and Pipeline</span>"
    ]
  },
  {
    "objectID": "ch04.html#preprocessing-features-with-columntransformer",
    "href": "ch04.html#preprocessing-features-with-columntransformer",
    "title": "4¬† Improving your workflow with ColumnTransformer and Pipeline",
    "section": "",
    "text": "Problems from Chapter 3:\n\nNeed to stack categorical features next to numerical features\nNeed to apply the same preprocessing to new data\n\n\n\n\n\n\nColumnTransformer will make it easy to apply different preprocessing steps to different columns.\nPipeline will make it easy to apply the same workflow to training data and new data.\n\n\n\n\n\n\n\nHow to solve those problems:\n\nColumnTransformer: Apply different preprocessing steps to different columns\nPipeline: Apply the same workflow to training data and new data\n\n\n\n\n\n\n\n\n\n\n\n\nThe first element is a transformer.\nThe second element is a list of columns to which that transformer should be applied. Note that in most cases, this element should be a list even if you are only specifying a single column.\n\n\n\n\n\n\n\n\n\n\n\n\nTuple elements for make_column_transformer:\n\nTransformer object\nList of columns to which the transformer should be applied\n\n\n\n\n\n\n\n\n\n\n\n\n\nOutput columns:\n\nColumns 1-3: Embarked\nColumns 4-5: Sex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOutput columns:\n\nColumns 1-3: Embarked\nColumns 4-5: Sex\nColumn 6: Parch\nColumn 7: Fare\n\n\n\n\n\n\n\n\nFirst, the get_feature_names method didn‚Äôt work with passthrough columns prior to scikit-learn version 0.23, so you‚Äôll get an error if you run the code with previous versions.\nSecond, the get_feature_names method has been replaced with a similar method called get_feature_names_out starting in scikit-learn 1.0.\n\n\n\n\n\n\n\nNotes about get_feature_names:\n\nBefore version 0.23: Didn‚Äôt work with passthrough columns\nStarting in version 1.0: Has been replaced with get_feature_names_out\n\n\n\n\n\n\n\n\n\n\n\n\nTuple elements for make_column_transformer (revised):\n\nTransformer object or 'drop' or 'passthrough'\nList of columns to which the transformer should be applied",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Improving your workflow with ColumnTransformer and Pipeline</span>"
    ]
  },
  {
    "objectID": "ch04.html#chaining-steps-with-pipeline",
    "href": "ch04.html#chaining-steps-with-pipeline",
    "title": "4¬† Improving your workflow with ColumnTransformer and Pipeline",
    "section": "4.2 Chaining steps with Pipeline",
    "text": "4.2 Chaining steps with Pipeline\nIn the previous lesson, we accomplished our first goal, which was to apply different preprocessing to different columns using ColumnTransformer. In this lesson, we‚Äôre moving on to our second goal, which is to apply the same workflow to training data and new data using the Pipeline class.\nA Pipeline is used to chain together sequential steps. In this case, we want to chain together two steps, namely data preprocessing followed by model building.\nWe‚Äôll start by importing the make_pipeline function. Then, we can create a Pipeline instance by passing it two objects: our ColumnTransformer instance for data preprocessing, and our LogisticRegression instance for model building. We‚Äôll save it as an object called pipe, which is a 2-step Pipeline.\n\nfrom sklearn.pipeline import make_pipeline\npipe = make_pipeline(ct, logreg)\n\n\n\n\n\n\n\nPipeline steps:\n\nData preprocessing with ColumnTransformer\nModel building with LogisticRegression\n\n\n\n\nYou might remember that back in Chapter 2, we used cross-validation to evaluate our model when it only included the Parch and Fare features. Now that we‚Äôve added the Embarked and Sex features, it would normally make sense to cross-validate the updated model to see whether the adding those features made our model better or worse. And in fact, you can (and should) cross-validate an entire Pipeline.\nHowever, any model evaluation procedure is highly unreliable with only 10 rows of data, and so any change in the cross-validated accuracy would be misleading. Thus we‚Äôre going to skip the cross-validation step for the moment, though we‚Äôll return to it in a later chapter once we‚Äôre using the full dataset.\nSince we‚Äôre skipping cross-validation, our next step is just to run the fit method on the Pipeline, and pass it X and y. Here‚Äôs what happens when we fit the Pipeline:\n\nFirst, it runs the ColumnTransformer step, meaning that it takes X, which is a 4-column DataFrame that contains both numbers and strings, and transforms it into the 7-column feature matrix that only includes numbers.\nSecond, it runs the LogisticRegression step, meaning that the model is fit to this 7-column feature matrix. In other words, it learns the relationship between those 7 features and the y values.\n\nNote that when you fit a Pipeline, it will actually print out the steps. You can see that step 1 is a ColumnTransformer that includes a OneHotEncoder and a passthrough transformer, and step 2 is a LogisticRegression model.\n\npipe.fit(X, y)\n\nPipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('onehotencoder',\n                                                  OneHotEncoder(),\n                                                  ['Embarked', 'Sex']),\n                                                 ('passthrough', 'passthrough',\n                                                  ['Parch', 'Fare'])])),\n                ('logisticregression',\n                 LogisticRegression(random_state=1, solver='liblinear'))])\n\n\n\n\n\n\n\n\nFitting the Pipeline:\n\nColumnTransformer converts X (4 columns) into a numeric feature matrix (7 columns)\nLogisticRegression model is fit to the feature matrix\n\n\n\n\nIn case it helps you to understand the Pipeline better, I‚Äôm going to show you what happens ‚Äúunder the hood‚Äù when you fit this Pipeline. To be clear, you should not actually write the following code, rather it is just for teaching purposes.\nFirst, X is transformed by the ColumnTransformer into X_t, which stands for ‚ÄúX transformed‚Äù. Second, the LogisticRegression model is fit on X_t and y.\n\nX_t = ct.fit_transform(X)\nlogreg.fit(X_t, y)\n\nLogisticRegression(random_state=1, solver='liblinear')\n\n\nAnd as you would expect, X has the shape 10 by 4, and X_t has the shape 10 by 7.\n\nprint(X.shape)\nprint(X_t.shape)\n\n(10, 4)\n(10, 7)",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Improving your workflow with ColumnTransformer and Pipeline</span>"
    ]
  },
  {
    "objectID": "ch04.html#using-the-pipeline-to-make-predictions",
    "href": "ch04.html#using-the-pipeline-to-make-predictions",
    "title": "4¬† Improving your workflow with ColumnTransformer and Pipeline",
    "section": "4.3 Using the Pipeline to make predictions",
    "text": "4.3 Using the Pipeline to make predictions\nNow that we‚Äôve fit our Pipeline, we want to use it to make predictions on new data.\nThe first step is to update the X_new DataFrame so that it contains the same columns as X. Recall that the cols object contains the names of our four columns, and so we can use it to select those four columns from the df_new DataFrame.\n\nX_new = df_new[cols]\nX_new\n\n\n\n\n\n\n\n\nParch\nFare\nEmbarked\nSex\n\n\n\n\n0\n0\n7.8292\nQ\nmale\n\n\n1\n0\n7.0000\nS\nfemale\n\n\n2\n0\n9.6875\nQ\nmale\n\n\n3\n0\n8.6625\nS\nmale\n\n\n4\n1\n12.2875\nS\nfemale\n\n\n5\n0\n9.2250\nS\nmale\n\n\n6\n0\n7.6292\nQ\nfemale\n\n\n7\n1\n29.0000\nS\nmale\n\n\n8\n0\n7.2292\nC\nfemale\n\n\n9\n0\n24.1500\nS\nmale\n\n\n\n\n\n\n\nNow, we can pass X_new to the Pipeline‚Äôs predict method to make predictions for these ten samples. When we run it, the Pipeline applies the same transformations to X_new that it applied to X, and the transformed version of X_new is passed to the fitted logistic regression model so that it can make predictions.\nIn other words, the Pipeline enabled us to accomplish our second goal, which is to apply the same workflow to training data and new data.\nAs a reminder, we can‚Äôt evaluate the accuracy of these ten predictions because we don‚Äôt know the true target values for X_new.\n\npipe.predict(X_new)\n\narray([0, 1, 0, 0, 1, 0, 1, 0, 1, 0])\n\n\n\n\n\n\n\n\nPredicting with the Pipeline:\n\nColumnTransformer applies the same transformations to X_new\nFitted LogisticRegression model makes predictions on the transformed version of X_new\n\n\n\n\nJust like before, I‚Äôm going to show you what happens ‚Äúunder the hood‚Äù when you make predictions using this Pipeline. Again, you should not actually write the following code, rather it is just for teaching purposes.\nFirst, X_new is transformed by the ColumnTransformer into X_new_t, which stands for ‚ÄúX_new transformed‚Äù. Second, the fitted LogisticRegression model makes predictions for the samples in X_new_t.\n\nX_new_t = ct.transform(X_new)\nlogreg.predict(X_new_t)\n\narray([0, 1, 0, 0, 1, 0, 1, 0, 1, 0])\n\n\nAnd as you would expect, X_new has the shape 10 by 4, and X_new_t has the shape 10 by 7.\n\nprint(X_new.shape)\nprint(X_new_t.shape)\n\n(10, 4)\n(10, 7)\n\n\nOne important point I want to highlight is that the Pipeline‚Äôs predict method called the ColumnTransformer‚Äôs transform method, not its fit_transform method. Why would that be?\nRecall that the fit step is when a transformer learns something, and the transform step is when it uses what it learned to do the transformation. Thus you fit on X to learn an encoding, and you transform on X and X_new to apply that encoding.\n\n\n\n\n\n\nColumnTransformer methods:\n\nRun fit_transform on X:\n\nfit: Learn the encoding\ntransform: Apply the encoding to create 7 columns\n\nRun transform on X_new:\n\ntransform: Apply the encoding to create 7 columns\n\n\n\n\n\nThis is critically important. Our logistic regression model was fit on 7 columns, and so it learned 7 coefficients. To make predictions, you need to pass 7 columns to the predict method, and those 7 columns need to mean the same thing as the 7 columns you used when fitting the model. Thus, the predict method only runs transform so that the exact same encoding will be applied to the training data and the new data.\nIt‚Äôs okay if you‚Äôre still a bit fuzzy on the difference between fit and transform, because the Pipeline object will just do the right thing for you when you run fit or predict. However, understanding the difference will ultimately help you to go further with scikit-learn.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Improving your workflow with ColumnTransformer and Pipeline</span>"
    ]
  },
  {
    "objectID": "ch04.html#qa-how-do-i-drop-some-columns-and-passthrough-others",
    "href": "ch04.html#qa-how-do-i-drop-some-columns-and-passthrough-others",
    "title": "4¬† Improving your workflow with ColumnTransformer and Pipeline",
    "section": "4.4 Q&A: How do I drop some columns and passthrough others?",
    "text": "4.4 Q&A: How do I drop some columns and passthrough others?\nCurrently we only have 4 columns in X, namely Parch, Fare, Embarked, and Sex. But imagine that we had many more columns, and we wanted to drop a few columns and pass through the rest. How would we do that efficiently?\nWe can use the special string 'drop' to tell the ColumnTransformer which columns to drop, and also tell it to pass through all remaining columns. So in this example, we‚Äôre one-hot encoding Embarked and Sex, which creates 5 columns, dropping Fare, and passing through Parch, which adds 1 more column.\nWe could use this same pattern to drop a few columns and pass through hundreds of columns without having to list the passthrough columns one-by-one.\n\nct = make_column_transformer(\n    (ohe, ['Embarked', 'Sex']),\n    ('drop', ['Fare']),\n    remainder='passthrough')\nct.fit_transform(X)\n\narray([[0., 0., 1., 0., 1., 0.],\n       [1., 0., 0., 1., 0., 0.],\n       [0., 0., 1., 1., 0., 0.],\n       [0., 0., 1., 1., 0., 0.],\n       [0., 0., 1., 0., 1., 0.],\n       [0., 1., 0., 0., 1., 0.],\n       [0., 0., 1., 0., 1., 0.],\n       [0., 0., 1., 0., 1., 1.],\n       [0., 0., 1., 1., 0., 2.],\n       [1., 0., 0., 1., 0., 0.]])\n\n\nConversely, we might want to pass through a few columns and drop the rest. We can use the special string 'passthrough' to tell the ColumnTransformer which columns to pass through, and also tell it to drop all remaining columns. So in this example, we‚Äôre one-hot encoding Embarked and Sex, which creates 5 columns, passing through Parch, which adds 1 more column, and dropping Fare.\nAgain, we can use this pattern to pass through a few columns and drop hundreds of columns without listing them all.\nFinally, just a reminder that 'drop' is the default value for remainder, so you aren‚Äôt actually required to specify it here.\n\nct = make_column_transformer(\n    (ohe, ['Embarked', 'Sex']),\n    ('passthrough', ['Parch']),\n    remainder='drop')\nct.fit_transform(X)\n\narray([[0., 0., 1., 0., 1., 0.],\n       [1., 0., 0., 1., 0., 0.],\n       [0., 0., 1., 1., 0., 0.],\n       [0., 0., 1., 1., 0., 0.],\n       [0., 0., 1., 0., 1., 0.],\n       [0., 1., 0., 0., 1., 0.],\n       [0., 0., 1., 0., 1., 0.],\n       [0., 0., 1., 0., 1., 1.],\n       [0., 0., 1., 1., 0., 2.],\n       [1., 0., 0., 1., 0., 0.]])",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Improving your workflow with ColumnTransformer and Pipeline</span>"
    ]
  },
  {
    "objectID": "ch04.html#qa-how-do-i-transform-the-unspecified-columns",
    "href": "ch04.html#qa-how-do-i-transform-the-unspecified-columns",
    "title": "4¬† Improving your workflow with ColumnTransformer and Pipeline",
    "section": "4.5 Q&A: How do I transform the unspecified columns?",
    "text": "4.5 Q&A: How do I transform the unspecified columns?\nWe know how to drop or pass through the unspecified columns in a ColumnTransformer, but let‚Äôs pretend we wanted to apply a transformation to all of the unspecified columns. This is actually simple to do by passing a transformer to the remainder parameter.\nFor example, we might want to scale all of the unspecified columns. One option is MaxAbsScaler, which divides each feature by its maximum value and thus scales it to the range negative 1 to positive 1. We‚Äôll import it from the preprocessing module and then create an instance.\n\nfrom sklearn.preprocessing import MaxAbsScaler\nscaler = MaxAbsScaler()\n\nThen, we can pass the scaler to the remainder parameter.\nWhen we run the fit_transform method, you can see that the first 5 columns were created from Embarked and Sex, and the sixth column is the scaled version of the Parch column.\n\nct = make_column_transformer(\n    (ohe, ['Embarked', 'Sex']),\n    ('drop', ['Fare']),\n    remainder=scaler)\nct.fit_transform(X)\n\narray([[0. , 0. , 1. , 0. , 1. , 0. ],\n       [1. , 0. , 0. , 1. , 0. , 0. ],\n       [0. , 0. , 1. , 1. , 0. , 0. ],\n       [0. , 0. , 1. , 1. , 0. , 0. ],\n       [0. , 0. , 1. , 0. , 1. , 0. ],\n       [0. , 1. , 0. , 0. , 1. , 0. ],\n       [0. , 0. , 1. , 0. , 1. , 0. ],\n       [0. , 0. , 1. , 0. , 1. , 0.5],\n       [0. , 0. , 1. , 1. , 0. , 1. ],\n       [1. , 0. , 0. , 1. , 0. , 0. ]])",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Improving your workflow with ColumnTransformer and Pipeline</span>"
    ]
  },
  {
    "objectID": "ch04.html#qa-how-do-i-select-columns-from-a-numpy-array",
    "href": "ch04.html#qa-how-do-i-select-columns-from-a-numpy-array",
    "title": "4¬† Improving your workflow with ColumnTransformer and Pipeline",
    "section": "4.6 Q&A: How do I select columns from a NumPy array?",
    "text": "4.6 Q&A: How do I select columns from a NumPy array?\nThroughout this book, we‚Äôve been using a pandas DataFrame as our input. But what if your input data was a NumPy array instead? Let‚Äôs see how that affects our workflow.\nWe‚Äôll start by converting the X and X_new DataFrames into NumPy arrays called X_array and X_new_array.\n\nX_array = X.to_numpy()\nX_new_array = X_new.to_numpy()\n\nHere‚Äôs what X_array looks like.\n\nX_array\n\narray([[0, 7.25, 'S', 'male'],\n       [0, 71.2833, 'C', 'female'],\n       [0, 7.925, 'S', 'female'],\n       [0, 53.1, 'S', 'female'],\n       [0, 8.05, 'S', 'male'],\n       [0, 8.4583, 'Q', 'male'],\n       [0, 51.8625, 'S', 'male'],\n       [1, 21.075, 'S', 'male'],\n       [2, 11.1333, 'S', 'female'],\n       [0, 30.0708, 'C', 'female']], dtype=object)\n\n\nIf this was our input data, and we wanted to use a ColumnTransformer, we wouldn‚Äôt be able to specify the columns by name because columns of a NumPy array don‚Äôt have names. However, we do have a couple of other options.\nFirst, we could specify the columns by integer position. Embarked and Sex are columns 2 and 3, so in this example, we‚Äôre one-hot encoding Embarked and Sex and passing through the remainder. Note that we‚Äôre passing X_array, not X, to the fit_transform method.\n\nct = make_column_transformer(\n    (ohe, [2, 3]),\n    remainder='passthrough')\nct.fit_transform(X_array)\n\narray([[0.0, 0.0, 1.0, 0.0, 1.0, 0, 7.25],\n       [1.0, 0.0, 0.0, 1.0, 0.0, 0, 71.2833],\n       [0.0, 0.0, 1.0, 1.0, 0.0, 0, 7.925],\n       [0.0, 0.0, 1.0, 1.0, 0.0, 0, 53.1],\n       [0.0, 0.0, 1.0, 0.0, 1.0, 0, 8.05],\n       [0.0, 1.0, 0.0, 0.0, 1.0, 0, 8.4583],\n       [0.0, 0.0, 1.0, 0.0, 1.0, 0, 51.8625],\n       [0.0, 0.0, 1.0, 0.0, 1.0, 1, 21.075],\n       [0.0, 0.0, 1.0, 1.0, 0.0, 2, 11.1333],\n       [1.0, 0.0, 0.0, 1.0, 0.0, 0, 30.0708]], dtype=object)\n\n\nAnother option is to specify the columns using slices, which is useful for large ranges of columns next to one another. In this case, we‚Äôre selecting columns 2 through 3 for one-hot encoding, and passing through the remainder. Remember that Python slices are inclusive of the starting value, which is 2 in this case, and exclusive of the ending value, which is 4 in this case.\n\nct = make_column_transformer(\n    (ohe, slice(2, 4)),\n    remainder='passthrough')\nct.fit_transform(X_array)\n\narray([[0.0, 0.0, 1.0, 0.0, 1.0, 0, 7.25],\n       [1.0, 0.0, 0.0, 1.0, 0.0, 0, 71.2833],\n       [0.0, 0.0, 1.0, 1.0, 0.0, 0, 7.925],\n       [0.0, 0.0, 1.0, 1.0, 0.0, 0, 53.1],\n       [0.0, 0.0, 1.0, 0.0, 1.0, 0, 8.05],\n       [0.0, 1.0, 0.0, 0.0, 1.0, 0, 8.4583],\n       [0.0, 0.0, 1.0, 0.0, 1.0, 0, 51.8625],\n       [0.0, 0.0, 1.0, 0.0, 1.0, 1, 21.075],\n       [0.0, 0.0, 1.0, 1.0, 0.0, 2, 11.1333],\n       [1.0, 0.0, 0.0, 1.0, 0.0, 0, 30.0708]], dtype=object)\n\n\nOne final option is to specify the columns using a boolean mask. Normally you would create the mask using some sort of condition, but in this case I‚Äôm just writing out a mask to select columns 2 and 3 for one-hot encoding, and passing through the remainder.\n\nct = make_column_transformer(\n    (ohe, [False, False, True, True]),\n    remainder='passthrough')\nct.fit_transform(X_array)\n\narray([[0.0, 0.0, 1.0, 0.0, 1.0, 0, 7.25],\n       [1.0, 0.0, 0.0, 1.0, 0.0, 0, 71.2833],\n       [0.0, 0.0, 1.0, 1.0, 0.0, 0, 7.925],\n       [0.0, 0.0, 1.0, 1.0, 0.0, 0, 53.1],\n       [0.0, 0.0, 1.0, 0.0, 1.0, 0, 8.05],\n       [0.0, 1.0, 0.0, 0.0, 1.0, 0, 8.4583],\n       [0.0, 0.0, 1.0, 0.0, 1.0, 0, 51.8625],\n       [0.0, 0.0, 1.0, 0.0, 1.0, 1, 21.075],\n       [0.0, 0.0, 1.0, 1.0, 0.0, 2, 11.1333],\n       [1.0, 0.0, 0.0, 1.0, 0.0, 0, 30.0708]], dtype=object)\n\n\nSo those are our three options for selecting columns in a ColumnTransformer when your input source is a NumPy array.\n\n\n\n\n\n\nOptions for selecting columns from a NumPy array:\n\nInteger position\nSlice\nBoolean mask\n\n\n\n\nOther than that, the rest of our workflow remains the same. We‚Äôll just update the Pipeline to use our new ColumnTransformer.\n\npipe = make_pipeline(ct, logreg)\n\nThen we can fit the Pipeline with X_array and y, and make predictions for X_new_array.\n\npipe.fit(X_array, y)\npipe.predict(X_new_array)\n\narray([0, 1, 0, 0, 1, 0, 1, 0, 1, 0])",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Improving your workflow with ColumnTransformer and Pipeline</span>"
    ]
  },
  {
    "objectID": "ch04.html#qa-how-do-i-select-columns-by-data-type",
    "href": "ch04.html#qa-how-do-i-select-columns-by-data-type",
    "title": "4¬† Improving your workflow with ColumnTransformer and Pipeline",
    "section": "4.7 Q&A: How do I select columns by data type?",
    "text": "4.7 Q&A: How do I select columns by data type?\nSo far in this book, we‚Äôve been selecting columns one-by-one. But let‚Äôs say that we had many more columns, and we simply wanted to one-hot encode all object columns and passthrough all numeric columns without listing all of them out. How would we do that?\nThe easiest way to do this is with the make_column_selector function, which was new in scikit-learn version 0.22.\n\nfrom sklearn.compose import make_column_selector\n\nWe‚Äôre going to create two column selectors called select_object and select_number. To do this, we just set the dtype_include parameter to the data type we want to include, and it outputs a callable.\n\nselect_object = make_column_selector(dtype_include=object)\nselect_number = make_column_selector(dtype_include='number')\n\nThen, we pass the callables to make_column_transformer instead of the column names, and the callables select the columns for us.\nWhen we run fit_transform, you can see that once again, the object columns have been one-hot encoded and the numeric columns have been passed through.\n\nct = make_column_transformer(\n    (ohe, select_object),\n    ('passthrough', select_number))\nct.fit_transform(X)\n\narray([[ 0.    ,  0.    ,  1.    ,  0.    ,  1.    ,  0.    ,  7.25  ],\n       [ 1.    ,  0.    ,  0.    ,  1.    ,  0.    ,  0.    , 71.2833],\n       [ 0.    ,  0.    ,  1.    ,  1.    ,  0.    ,  0.    ,  7.925 ],\n       [ 0.    ,  0.    ,  1.    ,  1.    ,  0.    ,  0.    , 53.1   ],\n       [ 0.    ,  0.    ,  1.    ,  0.    ,  1.    ,  0.    ,  8.05  ],\n       [ 0.    ,  1.    ,  0.    ,  0.    ,  1.    ,  0.    ,  8.4583],\n       [ 0.    ,  0.    ,  1.    ,  0.    ,  1.    ,  0.    , 51.8625],\n       [ 0.    ,  0.    ,  1.    ,  0.    ,  1.    ,  1.    , 21.075 ],\n       [ 0.    ,  0.    ,  1.    ,  1.    ,  0.    ,  2.    , 11.1333],\n       [ 1.    ,  0.    ,  0.    ,  1.    ,  0.    ,  0.    , 30.0708]])\n\n\nOne slight variation of this is that you can tell make_column_selector to exclude instead of include a specific data type. In this example, we‚Äôre using the dtype_exclude parameter to create a column selector that excludes the object data type.\n\nexclude_object = make_column_selector(dtype_exclude=object)\n\nThis time, we‚Äôll tell the ColumnTransformer to one-hot encode all object columns and pass through all non-object columns, which has the same effect as before.\n\nct = make_column_transformer(\n    (ohe, select_object),\n    ('passthrough', exclude_object))\nct.fit_transform(X)\n\narray([[ 0.    ,  0.    ,  1.    ,  0.    ,  1.    ,  0.    ,  7.25  ],\n       [ 1.    ,  0.    ,  0.    ,  1.    ,  0.    ,  0.    , 71.2833],\n       [ 0.    ,  0.    ,  1.    ,  1.    ,  0.    ,  0.    ,  7.925 ],\n       [ 0.    ,  0.    ,  1.    ,  1.    ,  0.    ,  0.    , 53.1   ],\n       [ 0.    ,  0.    ,  1.    ,  0.    ,  1.    ,  0.    ,  8.05  ],\n       [ 0.    ,  1.    ,  0.    ,  0.    ,  1.    ,  0.    ,  8.4583],\n       [ 0.    ,  0.    ,  1.    ,  0.    ,  1.    ,  0.    , 51.8625],\n       [ 0.    ,  0.    ,  1.    ,  0.    ,  1.    ,  1.    , 21.075 ],\n       [ 0.    ,  0.    ,  1.    ,  1.    ,  0.    ,  2.    , 11.1333],\n       [ 1.    ,  0.    ,  0.    ,  1.    ,  0.    ,  0.    , 30.0708]])\n\n\nThere are also other data type options you can use, such as the datetime data type or the pandas category data type.\n\nselect_datetime = make_column_selector(dtype_include='datetime')\nselect_category = make_column_selector(dtype_include='category')\n\nFinally, it‚Äôs worth noting that you can also pass a list of multiple data types to make_column_selector.\n\nselect_multiple = make_column_selector(dtype_include=[object,\n                                                      'category'])",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Improving your workflow with ColumnTransformer and Pipeline</span>"
    ]
  },
  {
    "objectID": "ch04.html#qa-how-do-i-select-columns-by-column-name-pattern",
    "href": "ch04.html#qa-how-do-i-select-columns-by-column-name-pattern",
    "title": "4¬† Improving your workflow with ColumnTransformer and Pipeline",
    "section": "4.8 Q&A: How do I select columns by column name pattern?",
    "text": "4.8 Q&A: How do I select columns by column name pattern?\nLet‚Äôs say that we had a lot of columns, and all of the columns that we wanted to select for a particular transformation had the same pattern in their names. For example, maybe all of those columns started with the same word.\nOnce again, we can use the make_column_selector function, which allows us to select columns by regular expression pattern. Here‚Äôs a silly example in which we select columns that include the capital letters E or S.\n\nselect_ES = make_column_selector(pattern='E|S')\n\nWhen we run the fit_transform method, Embarked and Sex have been one-hot encoded, and the remaining columns have been passed through.\n\nct = make_column_transformer(\n    (ohe, select_ES),\n    remainder='passthrough')\nct.fit_transform(X)\n\narray([[ 0.    ,  0.    ,  1.    ,  0.    ,  1.    ,  0.    ,  7.25  ],\n       [ 1.    ,  0.    ,  0.    ,  1.    ,  0.    ,  0.    , 71.2833],\n       [ 0.    ,  0.    ,  1.    ,  1.    ,  0.    ,  0.    ,  7.925 ],\n       [ 0.    ,  0.    ,  1.    ,  1.    ,  0.    ,  0.    , 53.1   ],\n       [ 0.    ,  0.    ,  1.    ,  0.    ,  1.    ,  0.    ,  8.05  ],\n       [ 0.    ,  1.    ,  0.    ,  0.    ,  1.    ,  0.    ,  8.4583],\n       [ 0.    ,  0.    ,  1.    ,  0.    ,  1.    ,  0.    , 51.8625],\n       [ 0.    ,  0.    ,  1.    ,  0.    ,  1.    ,  1.    , 21.075 ],\n       [ 0.    ,  0.    ,  1.    ,  1.    ,  0.    ,  2.    , 11.1333],\n       [ 1.    ,  0.    ,  0.    ,  1.    ,  0.    ,  0.    , 30.0708]])\n\n\nAgain, this is only useful if your column names follow a particular pattern and you know how to write regular expressions.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Improving your workflow with ColumnTransformer and Pipeline</span>"
    ]
  },
  {
    "objectID": "ch04.html#sec-4-9",
    "href": "ch04.html#sec-4-9",
    "title": "4¬† Improving your workflow with ColumnTransformer and Pipeline",
    "section": "4.9 Q&A: Should I use ColumnTransformer or make_column_transformer?",
    "text": "4.9 Q&A: Should I use ColumnTransformer or make_column_transformer?\nSo far in this book, we‚Äôve been creating ColumnTransformers using the make_column_transformer function. In this lesson, I‚Äôll show you how to use the ColumnTransformer class and then compare it to make_column_transformer so that you can decide which one you want to use.\nTo start, we‚Äôll import the ColumnTransformer class from the compose module, and then we‚Äôll create an instance.\nWhen creating an instance, the first difference you might notice is that the tuples have three elements rather than two. The first element of each tuple is a name of your choosing that you are required to assign to the transformer.\nIn this case, the first tuple is our one-hot encoding of Embarked and Sex, and we‚Äôre assigning it the name ‚ÄúOHE‚Äù (all caps). The second tuple is our special passthrough transformer for Parch and Fare, and we‚Äôre assigning it the name ‚Äúpass‚Äù. We can see these names when we print out the ColumnTransformer.\nYou might also notice that the tuples are in a list, which is a requirement of the ColumnTransformer class.\n\nfrom sklearn.compose import ColumnTransformer\nct = ColumnTransformer(\n    [('OHE', ohe, ['Embarked', 'Sex']),\n     ('pass', 'passthrough', ['Parch', 'Fare'])])\nct\n\nColumnTransformer(transformers=[('OHE', OneHotEncoder(), ['Embarked', 'Sex']),\n                                ('pass', 'passthrough', ['Parch', 'Fare'])])\n\n\n\n\n\n\n\n\nTuple elements for ColumnTransformer:\n\nTransformer name\nTransformer object or 'drop' or 'passthrough'\nList of columns to which the transformer should be applied\n\n\n\n\nNow let‚Äôs create the same ColumnTransformer using the make_column_transformer function. When using make_column_transformer, we don‚Äôt define names for the transformers. Instead, each transformer is assigned a default name, which is the lowercase version of the transformer‚Äôs class name.\nAs you can see when we print it out, the one-hot encoder is assigned the name ‚Äúonehotencoder‚Äù (all lowercase), and the passthrough transformer is assigned the name ‚Äúpassthrough‚Äù.\n\nct = make_column_transformer(\n    (ohe, ['Embarked', 'Sex']),\n    ('passthrough', ['Parch', 'Fare']))\nct\n\nColumnTransformer(transformers=[('onehotencoder', OneHotEncoder(),\n                                 ['Embarked', 'Sex']),\n                                ('passthrough', 'passthrough',\n                                 ['Parch', 'Fare'])])\n\n\nAll of that being said, which one should you use?\nI prefer make_column_transformer, because I find the code both easier to read and easier to write, so that‚Äôs what I‚Äôll use in this book. I usually don‚Äôt mind the default transformer names, and in fact I like that I don‚Äôt have to come up with a name for each transformer.\nHowever, there are times when defining names for the transformers is useful. Custom names can be clearer if you‚Äôre performing a grid search of transformer parameters, or if you‚Äôre using the same type of transformer multiple times in the same ColumnTransformer instance. We‚Äôll see examples of this later in the book.\nOne final note is that the ColumnTransformer class enables transformer weights, meaning you can emphasize the output of some transformers more than others. The specific use case of this is not yet clear to me, but if you do decide to use transformer weights, then you can‚Äôt use the make_column_transformer function and you must use the ColumnTransformer class.\n\n\n\n\n\n\n\n\n\n¬†\nColumnTransformer\nmake_column_transformer\n\n\n\n\nAllows custom names?\nYes\nNo\n\n\nAllows transformer weights?\nYes\nNo",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Improving your workflow with ColumnTransformer and Pipeline</span>"
    ]
  },
  {
    "objectID": "ch04.html#sec-4-10",
    "href": "ch04.html#sec-4-10",
    "title": "4¬† Improving your workflow with ColumnTransformer and Pipeline",
    "section": "4.10 Q&A: Should I use Pipeline or make_pipeline?",
    "text": "4.10 Q&A: Should I use Pipeline or make_pipeline?\nSo far in this book, we‚Äôve been creating Pipelines using the make_pipeline function. In this lesson, I‚Äôll show you how to use the Pipeline class and then compare it to make_pipeline so that you can decide which one you want to use.\nTo start, we‚Äôll import the Pipeline class from the pipeline module, and then we‚Äôll create an instance.\nWhen creating an instance, the main difference you might notice is that we‚Äôre passing in a list of tuples to the Pipeline constructor. Each tuple has two elements, in which the first element is the name you‚Äôre assigning to the Pipeline step, and the second element is the model or transformer you‚Äôre including in the Pipeline.\nIn this case, the first tuple is our preprocessing step using ColumnTransformer, and we‚Äôre assigning it the name ‚Äúpreprocessor‚Äù. The second tuple is our model building step using logistic regression, and we‚Äôre assigning it the name ‚Äúclassifier‚Äù. We can see these names when we print out the Pipeline.\n\nfrom sklearn.pipeline import Pipeline\npipe = Pipeline([('preprocessor', ct), ('classifier', logreg)])\npipe\n\nPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('onehotencoder',\n                                                  OneHotEncoder(),\n                                                  ['Embarked', 'Sex']),\n                                                 ('passthrough', 'passthrough',\n                                                  ['Parch', 'Fare'])])),\n                ('classifier',\n                 LogisticRegression(random_state=1, solver='liblinear'))])\n\n\n\n\n\n\n\n\nTuple elements for Pipeline:\n\nStep name\nModel or transformer object\n\n\n\n\nWe can also see the step names by accessing the named_steps attribute of the Pipeline and running the keys method.\n\npipe.named_steps.keys()\n\ndict_keys(['preprocessor', 'classifier'])\n\n\nNow let‚Äôs create the same Pipeline using the make_pipeline function. When using make_pipeline, we don‚Äôt define names for the steps. Instead, each step is assigned a default name, which is the lowercase version of the step‚Äôs class name.\nAs you can see when we print it out, the first step is assigned the name ‚Äúcolumntransformer‚Äù (all lowercase), and the second step is assigned the name ‚Äúlogisticregression‚Äù (all lowercase).\n\npipe = make_pipeline(ct, logreg)\npipe\n\nPipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('onehotencoder',\n                                                  OneHotEncoder(),\n                                                  ['Embarked', 'Sex']),\n                                                 ('passthrough', 'passthrough',\n                                                  ['Parch', 'Fare'])])),\n                ('logisticregression',\n                 LogisticRegression(random_state=1, solver='liblinear'))])\n\n\nAgain, we can also see the step names using the named_steps attribute.\n\npipe.named_steps.keys()\n\ndict_keys(['columntransformer', 'logisticregression'])\n\n\nAll of that being said, which one should you use?\nI prefer make_pipeline, because I find the code both easier to read and easier to write, so that‚Äôs what I‚Äôll use in this book. I usually don‚Äôt mind the default step names, and in fact I like that I don‚Äôt have to come up with a name for each step.\nHowever, custom step names can be useful for clarity, especially if you‚Äôre performing a grid search of a Pipeline. We‚Äôll see many examples of this later in the book.\n\n\n\n\n\n\n\n\n\n¬†\nPipeline\nmake_pipeline\n\n\n\n\nAllows custom names?\nYes\nNo",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Improving your workflow with ColumnTransformer and Pipeline</span>"
    ]
  },
  {
    "objectID": "ch04.html#qa-how-do-i-examine-the-steps-of-a-pipeline",
    "href": "ch04.html#qa-how-do-i-examine-the-steps-of-a-pipeline",
    "title": "4¬† Improving your workflow with ColumnTransformer and Pipeline",
    "section": "4.11 Q&A: How do I examine the steps of a Pipeline?",
    "text": "4.11 Q&A: How do I examine the steps of a Pipeline?\nSometimes you might want to examine the steps of a fitted Pipeline so that you can understand what‚Äôs happening within each step. In this lesson, I‚Äôll show you how to do it.\nWe‚Äôll start by fitting the Pipeline, which prints out the two steps.\n\npipe.fit(X, y)\n\nPipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('onehotencoder',\n                                                  OneHotEncoder(),\n                                                  ['Embarked', 'Sex']),\n                                                 ('passthrough', 'passthrough',\n                                                  ['Parch', 'Fare'])])),\n                ('logisticregression',\n                 LogisticRegression(random_state=1, solver='liblinear'))])\n\n\nAs I mentioned in the previous lesson, make_pipeline assigned a name to each step, which is the lowercase version of the step‚Äôs class name. In this case, our step names are ‚Äúcolumntransformer‚Äù and ‚Äúlogisticregression‚Äù.\n\npipe.named_steps.keys()\n\ndict_keys(['columntransformer', 'logisticregression'])\n\n\nTo examine an individual step, you select the named_steps attribute and pass the step name in brackets. Note that if we had assigned custom step names such as ‚Äúpreprocessor‚Äù and ‚Äúclassifier‚Äù, we would be using those here instead.\n\npipe.named_steps['columntransformer']\n\nColumnTransformer(transformers=[('onehotencoder', OneHotEncoder(),\n                                 ['Embarked', 'Sex']),\n                                ('passthrough', 'passthrough',\n                                 ['Parch', 'Fare'])])\n\n\n\npipe.named_steps['logisticregression']\n\nLogisticRegression(random_state=1, solver='liblinear')\n\n\nOnce you‚Äôve accessed a step, you can examine its attributes or run its methods. For example, we can run the get_feature_names method from the ‚Äúcolumntransformer‚Äù step to learn the names of each feature. As a reminder, the x0 means feature 0 that was passed to the OneHotEncoder, and the x1 means feature 1.\n\npipe.named_steps['columntransformer'].get_feature_names()\n\n['onehotencoder__x0_C',\n 'onehotencoder__x0_Q',\n 'onehotencoder__x0_S',\n 'onehotencoder__x1_female',\n 'onehotencoder__x1_male',\n 'Parch',\n 'Fare']\n\n\nWe can also see the coefficient values of the 7 features by examining the coef_ attribute of the ‚Äúlogisticregression‚Äù step. These coefficients are listed in the same order as the features, though the intercept is stored in a separate attribute.\nBy finding the 4 positive coefficients, you can determine that embarking at port C, being female, and having a higher Parch and Fare are all associated with a greater likelihood of survival. Note that these are just associations the model learned from 10 rows of training data. They are not necessarily statistically significant associations, and in fact scikit-learn does not provide p-values.\n\npipe.named_steps['logisticregression'].coef_\n\narray([[ 0.26491287, -0.19848033, -0.22907928,  1.0075062 , -1.17015293,\n         0.20056557,  0.01597307]])\n\n\nFinally, it‚Äôs worth noting that there are three other ways that you can examine the steps of a Pipeline:\n\nFirst, you can use named_steps with periods.\nSecond, you can exclude the named_steps attribute entirely.\nThird, you can reference the step by position rather than by name.\n\n\npipe.named_steps.logisticregression.coef_\n\narray([[ 0.26491287, -0.19848033, -0.22907928,  1.0075062 , -1.17015293,\n         0.20056557,  0.01597307]])\n\n\n\npipe['logisticregression'].coef_\n\narray([[ 0.26491287, -0.19848033, -0.22907928,  1.0075062 , -1.17015293,\n         0.20056557,  0.01597307]])\n\n\n\npipe[1].coef_\n\narray([[ 0.26491287, -0.19848033, -0.22907928,  1.0075062 , -1.17015293,\n         0.20056557,  0.01597307]])\n\n\nPersonally, I like the initial bracket notation because I think it‚Äôs the most readable, even though it‚Äôs the most typing. However, using named_steps with the periods seems to be the only option that supports autocompleting both the step name and the attribute, which is a nice benefit.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Improving your workflow with ColumnTransformer and Pipeline</span>"
    ]
  },
  {
    "objectID": "ch05.html",
    "href": "ch05.html",
    "title": "5¬† Workflow review #1",
    "section": "",
    "text": "5.1 Recap of our workflow\nIn this chapter, we‚Äôre going to review the workflow that we‚Äôve built so far to make sure you understand the key concepts before we start adding additional complexity.\nTo start, we‚Äôre going to walk through all of the code that is necessary to recreate our workflow up to this point. We begin by importing pandas, the OneHotEncoder and LogisticRegression classes, and the make_column_transformer and make_pipeline functions.\nimport pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.pipeline import make_pipeline\nNext, we create a list of the four columns we‚Äôre going to select from our data.\ncols = ['Parch', 'Fare', 'Embarked', 'Sex']\nThen, we read in 10 rows of training data and use it to define our X and y.\ndf = pd.read_csv('http://bit.ly/MLtrain', nrows=10)\nX = df[cols]\ny = df['Survived']\nAnd we read in 10 rows of new data and use it to define X_new.\ndf_new = pd.read_csv('http://bit.ly/MLnewdata', nrows=10)\nX_new = df_new[cols]\nWe create an instance of OneHotEncoder, which is our only transformer at this point.\nohe = OneHotEncoder()\nAnd then we build the ColumnTransformer, which one-hot encodes Embarked and Sex and passes through Parch and Fare.\nct = make_column_transformer(\n    (ohe, ['Embarked', 'Sex']),\n    ('passthrough', ['Parch', 'Fare']))\nWe also create an instance of LogisticRegression.\nlogreg = LogisticRegression(solver='liblinear', random_state=1)\nFinally, we create a two-step Pipeline, fit the Pipeline to X and y, and use the fitted Pipeline to make predictions on X_new.\npipe = make_pipeline(ct, logreg)\npipe.fit(X, y)\npipe.predict(X_new)\n\narray([0, 1, 0, 0, 1, 0, 1, 0, 1, 0])\nThat‚Äôs really all the code we need to recreate our entire workflow from the last few chapters. You‚Äôll notice that there are no calls to fit_transform or transform because all of that functionality is encapsulated by the Pipeline.\nI did exclude cross-validation from this recap because, as mentioned previously, any model evaluation procedure is highly unreliable with only 10 rows of data. However, we will thoroughly explore the topic of model evaluation later in the book, once we are using the full dataset.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Workflow review #1</span>"
    ]
  },
  {
    "objectID": "ch05.html#comparing-columntransformer-and-pipeline",
    "href": "ch05.html#comparing-columntransformer-and-pipeline",
    "title": "5¬† Workflow review #1",
    "section": "5.2 Comparing ColumnTransformer and Pipeline",
    "text": "5.2 Comparing ColumnTransformer and Pipeline\nIn order to be successful with the rest of this book, it‚Äôs very important that you clearly understand the differences between a ColumnTransformer and a Pipeline. In this lesson, I‚Äôm going to explain those differences. This diagram should help to illustrate the concepts.\nLet‚Äôs start with the ColumnTransformer, which received 4 columns of input from the X DataFrame:\n\nIt selected 2 of those columns, namely Embarked and Sex, and used the OneHotEncoder to transform them into 5 columns.\nIt selected the other 2 columns, namely Parch and Fare, and did nothing to them, which of course resulted in 2 columns.\nFinally, it stacked the 5 columns output by OneHotEncoder and the 2 columns output by the passthrough transformer side-by-side, resulting in a total of 7 columns.\n\nNow let‚Äôs talk about the Pipeline, which has 2 steps:\n\nStep 1 is a ColumnTransformer that received 4 columns of input and transformed them into 7 columns.\nStep 2 is a LogisticRegression model that received 7 columns of input and used those 7 columns either for fitting or predicting.\n\n\n\n\n\n\nWith those examples in mind, we can step back and summarize the differences between a ColumnTransformer and a Pipeline:\n\nA ColumnTransformer pulls out subsets of columns and transforms them independently, and then stacks the results side-by-side.\nIt only ever does data transformations, meaning your ColumnTransformer will never include a model.\nAnd it does not have steps, because each subset of columns is transformed independently. In other words, data does not flow from one transformer to the next.\n\nIn contrast:\n\nA Pipeline is a series of steps that occur in order, and the output of each step becomes the input to the next step.\nThus if you had a 3-step Pipeline, the output of step 1 becomes the input to step 2, and the output of step 2 becomes the input to step 3.\nThe last step of a Pipeline can be a model or a transformer, whereas all other steps must be transformers.\n\n\n\n\n\n\n\nColumnTransformer vs Pipeline:\n\nColumnTransformer:\n\nSelects subsets of columns, transforms them independently, stacks the results side-by-side\nOnly includes transformers\nDoes not have steps (transformers operate in parallel)\n\nPipeline:\n\nSeries of steps that occur in order\nOutput of each step becomes the input to the next step\nLast step is a model or transformer, all other steps are transformers\n\n\n\n\n\nIn summary, a Pipeline contains steps that operate in sequence, whereas a ColumnTransformer contains transformers that operate in parallel. In later chapters, you‚Äôll see why this difference is so important and how it guides the structure of our workflow.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Workflow review #1</span>"
    ]
  },
  {
    "objectID": "ch05.html#creating-a-pipeline-diagram",
    "href": "ch05.html#creating-a-pipeline-diagram",
    "title": "5¬† Workflow review #1",
    "section": "5.3 Creating a Pipeline diagram",
    "text": "5.3 Creating a Pipeline diagram\nTo wrap up this chapter, I want to show you a feature that‚Äôs new in scikit-learn version 0.23 that can help you to visualize and thus better understand your Pipeline.\nTo start, we‚Äôll import the set_config function. Then we run it and set the display parameter to 'diagram'.\n\nfrom sklearn import set_config\nset_config(display='diagram')\n\nWith that configuration, you‚Äôll see a diagram any time you print out a Pipeline (or any other estimator) within Jupyter.\nThis is basically the same diagram I created. And you can actually click on any element in order to see more details. For example, if you click on the transformer names, you can see the columns they‚Äôre transforming. And if you click on the class names, you can see any parameters that have been changed from their default values.\n\npipe\n\nPipelinePipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('onehotencoder',\n                                                  OneHotEncoder(),\n                                                  ['Embarked', 'Sex']),\n                                                 ('passthrough', 'passthrough',\n                                                  ['Parch', 'Fare'])])),\n                ('logisticregression',\n                 LogisticRegression(random_state=1, solver='liblinear'))])columntransformer: ColumnTransformerColumnTransformer(transformers=[('onehotencoder', OneHotEncoder(),\n                                 ['Embarked', 'Sex']),\n                                ('passthrough', 'passthrough',\n                                 ['Parch', 'Fare'])])onehotencoder['Embarked', 'Sex']OneHotEncoderOneHotEncoder()passthrough['Parch', 'Fare']passthroughpassthroughLogisticRegressionLogisticRegression(random_state=1, solver='liblinear')\n\n\nIf you use this configuration but you ever need to see the regular text output, you can just use the print function with your Pipeline.\n\nprint(pipe)\n\nPipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('onehotencoder',\n                                                  OneHotEncoder(),\n                                                  ['Embarked', 'Sex']),\n                                                 ('passthrough', 'passthrough',\n                                                  ['Parch', 'Fare'])])),\n                ('logisticregression',\n                 LogisticRegression(random_state=1, solver='liblinear'))])\n\n\nFinally, it‚Äôs worth noting that displaying diagrams is the default starting in scikit-learn version 1.1. If you‚Äôd prefer to always see the text output, you can change the configuration by setting the display parameter to 'text'.\n\nset_config(display='text')\npipe\n\nPipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('onehotencoder',\n                                                  OneHotEncoder(),\n                                                  ['Embarked', 'Sex']),\n                                                 ('passthrough', 'passthrough',\n                                                  ['Parch', 'Fare'])])),\n                ('logisticregression',\n                 LogisticRegression(random_state=1, solver='liblinear'))])",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Workflow review #1</span>"
    ]
  },
  {
    "objectID": "ch06.html",
    "href": "ch06.html",
    "title": "6¬† Encoding text data",
    "section": "",
    "text": "6.1 Vectorizing text\nSo far in this book, we‚Äôve only focused on numerical and categorical features. In this chapter, we‚Äôll learn how to create features from unstructured text data.\nLet‚Äôs take another look at our Titanic DataFrame.\nWe want to include the Name column in our model in case it contains predictive information about the likelihood of survival. For example, maybe their last name is predictive of survival if they‚Äôre part of an important family, and maybe certain titles are also predictive.\nThe Name column can‚Äôt be passed directly to the model because it‚Äôs not numeric. So in this lesson, we‚Äôll learn how to encode it numerically, and in the next lesson we‚Äôll add it to the ColumnTransformer and Pipeline.\ndf\n\n\n\n\n\n\n\n\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n5\n0\n3\nMoran, Mr. James\nmale\nNaN\n0\n0\n330877\n8.4583\nNaN\nQ\n\n\n6\n0\n1\nMcCarthy, Mr. Timothy J\nmale\n54.0\n0\n0\n17463\n51.8625\nE46\nS\n\n\n7\n0\n3\nPalsson, Master. Gosta Leonard\nmale\n2.0\n3\n1\n349909\n21.0750\nNaN\nS\n\n\n8\n1\n3\nJohnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)\nfemale\n27.0\n0\n2\n347742\n11.1333\nNaN\nS\n\n\n9\n1\n2\nNasser, Mrs. Nicholas (Adele Achem)\nfemale\n14.0\n1\n0\n237736\n30.0708\nNaN\nC\nOne idea for encoding would be to simply one-hot encode the Name column. However, OneHotEncoder would treat each full name as its own category. This is unlikely to be useful, since we don‚Äôt expect to see any full name repeated more than once in the training or new data.\nInstead, we want to consider each word in their name independently, so that we can learn different things from each part of their name. The CountVectorizer class was built for this purpose, so that‚Äôs what we‚Äôll use. In brief, CountVectorizer converts text into a matrix of token counts, and shortly you‚Äôll see exactly what that means.\nTo start, we import CountVectorizer from the feature_extraction module and create an instance called vect.\nfrom sklearn.feature_extraction.text import CountVectorizer\nvect = CountVectorizer()\nThen we‚Äôll pass the Name to the fit_transform method of vect. Notice that we‚Äôre using single brackets around Name to pass it as a Series, which is because CountVectorizer expects 1-dimensional input. This is unlike OneHotEncoder and most other transformers, which expect 2-dimensional input.\nThe fit_transform method outputs what is called a document-term matrix, which we save as dtm. When we print it out, you can see that it‚Äôs a sparse matrix containing 10 rows and 40 columns. There‚Äôs one row for each of the 10 names, and one column for each of the 40 features it created.\ndtm = vect.fit_transform(df['Name'])\ndtm\n\n&lt;10x40 sparse matrix of type '&lt;class 'numpy.int64'&gt;'\n    with 46 stored elements in Compressed Sparse Row format&gt;\nLet‚Äôs examine the names of the 40 feature columns it created by running the get_feature_names method. As an aside, this has been replaced by the get_feature_names_out method starting in scikit-learn 1.0.\nThese are the 40 unique words that were found in the Name column after lowercasing the words, removing all punctuation, and excluding words that were only 1 character long.\nNote that the features are sorted alphabetically, which is the same convention used by the OneHotEncoder when learning category names.\nprint(vect.get_feature_names())\n\n['achem', 'adele', 'allen', 'berg', 'bradley', 'braund', 'briggs', 'cumings', 'elisabeth', 'florence', 'futrelle', 'gosta', 'harris', 'heath', 'heikkinen', 'henry', 'jacques', 'james', 'john', 'johnson', 'laina', 'leonard', 'lily', 'master', 'may', 'mccarthy', 'miss', 'moran', 'mr', 'mrs', 'nasser', 'nicholas', 'oscar', 'owen', 'palsson', 'peel', 'thayer', 'timothy', 'vilhelmina', 'william']\nLet‚Äôs quickly summarize what we know about the document-term matrix it created:\nWe want to examine the document-term matrix to better understand it. Unlike OneHotEncoder, CountVectorizer does not have a sparse=False argument that we can set in order to view the dense representation. Instead, we‚Äôll use the toarray method to make it dense and then convert that into a DataFrame, using the feature names as the column headings.\npd.DataFrame(dtm.toarray(), columns=vect.get_feature_names())\n\n\n\n\n\n\n\n\nachem\nadele\nallen\nberg\nbradley\nbraund\nbriggs\ncumings\nelisabeth\nflorence\n...\nnasser\nnicholas\noscar\nowen\npalsson\npeel\nthayer\ntimothy\nvilhelmina\nwilliam\n\n\n\n\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n...\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n1\n0\n1\n1\n0\n1\n...\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n4\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n5\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n6\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n7\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n\n\n8\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n...\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n\n\n9\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n...\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n10 rows √ó 40 columns\nWhat can we learn by examining this? If we compared the Name column to the document-term matrix, we would see that in each row, CountVectorizer counted how many times each word appeared.\nLet‚Äôs take the first row as an example. Using the head method, we can see that the name in the first row of training data was ‚ÄúBraund, Mr.¬†Owen Harris‚Äù. Thus the first row of the document-term matrix contains 4 ones (under ‚Äúbraund‚Äù, ‚Äúmr‚Äù, ‚Äúowen‚Äù, and ‚Äúharris‚Äù), and the other 36 entries are all zeros. (Unfortunately, you can‚Äôt see the entries in the middle without modifying the display options for pandas.)\ndf.head(1)\n\n\n\n\n\n\n\n\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.25\nNaN\nS\nThis encoding is known as the ‚ÄúBag of Words‚Äù representation of text data. It‚Äôs known as a ‚Äúbag‚Äù because this encoding doesn‚Äôt capture the word ordering present in each name, rather it only captures the count of how many times a word appears in each name.\nThis representation is the feature matrix that will get passed to the model. Just like the OneHotEncoder created a 10 by 3 matrix from Embarked, CountVectorizer created this 10 by 40 matrix from Name.\nAnd from each of the 40 features, the model can learn the relationship between the target value and how many times that word appeared in each passenger‚Äôs name. Thus if a particular word is predictive of survival, such as ‚ÄúBraund‚Äù, the model will be able to learn that from the matrix.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Encoding text data</span>"
    ]
  },
  {
    "objectID": "ch06.html#vectorizing-text",
    "href": "ch06.html#vectorizing-text",
    "title": "6¬† Encoding text data",
    "section": "",
    "text": "Ideas for encoding the Name column:\n\nOneHotEncoder: Each full name is treated as a category (not recommended)\nCountVectorizer: Each word in a name is treated independently (recommended)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCountVectorizer vs other transformers:\n\nCountVectorizer: 1-dimensional input (Series)\nOther transformers: 2-dimensional input (DataFrame)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDefault settings for CountVectorizer:\n\nConvert all words to lowercase\nRemove all punctuation\nExclude one-character words\n\n\n\n\n\n\nThere are 10 rows and 40 columns.\nEach row represents a row from the training data, and each column represents a word.\nThe rows are known as documents, and the feature names are known as terms, which is why it‚Äôs called a document-term matrix.\nAnd it‚Äôs stored as a sparse matrix.\n\n\n\n\n\n\n\nAbout the document-term matrix:\n\n10 rows and 40 columns\nRows represent rows from training data, columns represent words\nRows are ‚Äúdocuments‚Äù, feature names are ‚Äúterms‚Äù\nSparse matrix\n\n\n\n\n\n\n\n\n\n\n\nHow to examine a document-term matrix:\n\nUse toarray method to make it dense\nConvert dense matrix into a DataFrame\nUse feature names as column headings\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n‚ÄúBag of Words‚Äù representation:\n\nIgnores word order\nOnly counts how many times a word appears",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Encoding text data</span>"
    ]
  },
  {
    "objectID": "ch06.html#including-text-data-in-the-model",
    "href": "ch06.html#including-text-data-in-the-model",
    "title": "6¬† Encoding text data",
    "section": "6.2 Including text data in the model",
    "text": "6.2 Including text data in the model\nNow that we know how to encode text data, we‚Äôre ready to include it in our model.\nWe‚Äôll start by updating the cols list to include the Name column, and then use cols to update the X DataFrame.\n\ncols = ['Parch', 'Fare', 'Embarked', 'Sex', 'Name']\nX = df[cols]\nX\n\n\n\n\n\n\n\n\nParch\nFare\nEmbarked\nSex\nName\n\n\n\n\n0\n0\n7.2500\nS\nmale\nBraund, Mr. Owen Harris\n\n\n1\n0\n71.2833\nC\nfemale\nCumings, Mrs. John Bradley (Florence Briggs Th...\n\n\n2\n0\n7.9250\nS\nfemale\nHeikkinen, Miss. Laina\n\n\n3\n0\n53.1000\nS\nfemale\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\n\n\n4\n0\n8.0500\nS\nmale\nAllen, Mr. William Henry\n\n\n5\n0\n8.4583\nQ\nmale\nMoran, Mr. James\n\n\n6\n0\n51.8625\nS\nmale\nMcCarthy, Mr. Timothy J\n\n\n7\n1\n21.0750\nS\nmale\nPalsson, Master. Gosta Leonard\n\n\n8\n2\n11.1333\nS\nfemale\nJohnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)\n\n\n9\n0\n30.0708\nC\nfemale\nNasser, Mrs. Nicholas (Adele Achem)\n\n\n\n\n\n\n\nNext, we‚Äôll update the ColumnTransformer by adding one more tuple to specify that the CountVectorizer should be applied to Name.\nNote that there are no brackets around 'Name'. This is not because there‚Äôs only one column being passed to CountVectorizer. Instead, it‚Äôs because CountVectorizer expects 1-dimensional input, and brackets would signal 2-dimensional input to the ColumnTransformer.\n\nct = make_column_transformer(\n    (ohe, ['Embarked', 'Sex']),\n    (vect, 'Name'),\n    ('passthrough', ['Parch', 'Fare']))\n\nNext, we‚Äôll use the fit_transform method to try out the transformation. The output contains 47 columns.\n\nct.fit_transform(X)\n\n&lt;10x47 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n    with 78 stored elements in Compressed Sparse Row format&gt;\n\n\nWe can confirm with get_feature_names the meaning of these 47 columns: 3 columns for Embarked, 2 colums for Sex, 40 columns for Name, 1 column for Parch, and 1 column for Fare. Again, the features are in this order because that‚Äôs the order in which they were passed to the ColumnTransformer.\n\nct.get_feature_names() \n\n['onehotencoder__x0_C',\n 'onehotencoder__x0_Q',\n 'onehotencoder__x0_S',\n 'onehotencoder__x1_female',\n 'onehotencoder__x1_male',\n 'countvectorizer__achem',\n 'countvectorizer__adele',\n 'countvectorizer__allen',\n 'countvectorizer__berg',\n 'countvectorizer__bradley',\n 'countvectorizer__braund',\n 'countvectorizer__briggs',\n 'countvectorizer__cumings',\n 'countvectorizer__elisabeth',\n 'countvectorizer__florence',\n 'countvectorizer__futrelle',\n 'countvectorizer__gosta',\n 'countvectorizer__harris',\n 'countvectorizer__heath',\n 'countvectorizer__heikkinen',\n 'countvectorizer__henry',\n 'countvectorizer__jacques',\n 'countvectorizer__james',\n 'countvectorizer__john',\n 'countvectorizer__johnson',\n 'countvectorizer__laina',\n 'countvectorizer__leonard',\n 'countvectorizer__lily',\n 'countvectorizer__master',\n 'countvectorizer__may',\n 'countvectorizer__mccarthy',\n 'countvectorizer__miss',\n 'countvectorizer__moran',\n 'countvectorizer__mr',\n 'countvectorizer__mrs',\n 'countvectorizer__nasser',\n 'countvectorizer__nicholas',\n 'countvectorizer__oscar',\n 'countvectorizer__owen',\n 'countvectorizer__palsson',\n 'countvectorizer__peel',\n 'countvectorizer__thayer',\n 'countvectorizer__timothy',\n 'countvectorizer__vilhelmina',\n 'countvectorizer__william',\n 'Parch',\n 'Fare']\n\n\n\n\n\n\n\n\nOutput columns:\n\nColumns 1-3: Embarked\nColumns 4-5: Sex\nColumns 6-45: Name\nColumn 46: Parch\nColumn 47: Fare\n\n\n\n\nNow we‚Äôll update the Pipeline to use the modified ColumnTransformer.\n\npipe = make_pipeline(ct, logreg)\n\nThen we can fit the Pipeline, which displays our Pipeline diagram.\n\npipe.fit(X, y)\n\nPipelinePipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('onehotencoder',\n                                                  OneHotEncoder(),\n                                                  ['Embarked', 'Sex']),\n                                                 ('countvectorizer',\n                                                  CountVectorizer(), 'Name'),\n                                                 ('passthrough', 'passthrough',\n                                                  ['Parch', 'Fare'])])),\n                ('logisticregression',\n                 LogisticRegression(random_state=1, solver='liblinear'))])columntransformer: ColumnTransformerColumnTransformer(transformers=[('onehotencoder', OneHotEncoder(),\n                                 ['Embarked', 'Sex']),\n                                ('countvectorizer', CountVectorizer(), 'Name'),\n                                ('passthrough', 'passthrough',\n                                 ['Parch', 'Fare'])])onehotencoder['Embarked', 'Sex']OneHotEncoderOneHotEncoder()countvectorizerNameCountVectorizerCountVectorizer()passthrough['Parch', 'Fare']passthroughpassthroughLogisticRegressionLogisticRegression(random_state=1, solver='liblinear')\n\n\nOur last step before prediction is to update X_new to include the Name column.\n\nX_new = df_new[cols]\n\nAnd finally, we‚Äôll use the fitted Pipeline to make predictions for X_new.\n\npipe.predict(X_new)\n\narray([0, 1, 0, 0, 1, 0, 1, 0, 1, 0])\n\n\nWe‚Äôve now accomplished our goal, which is to include the Name column in our model.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Encoding text data</span>"
    ]
  },
  {
    "objectID": "ch06.html#qa-why-is-the-document-term-matrix-stored-as-a-sparse-matrix",
    "href": "ch06.html#qa-why-is-the-document-term-matrix-stored-as-a-sparse-matrix",
    "title": "6¬† Encoding text data",
    "section": "6.3 Q&A: Why is the document-term matrix stored as a sparse matrix?",
    "text": "6.3 Q&A: Why is the document-term matrix stored as a sparse matrix?\nJust like OneHotEncoder, CountVectorizer outputs a sparse matrix by default. To explore why it does this, let‚Äôs create a Python list of two short text documents and call it text.\n\ntext = ['Machine Learning is fun', 'I am learning Machine Learning']\n\nWe‚Äôll pass those documents to the fit_transform method of CountVectorizer, make it dense with the toarray method, and then convert it to a DataFrame. As you can see, the word ‚ÄúI‚Äù was ignored because it‚Äôs only one character, and there‚Äôs a 2 under ‚Äúlearning‚Äù because the word ‚Äúlearning‚Äù appears twice in the second document.\n\npd.DataFrame(vect.fit_transform(text).toarray(),\n             columns=vect.get_feature_names())\n\n\n\n\n\n\n\n\nam\nfun\nis\nlearning\nmachine\n\n\n\n\n0\n0\n1\n1\n1\n1\n\n\n1\n1\n0\n0\n2\n1\n\n\n\n\n\n\n\nNow let‚Äôs use CountVectorizer on the same text to output a sparse matrix instead, which is the default representation. The matrix is 2 rows by 5 columns, and there are 7 stored elements, meaning 7 non-zero values in the matrix.\n\ndtm = vect.fit_transform(text)\ndtm\n\n&lt;2x5 sparse matrix of type '&lt;class 'numpy.int64'&gt;'\n    with 7 stored elements in Compressed Sparse Row format&gt;\n\n\nWe can actually see those 7 elements by using the print function. It turns out that a sparse matrix only stores the positions of the non-zero values and the values at those positions. In contrast, a dense matrix stores every value, whether or not it‚Äôs zero.\n\nprint(dtm)\n\n  (0, 4)    1\n  (0, 3)    1\n  (0, 2)    1\n  (0, 1)    1\n  (1, 4)    1\n  (1, 3)    2\n  (1, 0)    1\n\n\nAs you might imagine, most elements in a typical document-term matrix are zero. This is because a collection of documents tends to have a large number of unique words, whereas any given document in that collection only contains a small fraction of those words.\nWhen most elements in a matrix are zero, a sparse representation requires far less storage space than a dense representation and is also more performant. This is the same reason that OneHotEncoder outputs a sparse matrix, since its output also tends to be mostly zeros.\n\n\n\n\n\n\nPreferred matrix representation:\n\nMost elements are zero: Sparse matrix\nMost elements are non-zero: Dense matrix",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Encoding text data</span>"
    ]
  },
  {
    "objectID": "ch06.html#qa-what-happens-if-the-testing-data-includes-new-words",
    "href": "ch06.html#qa-what-happens-if-the-testing-data-includes-new-words",
    "title": "6¬† Encoding text data",
    "section": "6.4 Q&A: What happens if the testing data includes new words?",
    "text": "6.4 Q&A: What happens if the testing data includes new words?\nIn the previous lesson, we created two short text documents.\n\ntext\n\n['Machine Learning is fun', 'I am learning Machine Learning']\n\n\nLet‚Äôs pretend that those documents were our training data. If we passed those documents to the fit_transform method of CountVectorizer, these are the 5 features that would be learned.\n\ndtm = vect.fit_transform(text)\nvect.get_feature_names()\n\n['am', 'fun', 'is', 'learning', 'machine']\n\n\nNow, let‚Äôs create another short text document to act as our testing data. It includes two words, ‚Äúis‚Äù and ‚Äúfun‚Äù, that were in the training data, and two words, ‚Äúdata‚Äù and ‚Äúscience‚Äù, that were not in the training data.\n\ntext_new = ['Data Science is FUN!']\n\nAs we‚Äôve discussed throughout this book, your testing data needs to have the same columns as your training data. In other words, we need to create a document-term matrix from the testing data that has the same 5 columns as the training data.\nTo do this, we‚Äôll pass the testing data to the transform method, and it will build a matrix using the features it learned during the fit step.\n\nvect.transform(text_new).toarray()\n\narray([[0, 1, 1, 0, 0]])\n\n\nIn other words, the vectorizer learned its vocabulary from the training data, and it uses that same vocabulary when creating the document-term matrix for the testing data.\n\n\n\n\n\n\nCountVectorizer methods:\n\nfit: Learn the vocabulary\ntransform: Create the document-term matrix using that vocabulary\n\n\n\n\nAs you can see by comparing the output to the feature names, the vectorizer only learned the words ‚Äúfun‚Äù and ‚Äúis‚Äù from the testing data. The words ‚Äúdata‚Äù and ‚Äúscience‚Äù were ignored because they were not seen in the training data.\nIgnoring unknown words actually makes intuitive sense, because if a word wasn‚Äôt seen during training, then you don‚Äôt know anything about the relationship between that word and the target variable. This is similar to setting the OneHotEncoder‚Äôs handle_unknown parameter to 'ignore', since that ignores unknown categories encountered during the transform step by encoding them as all zeros.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Encoding text data</span>"
    ]
  },
  {
    "objectID": "ch06.html#qa-how-do-i-vectorize-multiple-columns-of-text",
    "href": "ch06.html#qa-how-do-i-vectorize-multiple-columns-of-text",
    "title": "6¬† Encoding text data",
    "section": "6.5 Q&A: How do I vectorize multiple columns of text?",
    "text": "6.5 Q&A: How do I vectorize multiple columns of text?\nLet‚Äôs take a look at the Name and Ticket columns from the Titanic DataFrame. Even though some of the Ticket values are entirely numeric, they‚Äôre actually all stored as strings.\n\ndf[['Name', 'Ticket']]\n\n\n\n\n\n\n\n\nName\nTicket\n\n\n\n\n0\nBraund, Mr. Owen Harris\nA/5 21171\n\n\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nPC 17599\n\n\n2\nHeikkinen, Miss. Laina\nSTON/O2. 3101282\n\n\n3\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\n113803\n\n\n4\nAllen, Mr. William Henry\n373450\n\n\n5\nMoran, Mr. James\n330877\n\n\n6\nMcCarthy, Mr. Timothy J\n17463\n\n\n7\nPalsson, Master. Gosta Leonard\n349909\n\n\n8\nJohnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)\n347742\n\n\n9\nNasser, Mrs. Nicholas (Adele Achem)\n237736\n\n\n\n\n\n\n\nIf we wanted to apply CountVectorizer to both columns so that we could include both in our model, how would we do it?\nFirst, let‚Äôs try applying CountVectorizer separately. Vectorizing Name creates a 10 by 40 matrix, and vectorizing Ticket creates a 10 by 13 matrix.\n\nvect.fit_transform(df['Name'])\n\n&lt;10x40 sparse matrix of type '&lt;class 'numpy.int64'&gt;'\n    with 46 stored elements in Compressed Sparse Row format&gt;\n\n\n\nvect.fit_transform(df['Ticket'])\n\n&lt;10x13 sparse matrix of type '&lt;class 'numpy.int64'&gt;'\n    with 13 stored elements in Compressed Sparse Row format&gt;\n\n\nWhat we want is to stack these matrices side-by-side as a 10 by 53 matrix.\nOne idea would be to pass both columns as a DataFrame to CountVectorizer, which is how we would transform multiple columns with OneHotEncoder, for example. However, you‚Äôll see that the output is not what we had hoped for. This is because CountVectorizer expects 1-dimensional input, and we passed it 2-dimensional input instead.\n\nvect.fit_transform(df[['Name', 'Ticket']])\n\n&lt;2x2 sparse matrix of type '&lt;class 'numpy.int64'&gt;'\n    with 2 stored elements in Compressed Sparse Row format&gt;\n\n\nTo actually get the result we‚Äôre looking for, we have to pass each column separately to make_column_transformer. And you‚Äôll see that it does indeed output a 10 by 53 matrix.\nTo be clear, it‚Äôs not problematic to use the same CountVectorizer object twice, since it will still learn two separate vocabularies.\n\nct = make_column_transformer(\n    (vect, 'Name'),\n    (vect, 'Ticket'))\n\n\nct.fit_transform(df)\n\n&lt;10x53 sparse matrix of type '&lt;class 'numpy.int64'&gt;'\n    with 59 stored elements in Compressed Sparse Row format&gt;\n\n\nRecall that make_column_transformer assigns names to all transformers. Normally the assigned name would be ‚Äúcountvectorizer‚Äù (all lowercase), but it can‚Äôt give both transformers the same name. Instead, it appends numbers at the end, as you can see from the diagram.\n\nct\n\nColumnTransformerColumnTransformer(transformers=[('countvectorizer-1', CountVectorizer(),\n                                 'Name'),\n                                ('countvectorizer-2', CountVectorizer(),\n                                 'Ticket')])countvectorizer-1NameCountVectorizerCountVectorizer()countvectorizer-2TicketCountVectorizerCountVectorizer()\n\n\nYou can also see these names by running the keys method on the named_transformers_ attribute.\n\nct.named_transformers_.keys()\n\ndict_keys(['countvectorizer-1', 'countvectorizer-2', 'remainder'])\n\n\nIf you wanted to avoid these names, you could instead create the ColumnTransformer using the ColumnTransformer class, since that requires you to assign a custom name to each transformer.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Encoding text data</span>"
    ]
  },
  {
    "objectID": "ch06.html#qa-should-i-one-hot-encode-or-vectorize-categorical-features",
    "href": "ch06.html#qa-should-i-one-hot-encode-or-vectorize-categorical-features",
    "title": "6¬† Encoding text data",
    "section": "6.6 Q&A: Should I one-hot encode or vectorize categorical features?",
    "text": "6.6 Q&A: Should I one-hot encode or vectorize categorical features?\nLet‚Äôs say you have categorical features which only contain one word, such as Embarked and Sex. We‚Äôve been using OneHotEncoder to encode them, but should we use CountVectorizer instead? Let‚Äôs try it out and see what happens.\n\ndf[['Embarked', 'Sex']]\n\n\n\n\n\n\n\n\nEmbarked\nSex\n\n\n\n\n0\nS\nmale\n\n\n1\nC\nfemale\n\n\n2\nS\nfemale\n\n\n3\nS\nfemale\n\n\n4\nS\nmale\n\n\n5\nQ\nmale\n\n\n6\nS\nmale\n\n\n7\nS\nmale\n\n\n8\nS\nfemale\n\n\n9\nC\nfemale\n\n\n\n\n\n\n\nIf we use CountVectorizer on the Sex column, it produces the exact same output as the OneHotEncoder.\n\nvect.fit_transform(df['Sex']).toarray()\n\narray([[0, 1],\n       [1, 0],\n       [1, 0],\n       [1, 0],\n       [0, 1],\n       [0, 1],\n       [0, 1],\n       [0, 1],\n       [1, 0],\n       [1, 0]])\n\n\nBut as we saw in the previous lesson, CountVectorizer won‚Äôt do what you expect if you try to encode multiple columns at once, since it expects 1-dimensional input.\n\nvect.fit_transform(df[['Embarked', 'Sex']]).toarray()\n\narray([[1, 0],\n       [0, 1]])\n\n\nIn addition, the default settings for CountVectorizer don‚Äôt allow for one-character tokens, so you would have to modify those settings if you wanted to use it with Embarked.\n\nvect.fit_transform(df['Embarked']).toarray()\n\n\n\nValueError: empty vocabulary; perhaps the documents only contain stop words\n\n\nFinally, OneHotEncoder lets you decide how you want to handle categories that weren‚Äôt seen during training (using the handle_unknown parameter), whereas CountVectorizer doesn‚Äôt provide that option and will always ignore words that it didn‚Äôt see during training.\nIn summary, OneHotEncoder is the better encoding mechanism for any data that you would consider categorical, since it can encode multiple columns at once, it allows one-character category names by default, and it provides more options for handling unknown categories.\n\n\n\n\n\n\nAdvantages of OneHotEncoder for categorical data:\n\nEncodes multiple columns at once\nAllows one-character category names\nGives more options for handling unknown categories",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Encoding text data</span>"
    ]
  },
  {
    "objectID": "ch07.html",
    "href": "ch07.html",
    "title": "7¬† Handling missing values",
    "section": "",
    "text": "7.1 Introduction to missing values\nIn this chapter, we‚Äôre going to deal with an issue that is common in real datasets, namely missing values.\nA missing value is simply a value that does not exist in the dataset. It might be missing because that value purposefully wasn‚Äôt collected for that sample, or it might be missing due to an error in the data collection process.\nLet‚Äôs see an example in the Titanic dataset. We want to use Age as a feature, but note that it has a missing value, encoded as NaN. This stands for ‚Äúnot a number‚Äù, and it‚Äôs how missing values are typically encoded in a pandas DataFrame.\ndf\n\n\n\n\n\n\n\n\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n5\n0\n3\nMoran, Mr. James\nmale\nNaN\n0\n0\n330877\n8.4583\nNaN\nQ\n\n\n6\n0\n1\nMcCarthy, Mr. Timothy J\nmale\n54.0\n0\n0\n17463\n51.8625\nE46\nS\n\n\n7\n0\n3\nPalsson, Master. Gosta Leonard\nmale\n2.0\n3\n1\n349909\n21.0750\nNaN\nS\n\n\n8\n1\n3\nJohnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)\nfemale\n27.0\n0\n2\n347742\n11.1333\nNaN\nS\n\n\n9\n1\n2\nNasser, Mrs. Nicholas (Adele Achem)\nfemale\n14.0\n1\n0\n237736\n30.0708\nNaN\nC\nWhen we use the phrase ‚Äúmissing values‚Äù, we‚Äôre talking about NaNs only. The phrase ‚Äúmissing values‚Äù does not refer to categories or words in new data that weren‚Äôt seen during training. For example, if our new data contained the value ‚ÄúZ‚Äù in the Embarked column, that is not called a ‚Äúmissing value‚Äù, rather that‚Äôs called ‚Äúan unknown category‚Äù or ‚Äúa category you didn‚Äôt see during training‚Äù.\nTo start our exploration of this topic, let‚Äôs see what happens if we try to add the Age column to our model.\nFirst, we‚Äôll add Age to the cols list and use that to modify the X DataFrame. Again, notice the missing value in Age.\ncols = ['Parch', 'Fare', 'Embarked', 'Sex', 'Name', 'Age']\nX = df[cols]\nX\n\n\n\n\n\n\n\n\nParch\nFare\nEmbarked\nSex\nName\nAge\n\n\n\n\n0\n0\n7.2500\nS\nmale\nBraund, Mr. Owen Harris\n22.0\n\n\n1\n0\n71.2833\nC\nfemale\nCumings, Mrs. John Bradley (Florence Briggs Th...\n38.0\n\n\n2\n0\n7.9250\nS\nfemale\nHeikkinen, Miss. Laina\n26.0\n\n\n3\n0\n53.1000\nS\nfemale\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\n35.0\n\n\n4\n0\n8.0500\nS\nmale\nAllen, Mr. William Henry\n35.0\n\n\n5\n0\n8.4583\nQ\nmale\nMoran, Mr. James\nNaN\n\n\n6\n0\n51.8625\nS\nmale\nMcCarthy, Mr. Timothy J\n54.0\n\n\n7\n1\n21.0750\nS\nmale\nPalsson, Master. Gosta Leonard\n2.0\n\n\n8\n2\n11.1333\nS\nfemale\nJohnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)\n27.0\n\n\n9\n0\n30.0708\nC\nfemale\nNasser, Mrs. Nicholas (Adele Achem)\n14.0\nThen, we‚Äôll add Age to the ColumnTransformer as a passthrough column.\nct = make_column_transformer(\n    (ohe, ['Embarked', 'Sex']),\n    (vect, 'Name'),\n    ('passthrough', ['Parch', 'Fare', 'Age']))\nThen, we‚Äôll update the Pipeline to include the modified ColumnTransformer.\npipe = make_pipeline(ct, logreg)\nFinally, we‚Äôll attempt to fit the Pipeline, but it throws an error due to the presence of a missing value.\npipe.fit(X, y)\nValueError: Input contains NaN\nscikit-learn models generally don‚Äôt accept data with missing values, with the exception of histogram-based gradient boosting trees. As such, we‚Äôll need to figure out a way to handle the missing value if we want to include Age in our model.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Handling missing values</span>"
    ]
  },
  {
    "objectID": "ch07.html#introduction-to-missing-values",
    "href": "ch07.html#introduction-to-missing-values",
    "title": "7¬† Handling missing values",
    "section": "",
    "text": "Common sources of missing values:\n\nValue purposefully wasn‚Äôt collected\nError in the data collection process\n\n\n\n\n\n\n\n\n\n\n\n\n\nMissing values vs unknown categories:\n\nMissing value: Value encoded as NaN\nUnknown category: Category not seen in the training data",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Handling missing values</span>"
    ]
  },
  {
    "objectID": "ch07.html#three-ways-to-handle-missing-values",
    "href": "ch07.html#three-ways-to-handle-missing-values",
    "title": "7¬† Handling missing values",
    "section": "7.2 Three ways to handle missing values",
    "text": "7.2 Three ways to handle missing values\nLet‚Äôs talk about three different ways we can handle missing values in our dataset.\nThe first way is to drop any rows from the DataFrame that have missing values. We can use the dropna method in pandas to do this, and you‚Äôll notice that row 5 is now gone. Note that you would also need to drop the same row from y.\n\nX.dropna()\n\n\n\n\n\n\n\n\nParch\nFare\nEmbarked\nSex\nName\nAge\n\n\n\n\n0\n0\n7.2500\nS\nmale\nBraund, Mr. Owen Harris\n22.0\n\n\n1\n0\n71.2833\nC\nfemale\nCumings, Mrs. John Bradley (Florence Briggs Th...\n38.0\n\n\n2\n0\n7.9250\nS\nfemale\nHeikkinen, Miss. Laina\n26.0\n\n\n3\n0\n53.1000\nS\nfemale\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\n35.0\n\n\n4\n0\n8.0500\nS\nmale\nAllen, Mr. William Henry\n35.0\n\n\n6\n0\n51.8625\nS\nmale\nMcCarthy, Mr. Timothy J\n54.0\n\n\n7\n1\n21.0750\nS\nmale\nPalsson, Master. Gosta Leonard\n2.0\n\n\n8\n2\n11.1333\nS\nfemale\nJohnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)\n27.0\n\n\n9\n0\n30.0708\nC\nfemale\nNasser, Mrs. Nicholas (Adele Achem)\n14.0\n\n\n\n\n\n\n\nHowever, there are three main problems with this approach:\n\nFirst, if a high proportion of your rows have missing values, then this approach is impractical because it will discard too much of your training data.\nSecond, if there‚Äôs a useful pattern in the ‚Äúmissingness‚Äù, then dropping the rows will obscure this pattern.\nThird, this takes care of the training data, but it doesn‚Äôt help you deal with any missing values that might appear in new data.\n\n\n\n\n\n\n\nApproach 1: Drop rows with missing values\n\nMay discard too much training data\nMay obscure a pattern in the ‚Äúmissingness‚Äù\nDoesn‚Äôt help you with new data\n\n\n\n\nA second option for handling missing values is to drop any columns that have missing values. Again, we can use the dropna method to do this if we set the axis parameter to 'columns'. You can see that row 5 is back, but the Age column is now gone.\n\nX.dropna(axis='columns')\n\n\n\n\n\n\n\n\nParch\nFare\nEmbarked\nSex\nName\n\n\n\n\n0\n0\n7.2500\nS\nmale\nBraund, Mr. Owen Harris\n\n\n1\n0\n71.2833\nC\nfemale\nCumings, Mrs. John Bradley (Florence Briggs Th...\n\n\n2\n0\n7.9250\nS\nfemale\nHeikkinen, Miss. Laina\n\n\n3\n0\n53.1000\nS\nfemale\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\n\n\n4\n0\n8.0500\nS\nmale\nAllen, Mr. William Henry\n\n\n5\n0\n8.4583\nQ\nmale\nMoran, Mr. James\n\n\n6\n0\n51.8625\nS\nmale\nMcCarthy, Mr. Timothy J\n\n\n7\n1\n21.0750\nS\nmale\nPalsson, Master. Gosta Leonard\n\n\n8\n2\n11.1333\nS\nfemale\nJohnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)\n\n\n9\n0\n30.0708\nC\nfemale\nNasser, Mrs. Nicholas (Adele Achem)\n\n\n\n\n\n\n\nThe main problem with this approach is that you‚Äôre discarding a feature that may be useful to your model.\n\n\n\n\n\n\nApproach 2: Drop columns with missing values\n\nMay discard useful features\n\n\n\n\nA third option for handling missing values is to impute missing values. Imputation means that you‚Äôre filling in missing values based on what you know from the non-missing data. Before proceeding with imputation, it‚Äôs important to carefully consider its costs and benefits:\n\nThe benefit is that you‚Äôre able to keep more samples and features in your dataset, which may help to improve your model.\nThe cost is that you‚Äôre inserting values that may not match the true, unknown values, which may make your model less reliable.\n\n\n\n\n\n\n\nApproach 3: Impute missing values\n\nBenefit: Keeps more samples and features\nCost: Imputed values may not match the true values\n\n\n\n\nWhen making this decision, here are some useful factors to consider:\n\nHow important are the particular samples that imputation would allow you to keep?\nHow important are the particular features that imputation would allow you to keep?\nWhat percentage of the values in a feature would need to be imputed?\nAre there samples or features without missing values that are highly correlated with the samples or features with missing values, such that the model wouldn‚Äôt be negatively affected by dropping those samples or features?\nIs the missingness random, or is there a useful pattern in the missingness that would be lost if those samples or features were dropped?\n\n\n\n\n\n\n\nFactors to consider before imputing:\n\nHow important are the samples?\nHow important are the features?\nWhat percentage of values would need to be imputed?\nAre there other samples or features that contain the same information?\nIs the missingness random?\n\n\n\n\nImputation is a complex topic, and ultimately I can‚Äôt tell you whether you should impute missing values in your particular situation. Instead, I‚Äôm going to show you how to do imputation in scikit-learn, and you can decide whether or not to do it. As well, I‚Äôll provide some advice for imputation in lesson 7.6.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Handling missing values</span>"
    ]
  },
  {
    "objectID": "ch07.html#missing-value-imputation",
    "href": "ch07.html#missing-value-imputation",
    "title": "7¬† Handling missing values",
    "section": "7.3 Missing value imputation",
    "text": "7.3 Missing value imputation\nIn this lesson, we‚Äôre going to perform missing value imputation on the Age column so that we can include it in our model. There are three different imputers that we can use in scikit-learn, but we‚Äôre going to start with SimpleImputer and I‚Äôll show you the others later in the chapter.\nFirst, we‚Äôll import SimpleImputer from the impute module and create an instance called imp.\n\nfrom sklearn.impute import SimpleImputer\nimp = SimpleImputer()\n\nThen, we‚Äôll pass the Age column to the fit_transform method to perform the imputation. Note that SimpleImputer, like most transformers, requires 2-dimensional input, which is why there are double brackets around Age.\n\nimp.fit_transform(X[['Age']])\n\narray([[22.        ],\n       [38.        ],\n       [26.        ],\n       [35.        ],\n       [35.        ],\n       [28.11111111],\n       [54.        ],\n       [ 2.        ],\n       [27.        ],\n       [14.        ]])\n\n\nBy default, SimpleImputer fills any missing values with the mean of the non-missing values, which is 28.11 in this case. It also supports other imputation strategies, namely the median value, the most frequent value, and a user-defined value. Note that all of these strategies can be applied to numeric features, but only the most frequent and user-defined strategies can be applied to categorical features.\n\n\n\n\n\n\nSimple imputation strategies:\n\nMean value\nMedian value\nMost frequent value\nUser-defined value\n\n\n\n\nIn order to confirm what value was imputed, we can examine the statistics_ attribute, which was learned from the data during the fit step.\n\nimp.statistics_\n\narray([28.11111111])\n\n\nNow that we know how SimpleImputer works, we‚Äôll update the ColumnTransformer to include imputation of the Age column. As a reminder, brackets are required around Age because SimpleImputer expects 2-dimensional input, whereas brackets are not allowed around Name because CountVectorizer expects 1-dimensional input.\n\nct = make_column_transformer(\n    (ohe, ['Embarked', 'Sex']),\n    (vect, 'Name'),\n    (imp, ['Age']),\n    ('passthrough', ['Parch', 'Fare']))\n\nNow we‚Äôll run the fit_transform method, and you can see that there are now 48 columns in the feature matrix, whereas in the last chapter there were 47.\n\nct.fit_transform(X)\n\n&lt;10x48 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n    with 88 stored elements in Compressed Sparse Row format&gt;\n\n\nNext, we‚Äôll update the Pipeline to include the revised ColumnTransformer, and fit it on X and y. There was no error this time because the one missing value in X was imputed.\n\npipe = make_pipeline(ct, logreg)\npipe.fit(X, y)\n\nPipelinePipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('onehotencoder',\n                                                  OneHotEncoder(),\n                                                  ['Embarked', 'Sex']),\n                                                 ('countvectorizer',\n                                                  CountVectorizer(), 'Name'),\n                                                 ('simpleimputer',\n                                                  SimpleImputer(), ['Age']),\n                                                 ('passthrough', 'passthrough',\n                                                  ['Parch', 'Fare'])])),\n                ('logisticregression',\n                 LogisticRegression(random_state=1, solver='liblinear'))])columntransformer: ColumnTransformerColumnTransformer(transformers=[('onehotencoder', OneHotEncoder(),\n                                 ['Embarked', 'Sex']),\n                                ('countvectorizer', CountVectorizer(), 'Name'),\n                                ('simpleimputer', SimpleImputer(), ['Age']),\n                                ('passthrough', 'passthrough',\n                                 ['Parch', 'Fare'])])onehotencoder['Embarked', 'Sex']OneHotEncoderOneHotEncoder()countvectorizerNameCountVectorizerCountVectorizer()simpleimputer['Age']SimpleImputerSimpleImputer()passthrough['Parch', 'Fare']passthroughpassthroughLogisticRegressionLogisticRegression(random_state=1, solver='liblinear')\n\n\nBefore we make predictions, we‚Äôre going to examine the Pipeline to confirm the imputation value for Age that was learned from X:\n\nFirst, we access the columntransformer step of the Pipeline using the named_steps attribute.\nThen, from that Pipeline step, we access the simpleimputer transformer using the named_transformers_ attribute.\nFinally, from that transformer, we access the statistics_ attribute.\n\nThis confirms what we saw previously, which is that the imputer learned a value of 28.11 from the Age column.\n\n(pipe.named_steps['columntransformer']\n     .named_transformers_['simpleimputer']\n     .statistics_)\n\narray([28.11111111])\n\n\nNext, we‚Äôll update the X_new DataFrame to use the same columns as X.\n\nX_new = df_new[cols]\nX_new\n\n\n\n\n\n\n\n\nParch\nFare\nEmbarked\nSex\nName\nAge\n\n\n\n\n0\n0\n7.8292\nQ\nmale\nKelly, Mr. James\n34.5\n\n\n1\n0\n7.0000\nS\nfemale\nWilkes, Mrs. James (Ellen Needs)\n47.0\n\n\n2\n0\n9.6875\nQ\nmale\nMyles, Mr. Thomas Francis\n62.0\n\n\n3\n0\n8.6625\nS\nmale\nWirz, Mr. Albert\n27.0\n\n\n4\n1\n12.2875\nS\nfemale\nHirvonen, Mrs. Alexander (Helga E Lindqvist)\n22.0\n\n\n5\n0\n9.2250\nS\nmale\nSvensson, Mr. Johan Cervin\n14.0\n\n\n6\n0\n7.6292\nQ\nfemale\nConnolly, Miss. Kate\n30.0\n\n\n7\n1\n29.0000\nS\nmale\nCaldwell, Mr. Albert Francis\n26.0\n\n\n8\n0\n7.2292\nC\nfemale\nAbrahim, Mrs. Joseph (Sophie Halaut Easu)\n18.0\n\n\n9\n0\n24.1500\nS\nmale\nDavies, Mr. John Samuel\n21.0\n\n\n\n\n\n\n\nFinally, we‚Äôll use the fitted Pipeline to make predictions for X_new.\n\npipe.predict(X_new)\n\narray([0, 0, 0, 0, 1, 0, 1, 0, 1, 0])\n\n\nWith respect to imputation, it‚Äôs worth talking about exactly what happens during the predict step. Since X_new didn‚Äôt have any missing values in the Age column, then nothing got imputed during prediction.\nBut let‚Äôs pretend that X_new did have a missing value in Age. If that was the case, then the imputed value would have been the mean of Age in X, which is 28.11, not the mean of Age in X_new. This is critically important because a transformer is only allowed to learn from the training data, and then apply what it learned to both the training and new data.\n\n\n\n\n\n\nWhat would have been imputed for Age in X_new?\n\nImputed value would be the mean of Age in X, not the mean of Age in X_new\nTransformer is only allowed to learn from the training data\n\n\n\n\nAs we‚Äôve already seen in this book, the OneHotEncoder learns its categories from the training data, the CountVectorizer learns its vocabulary from the training data, and similarly, the SimpleImputer learns its imputation value from the training data. This is one of the main reasons why we run fit_transform on the training data but transform only on the new data.\n\n\n\n\n\n\nWhat do transformers learn from the training data?\n\nOneHotEncoder: Learns categories\nCountVectorizer: Learns vocabulary\nSimpleImputer: Learns imputation value\n\n\n\n\nIf you‚Äôre struggling with this concept, here‚Äôs another way of looking at it that might be helpful to you. Pretend for a second that X_new only contained a single sample, and that sample had a missing Age value. If you passed that X_new to the predict method, it‚Äôs clear that scikit-learn has to look to the training data for the imputation value, since there would be no other Age values in X_new for it to examine.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Handling missing values</span>"
    ]
  },
  {
    "objectID": "ch07.html#sec-7-4",
    "href": "ch07.html#sec-7-4",
    "title": "7¬† Handling missing values",
    "section": "7.4 Using ‚Äúmissingness‚Äù as a feature",
    "text": "7.4 Using ‚Äúmissingness‚Äù as a feature\nWhen imputing missing values, it‚Äôs also possible to use ‚Äúmissingness‚Äù as a feature of its own.\nStarting in scikit-learn version 0.21, we can set the add_indicator parameter to True when creating a SimpleImputer.\n\nimp_indicator = SimpleImputer(add_indicator=True)\n\nThen when we use fit_transform, a separate column known as a ‚Äúmissing indicator‚Äù is included in the output matrix, and it indicates the presence of missing values.\n\nimp_indicator.fit_transform(X[['Age']])\n\narray([[22.        ,  0.        ],\n       [38.        ,  0.        ],\n       [26.        ,  0.        ],\n       [35.        ,  0.        ],\n       [35.        ,  0.        ],\n       [28.11111111,  1.        ],\n       [54.        ,  0.        ],\n       [ 2.        ,  0.        ],\n       [27.        ,  0.        ],\n       [14.        ,  0.        ]])\n\n\nAdding a missing indicator is useful when the data is not missing at random, since there might be a relationship between the ‚Äúmissingness‚Äù and the target value.\n\n\n\n\n\n\nWhy add a missing indicator?\n\nUseful when the data is not missing at random\nCan encode the relationship between ‚Äúmissingness‚Äù and the target value\n\n\n\n\nFor example, if Age was missing for some samples because older passengers declined to give their ages, and older passengers are more likely to have survived, then there is a relationship between Age being missing and the likelihood of survival. Thus the missingness itself can be a useful feature, and we can include that feature in the model using a missing indicator.\nWe‚Äôre not going to modify our Pipeline to include a missing indicator at this time, but we‚Äôll return to this concept later in the book.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Handling missing values</span>"
    ]
  },
  {
    "objectID": "ch07.html#qa-how-do-i-perform-multivariate-imputation",
    "href": "ch07.html#qa-how-do-i-perform-multivariate-imputation",
    "title": "7¬† Handling missing values",
    "section": "7.5 Q&A: How do I perform multivariate imputation?",
    "text": "7.5 Q&A: How do I perform multivariate imputation?\nSo far in this chapter, we‚Äôve used SimpleImputer for imputation. SimpleImputer does univariate imputation, which means that it only looks at the feature being imputed when deciding what values to impute. Thus when imputing missing values for Age, SimpleImputer only considers the values in the Age column.\nHowever, there‚Äôs another type of imputation called multivariate imputation. The intuition of multivariate imputation is that it can be useful to take other features into account when deciding what value to impute.\nFor example, maybe a high Parch and a low Fare is common for kids. Thus if Age is missing for a row which has a high Parch and a low Fare, then you should impute a low Age rather than the mean of Age, which is what SimpleImputer would do.\nMultivariate imputation is available in scikit-learn via the IterativeImputer and KNNImputer classes, and in this lesson I‚Äôll explain how both of them work.\n\n\n\n\n\n\nTypes of imputation:\n\nUnivariate imputation: Only examines the feature being imputed\n\nSimpleImputer\n\nMultivariate imputation: Takes multiple features into account\n\nIterativeImputer\nKNNImputer\n\n\n\n\n\nIterativeImputer was introduced in scikit-learn version 0.21. It‚Äôs considered experimental, which means that the API and predictions may change in future versions. As such, scikit-learn will only allow you to import the IterativeImputer class from the impute module if you first import the enable_iterative_imputer function from the experimental module. This is scikit-learn‚Äôs way of requiring you to acknowledge that you‚Äôre using experimental functionality.\n\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\nThen, we‚Äôll create an instance of IterativeImputer called imp_iterative, and pass three columns from X to its fit_transform method: Parch, Fare, and Age. Parch and Fare don‚Äôt have any missing values, and Age has 1 missing value. You can see that 24.24 was imputed for the missing value of Age.\n\nimp_iterative = IterativeImputer()\nimp_iterative.fit_transform(X[['Parch', 'Fare', 'Age']])\n\narray([[ 0.        ,  7.25      , 22.        ],\n       [ 0.        , 71.2833    , 38.        ],\n       [ 0.        ,  7.925     , 26.        ],\n       [ 0.        , 53.1       , 35.        ],\n       [ 0.        ,  8.05      , 35.        ],\n       [ 0.        ,  8.4583    , 24.23702669],\n       [ 0.        , 51.8625    , 54.        ],\n       [ 1.        , 21.075     ,  2.        ],\n       [ 2.        , 11.1333    , 27.        ],\n       [ 0.        , 30.0708    , 14.        ]])\n\n\nHere‚Äôs how the IterativeImputer works:\n\nFor the 9 rows in which Age is not missing, scikit-learn trains a regression model in which Parch and Fare are the features and Age is the target.\nFor the 1 row in which Age is missing, scikit-learn passes the Parch and Fare values to the trained model. The model makes a prediction for Age, and that value is used for imputation.\n\nIn summary, IterativeImputer turned this into a regression problem with 2 features, 9 samples of training data, and 1 sample of new data.\n\n\n\n\n\n\nHow IterativeImputer works:\n\nAge not missing: Train a regression model to predict Age using Parch and Fare\nAge missing: Predict Age using trained model\n\n\n\n\nThere are four things I want to comment on about IterativeImputer:\n\nFirst, it only works with numerical features. This is unlike SimpleImputer, which also works with categorical features.\nSecond, you have to decide which features to pass to IterativeImputer. My advice is to use features that you believe are highly correlated with one another.\nThird, you are allowed to pass it multiple features with missing values. In other words, Parch, Fare, and Age could all have missing values. IterativeImputer will just do three different regression problems, and each column will take a turn being the target.\nFourth, you can actually choose which regression model IterativeImputer uses for the regression problem. By default, it uses Bayesian ridge regression.\n\n\n\n\n\n\n\nNotes about IterativeImputer:\n\nOnly works with numerical features\nYou have to decide which features to include\nYou can include multiple features with missing values\nYou can choose the regression model\n\n\n\n\nAnyway, if you decided you wanted to include it in the ColumnTransformer, you would just specify that Parch, Fare, and Age should be transformed by imp_iterative, and thus there would no longer be any passthrough columns.\n\nct = make_column_transformer(\n    (ohe, ['Embarked', 'Sex']),\n    (vect, 'Name'),\n    (imp_iterative, ['Parch', 'Fare', 'Age']))\n\nOur other option for multivariate imputation is KNNImputer, which was introduced in scikit-learn version 0.22 but is not considered experimental.\nTo use it, we‚Äôll import it from the impute module, create an instance called imp_knn, and then pass the same three columns from X to the fit_transform method. This time, you‚Äôll see that 30.5 was imputed for the missing value of Age.\n\nfrom sklearn.impute import KNNImputer\nimp_knn = KNNImputer(n_neighbors=2)\nimp_knn.fit_transform(X[['Parch', 'Fare', 'Age']])\n\narray([[ 0.    ,  7.25  , 22.    ],\n       [ 0.    , 71.2833, 38.    ],\n       [ 0.    ,  7.925 , 26.    ],\n       [ 0.    , 53.1   , 35.    ],\n       [ 0.    ,  8.05  , 35.    ],\n       [ 0.    ,  8.4583, 30.5   ],\n       [ 0.    , 51.8625, 54.    ],\n       [ 1.    , 21.075 ,  2.    ],\n       [ 2.    , 11.1333, 27.    ],\n       [ 0.    , 30.0708, 14.    ]])\n\n\nHere‚Äôs how the KNNImputer works:\n\nFirst, KNNImputer finds the row in which Age is missing.\nBecause I set the n_neighbors parameter to 2, scikit-learn finds the 2 rows that are nearest to this row in which Age is not missing, measured by how close the Parch and Fare values are to this row. In this case, the third and the fifth rows are the two nearest rows.\nFinally, scikit-learn calculates the mean of the Age values from those 2 rows. In this case, it takes the mean of 26 and 35 which is 30.5, and that value is used as the imputation value.\n\n\n\n\n\n\n\nHow KNNImputer works:\n\nFind the row in which Age is missing\nFind the n_neighbors nearest rows in which Age is not missing\nCalculate the mean of Age from the nearest rows\n\n\n\n\nYou might be wondering how you should choose the value for n_neighbors. As you‚Äôll see later in the book, transformer hyperparameters like this should be chosen through a tuning process in the same way that you would tune the hyperparameters for a model.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Handling missing values</span>"
    ]
  },
  {
    "objectID": "ch07.html#qa-what-are-the-best-practices-for-missing-value-imputation",
    "href": "ch07.html#qa-what-are-the-best-practices-for-missing-value-imputation",
    "title": "7¬† Handling missing values",
    "section": "7.6 Q&A: What are the best practices for missing value imputation?",
    "text": "7.6 Q&A: What are the best practices for missing value imputation?\nMissing value imputation may be considered dubious from a statistics point of view, but if your primary goal is prediction, then imputation has been shown experimentally to work well under certain conditions.\nIn this lesson, I‚Äôm going to summarize what I‚Äôve learned from research papers about the best practices for effective missing value imputation.\n\n\n\n\n\n\nTypes of missing data:\n\nMissing Completely At Random (MCAR):\n\nNo relationship between missingness and underlying data\nExample: Booking agent forgot to gather Age\n\nMissing Not At Random (MNAR):\n\nRelationship between missingness and underlying data\nExample: Older passengers declined to give their Age\n\nMissing due to a structural deficiency:\n\nData omitted for a specific purpose\nExample: Staff members did not pay a Fare\n\n\n\n\n\nThe best practices actually differ depending upon the type of missing data you have, and so I‚Äôm going to start with the first type of missing data, which is known as Missing Completely At Random or ‚ÄúMCAR‚Äù. Missing data is designated as MCAR if there‚Äôs no relationship between the missingness and the underlying data.\nAn example of this would be if Age was missing because the booking agent forgot to gather that information from a few of the passengers. In other words, the data is just as likely to be missing for an older person as a younger person, and thus missingness is not meaningfully related to Age.\nIf you determine that your data is MCAR, here‚Äôs what the research indicates:\n\nIf you have a small dataset, then IterativeImputer tends to be more effective than mean imputation.\nIf you have a large dataset, IterativeImputer and mean imputation tend to work equally well, though IterativeImputer will have a much higher computational cost.\nAnd there‚Äôs no benefit to adding a missing indicator since the missingness is random.\n\n\n\n\n\n\n\nAdvice for MCAR imputation:\n\nSmall dataset: IterativeImputer is more effective than mean imputation\nLarge dataset: IterativeImputer and mean imputation work equally well\nNo benefit to adding a missing indicator\n\n\n\n\nThe next type of missing data is known as Missing Not At Random or ‚ÄúMNAR‚Äù. Missing data is designated as MNAR if there is a relationship between the missingness and the underlying data, which is commonly the case in real-world datasets.\nI mentioned an example of this in a previous lesson, namely if Age was missing because some of the older passengers declined to give their ages. In other words, there is a relationship between missingness and Age.\nIf your data is MNAR, here is what the research indicates:\n\nMean imputation is more effective than IterativeImputer because IterativeImputer actually obscures the pattern that the model might otherwise be able to learn.\nIt‚Äôs important that you add a missing indicator so that the model can learn from the pattern of missingness.\nAnd finally, it‚Äôs recommended to use a powerful, non-linear model, since mean imputation tends not to work as well in combination with a linear model.\n\n\n\n\n\n\n\nAdvice for MNAR imputation:\n\nMean imputation is more effective than IterativeImputer\nAdd a missing indicator\nUse a powerful, non-linear model\n\n\n\n\nThe final type of missing data I‚Äôll mention is data that‚Äôs missing due to a structural deficiency. This means that a value is missing because it was omitted for a specific purpose. An example of this would be if the passenger list included staff members, and their values for the Fare column were listed as missing to indicate that they did not pay a fare.\nIn the case of a structural deficiency, my advice is to impute the most logical and reasonable user-defined value, and also add a missing indicator. For the example I mentioned, it would make the most sense to insert a Fare value of 0 for all staff members.\n\n\n\n\n\n\nAdvice for structural deficiency imputation:\n\nImpute a logical and reasonable user-defined value\nAdd a missing indicator\n\n\n\n\nAll of these examples should make clear that if you‚Äôre going to impute missing values, it‚Äôs important to thoroughly understand your data before choosing an imputation strategy.\nFinally, it‚Äôs worth reiterating that histogram-based gradient boosting trees in scikit-learn have built-in support for missing values. In other words, you can pass it data with missing values, and it will handle them internally. This is significant because it has a lower computation cost than complex imputation strategies like IterativeImputer, and yet it can perform just as well or better than imputation across a variety of missing value scenarios.\n\n\n\n\n\n\nAdvantages of histogram-based gradient boosting trees:\n\nBuilt-in support for missing values\nLower computational cost than IterativeImputer\nPerforms well across many missing value scenarios\n\n\n\n\nTherefore, if you have a large dataset with a lot of missing values, it‚Äôs worth trying out a histogram-based gradient boosting tree as your prediction model and excluding the imputation step, and comparing its performance with any other model that does require an imputation step.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Handling missing values</span>"
    ]
  },
  {
    "objectID": "ch07.html#qa-whats-the-difference-between-columntransformer-and-featureunion",
    "href": "ch07.html#qa-whats-the-difference-between-columntransformer-and-featureunion",
    "title": "7¬† Handling missing values",
    "section": "7.7 Q&A: What‚Äôs the difference between ColumnTransformer and FeatureUnion?",
    "text": "7.7 Q&A: What‚Äôs the difference between ColumnTransformer and FeatureUnion?\nAs we saw earlier in this chapter, you can add a missing indicator to the output of SimpleImputer by setting the add_indicator parameter to True.\n\nimp_indicator = SimpleImputer(add_indicator=True)\nimp_indicator.fit_transform(X[['Age']])\n\narray([[22.        ,  0.        ],\n       [38.        ,  0.        ],\n       [26.        ,  0.        ],\n       [35.        ,  0.        ],\n       [35.        ,  0.        ],\n       [28.11111111,  1.        ],\n       [54.        ,  0.        ],\n       [ 2.        ,  0.        ],\n       [27.        ,  0.        ],\n       [14.        ,  0.        ]])\n\n\nIn this lesson, we‚Äôre going to explore a different way to create this same matrix as a way of learning about the FeatureUnion class.\nTo create the left column of this matrix, we can use the imp object, which is a SimpleImputer without a missing indicator.\n\nimp.fit_transform(X[['Age']])\n\narray([[22.        ],\n       [38.        ],\n       [26.        ],\n       [35.        ],\n       [35.        ],\n       [28.11111111],\n       [54.        ],\n       [ 2.        ],\n       [27.        ],\n       [14.        ]])\n\n\nTo create the right column of the matrix, we can use the MissingIndicator class. We‚Äôll import the class from the impute module and create an instance called indicator.\n\nfrom sklearn.impute import MissingIndicator\nindicator = MissingIndicator()\n\nWhen we pass Age to the fit_transform method, it outputs False and True instead of 0 and 1, but otherwise the results are identical to the right column of the matrix.\n\nindicator.fit_transform(X[['Age']])\n\narray([[False],\n       [False],\n       [False],\n       [False],\n       [False],\n       [ True],\n       [False],\n       [False],\n       [False],\n       [False]])\n\n\nThe final step in recreating our original matrix is to stack these two columns side-by-side, which we can do by using the FeatureUnion class. A FeatureUnion applies multiple transformations to a single input column and stacks the results side-by-side.\nWe‚Äôll import the make_union function from the pipeline module, and then create an instance called union that contains both the imp and indicator objects.\n\nfrom sklearn.pipeline import make_union\nunion = make_union(imp, indicator)\n\nWhen we pass Age to the union‚Äôs fit_transform method, it runs both the imp and indicator transformations and stacks the results side-by-side. Note that False and True are converted to 0 and 1 when included in a numeric array, and thus we‚Äôve recreated our original matrix.\n\nunion.fit_transform(X[['Age']])\n\narray([[22.        ,  0.        ],\n       [38.        ,  0.        ],\n       [26.        ,  0.        ],\n       [35.        ,  0.        ],\n       [35.        ,  0.        ],\n       [28.11111111,  1.        ],\n       [54.        ,  0.        ],\n       [ 2.        ,  0.        ],\n       [27.        ,  0.        ],\n       [14.        ,  0.        ]])\n\n\nAlternatively, we could have achieved the same results using a ColumnTransformer. Specifically, we could have passed the Age column to two separate transformations.\n\nct = make_column_transformer(\n    (imp, ['Age']),\n    (indicator, ['Age']))\nct.fit_transform(X)\n\narray([[22.        ,  0.        ],\n       [38.        ,  0.        ],\n       [26.        ,  0.        ],\n       [35.        ,  0.        ],\n       [35.        ,  0.        ],\n       [28.11111111,  1.        ],\n       [54.        ,  0.        ],\n       [ 2.        ,  0.        ],\n       [27.        ,  0.        ],\n       [14.        ,  0.        ]])\n\n\nStepping back, here‚Äôs a quick comparison between FeatureUnion and ColumnTransformer:\n\nA FeatureUnion works on a single input column, and applies multiple different transformations to that one column in parallel.\nA ColumnTransformer works on multiple input columns, and applies a different transformation to each input column in parallel.\n\n\n\n\n\n\n\nFeatureUnion vs ColumnTransformer:\n\nFeatureUnion:\n\nSingle input column\nApplies multiple different transformations to that column in parallel\n\nColumnTransformer:\n\nMultiple input columns\nApplies a different transformation to each column in parallel\n\n\n\n\n\nYou can see that ColumnTransformer is far more flexible than FeatureUnion, and so my recommendation is that you use ColumnTransformer to do all of your transformations. For the rare case in which you need to apply multiple different transformations in parallel to the same column, you can use either a FeatureUnion or a ColumnTransformer, depending upon which solution makes more sense to you.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Handling missing values</span>"
    ]
  },
  {
    "objectID": "ch08.html",
    "href": "ch08.html",
    "title": "8¬† Fixing common workflow problems",
    "section": "",
    "text": "8.1 Two new problems\nUp to now, we‚Äôve only been working with the first 10 rows of the Titanic dataset to make it easy to examine the input and output of each workflow step. In this chapter, we‚Äôll begin using the full Titanic dataset. This will create a few new problems that are common with real datasets, and we‚Äôll figure out how to handle those problems appropriately.\nWe‚Äôll start by reading the training data into df and reading the new data into df_new, overwriting the existing objects.\nWhen examining the shapes, you can see that df_new has one less column than df because it doesn‚Äôt contain the target column of Survived.\ndf = pd.read_csv('http://bit.ly/MLtrain')\ndf.shape\n\n(891, 11)\ndf_new = pd.read_csv('http://bit.ly/MLnewdata')\ndf_new.shape\n\n(418, 10)\nWe‚Äôll check for missing values in these two DataFrames by chaining together the isna and sum methods. The results tell us how many missing values are present in each column.\nThis reveals two problems we‚Äôll have to handle that weren‚Äôt present in our 10-row datasets. First, Embarked contains missing values in df, and second, Fare contains a missing value in df_new. We‚Äôll spend the rest of this chapter addressing those two problems.\nNote that we don‚Äôt have to worry about missing values in Cabin because we‚Äôre not yet using that as a feature, and our existing workflow already accounts for the missing values in Age.\ndf.isna().sum()\n\nSurvived      0\nPclass        0\nName          0\nSex           0\nAge         177\nSibSp         0\nParch         0\nTicket        0\nFare          0\nCabin       687\nEmbarked      2\ndtype: int64\ndf_new.isna().sum()\n\nPclass        0\nName          0\nSex           0\nAge          86\nSibSp         0\nParch         0\nTicket        0\nFare          1\nCabin       327\nEmbarked      0\ndtype: int64",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Fixing common workflow problems</span>"
    ]
  },
  {
    "objectID": "ch08.html#two-new-problems",
    "href": "ch08.html#two-new-problems",
    "title": "8¬† Fixing common workflow problems",
    "section": "",
    "text": "Features with missing values:\n\nProblematic:\n\nEmbarked: Missing values in df\nFare: Missing value in df_new\n\nNot problematic:\n\nCabin: Not currently using\nAge: Already being imputed",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Fixing common workflow problems</span>"
    ]
  },
  {
    "objectID": "ch08.html#problem-1-missing-values-in-a-categorical-feature",
    "href": "ch08.html#problem-1-missing-values-in-a-categorical-feature",
    "title": "8¬† Fixing common workflow problems",
    "section": "8.2 Problem 1: Missing values in a categorical feature",
    "text": "8.2 Problem 1: Missing values in a categorical feature\nIn this lesson, we‚Äôre going to figure out how to handle the missing values in the Embarked column.\nWe‚Äôll start with a reminder of the six feature columns we‚Äôre using.\n\ncols = ['Parch', 'Fare', 'Embarked', 'Sex', 'Name', 'Age']\n\nWe‚Äôll redefine X and y to use the full dataset rather than the 10-row dataset.\n\nX = df[cols]\ny = df['Survived']\n\nAnd here‚Äôs a reminder of the ColumnTransformer we created in the previous chapter.\n\nct = make_column_transformer(\n    (ohe, ['Embarked', 'Sex']),\n    (vect, 'Name'),\n    (imp, ['Age']),\n    ('passthrough', ['Parch', 'Fare']))\n\nNormally we would pass X to the fit_transform method, but in this case it will error because the Embarked column contains missing values. Our solution will be to impute missing values for Embarked before one-hot encoding it.\n\nct.fit_transform(X)\n\n\n\nValueError: Input contains NaN\n\n\nAs an aside, OneHotEncoder automatically handles missing values by treating them as a new category starting in scikit-learn version 0.24. I‚Äôm using version 0.23, and so I‚Äôll be writing the code to manually treat missing values as a new category. Even if you‚Äôre using version 0.24 or later, I still recommend following my code because what I‚Äôm about to teach you will enable you to solve other similar problems that scikit-learn does not automatically handle.\n\n\n\n\n\n\nHow OneHotEncoder handles missing values:\n\nBefore version 0.24: Errors if the input contains missing values\nStarting in version 0.24: Treats missing values as a new category\n\n\n\n\nAs I was saying, our solution is to impute missing values for Embarked and then one-hot encode it.\nThe first step of this solution is to create a new instance of SimpleImputer, which we‚Äôll call imp_constant. For categorical features, you can either impute the most frequent value or a constant user-defined value. We‚Äôll choose the latter by setting the strategy parameter to 'constant', and the constant value we‚Äôll impute is the string 'missing'.\n\n\n\n\n\n\nImputation strategies for categorical features:\n\nMost frequent value\nUser-defined value\n\n\n\n\n\nimp_constant = SimpleImputer(strategy='constant', fill_value='missing')\n\nNext, we‚Äôll create a two-step Pipeline that only contains transformers. The first step is imputation using our new imputer, and the second step is one-hot encoding. We‚Äôll call this Pipeline imp_ohe to remind us of the two steps it contains.\n\nimp_ohe = make_pipeline(imp_constant, ohe)\n\nWe can test out the imp_ohe Pipeline by passing the Embarked column to its fit_transform method. It outputs four columns because missing values are essentially being treated as a fourth category in addition to C, Q, and S.\n\nimp_ohe.fit_transform(X[['Embarked']])\n\n&lt;891x4 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n    with 891 stored elements in Compressed Sparse Row format&gt;\n\n\nWe can confirm this by accessing the second step of the Pipeline, which is the OneHotEncoder, and then examining the categories_ attribute.\n\nimp_ohe[1].categories_\n\n[array(['C', 'Q', 'S', 'missing'], dtype=object)]\n\n\nIn case it helps you to understand imp_ohe better, I‚Äôm going to show you what happens ‚Äúunder the hood‚Äù when you fit_transform this Pipeline. To be clear, you should not actually write the following code, rather it‚Äôs just for teaching purposes.\nFirst, the imp_constant object imputes a string value of ‚Äúmissing‚Äù for any missing values in the Embarked column. Then, the output of the imputer is one-hot encoded by the ohe object, which outputs four columns.\n\nohe.fit_transform(imp_constant.fit_transform(X[['Embarked']]))\n\n&lt;891x4 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n    with 891 stored elements in Compressed Sparse Row format&gt;\n\n\nNow that we‚Äôve created a transformer-only Pipeline to handle the missing values in Embarked, we‚Äôll simply replace the ohe transformer in our ColumnTransformer with the imp_ohe Pipeline.\n\nct = make_column_transformer(\n    (imp_ohe, ['Embarked', 'Sex']),\n    (vect, 'Name'),\n    (imp, ['Age']),\n    ('passthrough', ['Parch', 'Fare']))\n\nThere are two things I want to note about the imp_ohe Pipeline:\n\nFirst, you‚Äôre only allowed to include transformer objects in a ColumnTransformer, but imp_ohe is eligible because all of its steps are transformers.\nSecond, it‚Äôs completely fine to apply imp_ohe to the Sex column as well as Embarked. There are no missing values in the Sex column, so the imputation step won‚Äôt affect it, and it will simply get passed along to the one-hot encoding step.\n\n\n\n\n\n\n\nNotes about the imp_ohe Pipeline:\n\nTreated like a transformer because all of its steps are transformers\nImputation step won‚Äôt affect the Sex column\n\n\n\n\nBy replacing ohe with imp_ohe, we have now solved the problem of missing values in the Embarked column. Thus, we can pass X to the ColumnTransformer‚Äôs fit_transform method, and it will not throw an error.\nAs an aside, the output matrix is now much wider than before because the Name column of X contains a large number of unique words.\n\nct.fit_transform(X)\n\n&lt;891x1518 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n    with 7328 stored elements in Compressed Sparse Row format&gt;",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Fixing common workflow problems</span>"
    ]
  },
  {
    "objectID": "ch08.html#problem-2-missing-values-in-the-new-data",
    "href": "ch08.html#problem-2-missing-values-in-the-new-data",
    "title": "8¬† Fixing common workflow problems",
    "section": "8.3 Problem 2: Missing values in the new data",
    "text": "8.3 Problem 2: Missing values in the new data\nNow that we‚Äôve solved our first problem, we‚Äôre going to move on to the second problem, which is the missing values in the Fare column. Recall that Fare has missing values in X_new but not in X, and thus our modeling Pipeline would error when making predictions for X_new if we don‚Äôt account for these missing values.\nOur solution to this problem is to impute missing values for Fare. The ColumnTransformer already contains an imputer that does mean imputation, so we‚Äôll apply the existing imputer to the Fare column, whereas previously Fare was a passthrough column. This is actually all that is required to solve our problem.\n\nct = make_column_transformer(\n    (imp_ohe, ['Embarked', 'Sex']),\n    (vect, 'Name'),\n    (imp, ['Age', 'Fare']),\n    ('passthrough', ['Parch']))\n\nNow, we‚Äôll pass X to the fit_transform method of the ColumnTransformer. It will output the same number of columns as it did before, since Fare just moved from a passthrough column to a transformed column.\n\nct.fit_transform(X)\n\n&lt;891x1518 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n    with 7328 stored elements in Compressed Sparse Row format&gt;\n\n\nTo be clear, the Fare column does not have any missing values in X, thus the imputer did not impute any values for Fare during the fit_transform. However, it did learn the mean of Fare in X, which is the imputation value that will be applied to the Fare column of X_new during prediction.\n\n\n\n\n\n\nWhat will be imputed for Fare?\n\nX: No missing Fare values, thus no imputation of Fare\nX_new: Missing Fare value, thus impute the mean of Fare in X during prediction\n\n\n\n\nNext, we‚Äôll update our modeling Pipeline to include the revised ColumnTransformer, and fit it on X and y. You can see from the diagram that there‚Äôs now a transformer Pipeline within the ColumnTransformer, which is within the modeling Pipeline.\n\npipe = make_pipeline(ct, logreg)\npipe.fit(X, y)\n\nPipelinePipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('pipeline',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer(fill_value='missing',\n                                                                                 strategy='constant')),\n                                                                  ('onehotencoder',\n                                                                   OneHotEncoder())]),\n                                                  ['Embarked', 'Sex']),\n                                                 ('countvectorizer',\n                                                  CountVectorizer(), 'Name'),\n                                                 ('simpleimputer',\n                                                  SimpleImputer(),\n                                                  ['Age', 'Fare']),\n                                                 ('passthrough', 'passthrough',\n                                                  ['Parch'])])),\n                ('logisticregression',\n                 LogisticRegression(random_state=1, solver='liblinear'))])columntransformer: ColumnTransformerColumnTransformer(transformers=[('pipeline',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer(fill_value='missing',\n                                                                strategy='constant')),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder())]),\n                                 ['Embarked', 'Sex']),\n                                ('countvectorizer', CountVectorizer(), 'Name'),\n                                ('simpleimputer', SimpleImputer(),\n                                 ['Age', 'Fare']),\n                                ('passthrough', 'passthrough', ['Parch'])])pipeline['Embarked', 'Sex']SimpleImputerSimpleImputer(fill_value='missing', strategy='constant')OneHotEncoderOneHotEncoder()countvectorizerNameCountVectorizerCountVectorizer()simpleimputer['Age', 'Fare']SimpleImputerSimpleImputer()passthrough['Parch']passthroughpassthroughLogisticRegressionLogisticRegression(random_state=1, solver='liblinear')\n\n\nFinally, we‚Äôll redefine X_new to use the full dataset, and then use the fitted Pipeline to make predictions for X_new. We know that we‚Äôve solved our second problem because the Pipeline did not throw any errors during the predict step.\n\nX_new = df_new[cols]\npipe.predict(X_new)\n\narray([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,\n       1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n       1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,\n       1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,\n       1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n       0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n       1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n       1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,\n       0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n       1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n       1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n       0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,\n       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n       0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n       1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,\n       0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n       1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n       0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1])",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Fixing common workflow problems</span>"
    ]
  },
  {
    "objectID": "ch08.html#sec-8-4",
    "href": "ch08.html#sec-8-4",
    "title": "8¬† Fixing common workflow problems",
    "section": "8.4 Q&A: How do I see the feature names output by the ColumnTransformer?",
    "text": "8.4 Q&A: How do I see the feature names output by the ColumnTransformer?\nWhen we pass X to the ColumnTransformer‚Äôs fit_transform method, it outputs a matrix with 1518 columns. How can we find out the names of these columns?\n\nct.fit_transform(X)\n\n&lt;891x1518 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n    with 7328 stored elements in Compressed Sparse Row format&gt;\n\n\nEarlier in the book, we used the get_feature_names method for this purpose, which, as I mentioned previously, has been replaced by get_feature_names_out starting in scikit-learn 1.0. However, get_feature_names will only work if all of the underlying transformers have a get_feature_names method. In this case, it errors because neither Pipeline transformers nor SimpleImputer transformers have a get_feature_names method.\n\nct.get_feature_names()\n\n\n\nAttributeError: Transformer pipeline does not provide get_feature_names\n\n\nThe good news is that starting in scikit-learn 1.1, the get_feature_names_out method is available for all transformers, which means that retrieving the feature names will no longer error.\n\n\n\n\n\n\nChanges to get_feature_names:\n\nStarting in version 1.0: get_feature_names replaced with get_feature_names_out\nStarting in version 1.1: get_feature_names_out available for all transformers\n\n\n\n\nIn the meantime, our only solution for figuring out the column names is to inspect the transformers one-by-one.\nWhen we print out the transformers_ attribute, we can see that there are 4 transformers.\n\nct.transformers_\n\n[('pipeline',\n  Pipeline(steps=[('simpleimputer',\n                   SimpleImputer(fill_value='missing', strategy='constant')),\n                  ('onehotencoder', OneHotEncoder())]),\n  ['Embarked', 'Sex']),\n ('countvectorizer', CountVectorizer(), 'Name'),\n ('simpleimputer', SimpleImputer(), ['Age', 'Fare']),\n ('passthrough', 'passthrough', ['Parch'])]\n\n\nThe first transformer is a Pipeline of SimpleImputer and OneHotEncoder. OneHotEncoder has a get_feature_names method, which we can access by selecting the pipeline transformer and then its onehotencoder step. get_feature_names outputs 6 features, which we know are the first 6 features in the matrix because this is the first transformer in the ColumnTransformer.\n\n(ct.named_transformers_['pipeline']\n   .named_steps['onehotencoder']\n   .get_feature_names())\n\narray(['x0_C', 'x0_Q', 'x0_S', 'x0_missing', 'x1_female', 'x1_male'],\n      dtype=object)\n\n\nThe second transformer is a CountVectorizer. It also has a get_feature_names method, which we can access by selecting the countvectorizer transformer. We could print out all of the feature names, but instead we‚Äôll pass it to the len function, which indicates that the next 1509 features in the matrix came from CountVectorizer.\n\nlen(ct.named_transformers_['countvectorizer'].get_feature_names())\n\n1509\n\n\nThe third transformer is a SimpleImputer, which doesn‚Äôt change the number of columns since we‚Äôre not adding a missing indicator, so we know that the next two features in the matrix are Age and Fare.\nThe fourth transformer is a passthrough transformer, which also doesn‚Äôt change the number of columns, so we know that the final feature in the matrix is Parch.\nWe‚Äôve now accounted for all 1518 features: 6 from the Pipeline transformer, 1509 from the CountVectorizer, 2 from the SimpleImputer, and 1 from the passthrough transformer.\n\n\n\n\n\n\nFeatures output by each transformer:\n\nPipeline: 6 features (Embarked and Sex)\nCountVectorizer: 1509 features (Name)\nSimpleImputer: 2 features (Age and Fare)\npassthrough: 1 feature (Parch)",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Fixing common workflow problems</span>"
    ]
  },
  {
    "objectID": "ch08.html#qa-why-did-we-create-a-pipeline-inside-of-the-columntransformer",
    "href": "ch08.html#qa-why-did-we-create-a-pipeline-inside-of-the-columntransformer",
    "title": "8¬† Fixing common workflow problems",
    "section": "8.5 Q&A: Why did we create a Pipeline inside of the ColumnTransformer?",
    "text": "8.5 Q&A: Why did we create a Pipeline inside of the ColumnTransformer?\nEarlier in this chapter, since the Embarked column contained missing values and needed one-hot encoding, we created a two-step Pipeline called imp_ohe. The first step of this Pipeline is imputation of a constant value, and the second step is one-hot encoding.\n\nimp_ohe = make_pipeline(imp_constant, ohe)\n\nWe included the imp_ohe Pipeline in the ColumnTransformer, and applied it to both the Embarked and Sex columns. Here‚Äôs what it would look like if the ColumnTransformer only contained the imp_ohe Pipeline.\n\nct = make_column_transformer(\n    (imp_ohe, ['Embarked', 'Sex']))\n\nWhen you run the fit_transform method, Embarked turns into 4 columns and Sex turns into 2 columns, and the results are stacked side-by-side.\n\nct.fit_transform(X)\n\narray([[0., 0., 1., 0., 0., 1.],\n       [1., 0., 0., 0., 1., 0.],\n       [0., 0., 1., 0., 1., 0.],\n       ...,\n       [0., 0., 1., 0., 1., 0.],\n       [1., 0., 0., 0., 0., 1.],\n       [0., 1., 0., 0., 0., 1.]])\n\n\nBecause the Sex column didn‚Äôt contain any missing values and only needed one-hot encoding, we could have achieved the exact same results by applying imp_ohe just to Embarked and then separately applying ohe to Sex.\n\nct = make_column_transformer(\n    (imp_ohe, ['Embarked']),\n    (ohe, ['Sex']))\n\nThe fit_transform does indeed output the same results as above, though I personally prefer the first ColumnTransformer.\n\nct.fit_transform(X)\n\narray([[0., 0., 1., 0., 0., 1.],\n       [1., 0., 0., 0., 1., 0.],\n       [0., 0., 1., 0., 1., 0.],\n       ...,\n       [0., 0., 1., 0., 1., 0.],\n       [1., 0., 0., 0., 0., 1.],\n       [0., 1., 0., 0., 0., 1.]])\n\n\nOne common question is whether you can avoid using the imp_ohe Pipeline entirely by making a ColumnTransformer like this instead, in which the imputation of a constant value is applied to Embarked, and one-hot encoding is applied to both Embarked and Sex.\n\nct = make_column_transformer(\n    (imp_constant, ['Embarked']),\n    (ohe, ['Embarked', 'Sex']))\n\nThe answer is no, you cannot. The fit_transform method throws an error because Embarked contains missing values, and the ohe transformer is not able to handle missing values.\n\nct.fit_transform(X)\n\n\n\nValueError: Input contains NaN\n\n\nThe key insight here is that there‚Äôs no interaction between the transformers of a ColumnTransformer. In other words, there‚Äôs no flow of data from one transformer to the next, meaning the output of the imp_constant transformer does not become the input to the ohe transformer. Thus, the ohe transformer is operating on the original Embarked column, not a transformed Embarked column in which missing values have been imputed.\nIf that‚Äôs confusing, it might be useful to recall the key differences between a Pipeline and a ColumnTransformer:\n\nIn a Pipeline, the output of one step becomes the input to the next step. This is precisely why we created the imp_ohe Pipeline: We needed the output of the imputer to become the input to the ohe-hot encoder.\nIn contrast, a ColumnTransformer does not have steps. Instead, it has transformers that operate in parallel, and the output of each transformer is stacked beside the other transformer outputs.\n\n\n\n\n\n\n\nPipeline vs ColumnTransformer:\n\nPipeline:\n\nOutput of one step becomes the input to the next step\nimp_ohe: Output of imp_constant becomes the input to ohe\n\nColumnTransformer:\n\nTransformers operate in parallel\nct: Output of each transformer is stacked beside the other transformer outputs",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Fixing common workflow problems</span>"
    ]
  },
  {
    "objectID": "ch08.html#qa-which-imputation-strategy-should-i-use-with-categorical-features",
    "href": "ch08.html#qa-which-imputation-strategy-should-i-use-with-categorical-features",
    "title": "8¬† Fixing common workflow problems",
    "section": "8.6 Q&A: Which imputation strategy should I use with categorical features?",
    "text": "8.6 Q&A: Which imputation strategy should I use with categorical features?\nWhen imputing missing values for a categorical feature, you can either impute the most frequent value or a constant user-defined value. In this lesson, I‚Äôm going to discuss how you might choose between these two strategies.\nImputing a constant value essentially treats the missing values as a new category, which I believe is the better choice regardless of whether the values are missing at random or not at random. Imputing a constant value is especially important if the majority of values are missing, since imputing the most frequent value in that case would more than double the size of the category that was imputed, which would be quite misleading to the model.\nThat being said, imputing the most frequent value is much more acceptable when you have only a small number of missing values for a given feature, since the imputation won‚Äôt have much of an impact on the model anyway.\n\n\n\n\n\n\nImputation strategies for categorical features:\n\nConstant user-defined value:\n\nTreats missing values as a new category (recommended)\nImportant if the majority of values are missing\n\nMost frequent value:\n\nAcceptable if only a small number of values are missing\n\n\n\n\n\nIt‚Äôs important to note that if you impute a constant value for a feature, and that feature has missing values in the new data but not the training data, then you‚Äôll need to set the OneHotEncoder‚Äôs handle_unknown parameter to 'ignore'. That‚Äôs because the missing values category won‚Äôt be learned during the OneHotEncoder‚Äôs fit step, and thus unknown values seen during the transform step need to be ignored in order to avoid an error.\nThe alternative here is just to impute the most frequent value for that feature, in which case you can leave the handle_unknown parameter set to its default value of 'error'.\n\n\n\n\n\n\nPossible problem with imputing a constant value:\n\nCondition: The feature only has missing values in the new data\nSolution: Set handle_unknown to 'ignore' for the OneHotEncoder\nAlternative: Impute the most frequent value, and leave handle_unknown set to 'error'",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Fixing common workflow problems</span>"
    ]
  },
  {
    "objectID": "ch08.html#qa-should-i-impute-missing-values-before-all-other-transformations",
    "href": "ch08.html#qa-should-i-impute-missing-values-before-all-other-transformations",
    "title": "8¬† Fixing common workflow problems",
    "section": "8.7 Q&A: Should I impute missing values before all other transformations?",
    "text": "8.7 Q&A: Should I impute missing values before all other transformations?\nHere‚Äôs the Pipeline that we‚Äôve built throughout the book. The strategy I‚Äôve used throughout is to include all data transformations within a single ColumnTransformer, including any missing value imputation, and then use that ColumnTransformer as the first step in a two-step Pipeline.\n\npipe\n\nPipelinePipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('pipeline',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer(fill_value='missing',\n                                                                                 strategy='constant')),\n                                                                  ('onehotencoder',\n                                                                   OneHotEncoder())]),\n                                                  ['Embarked', 'Sex']),\n                                                 ('countvectorizer',\n                                                  CountVectorizer(), 'Name'),\n                                                 ('simpleimputer',\n                                                  SimpleImputer(),\n                                                  ['Age', 'Fare']),\n                                                 ('passthrough', 'passthrough',\n                                                  ['Parch'])])),\n                ('logisticregression',\n                 LogisticRegression(random_state=1, solver='liblinear'))])columntransformer: ColumnTransformerColumnTransformer(transformers=[('pipeline',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer(fill_value='missing',\n                                                                strategy='constant')),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder())]),\n                                 ['Embarked', 'Sex']),\n                                ('countvectorizer', CountVectorizer(), 'Name'),\n                                ('simpleimputer', SimpleImputer(),\n                                 ['Age', 'Fare']),\n                                ('passthrough', 'passthrough', ['Parch'])])pipeline['Embarked', 'Sex']SimpleImputerSimpleImputer(fill_value='missing', strategy='constant')OneHotEncoderOneHotEncoder()countvectorizerNameCountVectorizerCountVectorizer()simpleimputer['Age', 'Fare']SimpleImputerSimpleImputer()passthrough['Parch']passthroughpassthroughLogisticRegressionLogisticRegression(random_state=1, solver='liblinear')\n\n\nHowever, an alternative approach would to create a three-step Pipeline in which the first step is missing value imputation, the second step includes all other data transformations, and the third step is the model. Let‚Äôs try it out to see if this is a better approach.\n\n\n\n\n\n\nImpute missing values as a first step?\n\nCurrent Pipeline:\n\nStep 1: All data transformations\nStep 2: Model\n\nAlternative Pipeline:\n\nStep 1: Missing value imputation\nStep 2: All other data transformations\nStep 3: Model\n\n\n\n\n\nThis would be the first ColumnTransformer, which only does missing value imputation. Constant value imputation is applied to Embarked, mean imputation is applied to Age and Fare, and the other columns are passed through because they don‚Äôt contain any missing values in the training or new data. It would be the first step in the Pipeline.\n\nct1 = make_column_transformer(\n    (imp_constant, ['Embarked']),\n    (imp, ['Age', 'Fare']),\n    ('passthrough', ['Sex', 'Name', 'Parch']))\n\nThis would be the second ColumnTransformer, which handles all other data transformations. It would be the second step in the Pipeline, and thus it would operate on the output of the first ColumnTransformer. However, a ColumnTransformer outputs a NumPy array, not a DataFrame, and thus in this ColumnTransformer we would have to reference the columns by position instead of by name.\nWe know the order of the columns from the first ColumnTransformer, and thus Embarked and Sex would be columns 0 and 3 and Name would be column 4. Embarked and Sex are one-hot encoded, Name is vectorized, and the other columns are passed through.\n\nct2 = make_column_transformer(\n    (ohe, [0, 3]),\n    (vect, 4),\n    ('passthrough', [1, 2, 5]))\n\nNow that we‚Äôve created the ColumnTransformers, we can include them in a three-step Pipeline and fit the Pipeline to X and y.\n\npipe = make_pipeline(ct1, ct2, logreg)\npipe.fit(X, y)\n\nPipelinePipeline(steps=[('columntransformer-1',\n                 ColumnTransformer(transformers=[('simpleimputer-1',\n                                                  SimpleImputer(fill_value='missing',\n                                                                strategy='constant'),\n                                                  ['Embarked']),\n                                                 ('simpleimputer-2',\n                                                  SimpleImputer(),\n                                                  ['Age', 'Fare']),\n                                                 ('passthrough', 'passthrough',\n                                                  ['Sex', 'Name', 'Parch'])])),\n                ('columntransformer-2',\n                 ColumnTransformer(transformers=[('onehotencoder',\n                                                  OneHotEncoder(), [0, 3]),\n                                                 ('countvectorizer',\n                                                  CountVectorizer(), 4),\n                                                 ('passthrough', 'passthrough',\n                                                  [1, 2, 5])])),\n                ('logisticregression',\n                 LogisticRegression(random_state=1, solver='liblinear'))])columntransformer-1: ColumnTransformerColumnTransformer(transformers=[('simpleimputer-1',\n                                 SimpleImputer(fill_value='missing',\n                                               strategy='constant'),\n                                 ['Embarked']),\n                                ('simpleimputer-2', SimpleImputer(),\n                                 ['Age', 'Fare']),\n                                ('passthrough', 'passthrough',\n                                 ['Sex', 'Name', 'Parch'])])simpleimputer-1['Embarked']SimpleImputerSimpleImputer(fill_value='missing', strategy='constant')simpleimputer-2['Age', 'Fare']SimpleImputerSimpleImputer()passthrough['Sex', 'Name', 'Parch']passthroughpassthroughcolumntransformer-2: ColumnTransformerColumnTransformer(transformers=[('onehotencoder', OneHotEncoder(), [0, 3]),\n                                ('countvectorizer', CountVectorizer(), 4),\n                                ('passthrough', 'passthrough', [1, 2, 5])])onehotencoder[0, 3]OneHotEncoderOneHotEncoder()countvectorizer4CountVectorizerCountVectorizer()passthrough[1, 2, 5]passthroughpassthroughLogisticRegressionLogisticRegression(random_state=1, solver='liblinear')\n\n\nFinally, we can use this three-step Pipeline to make predictions, and it does indeed make the same predictions as our original two-step Pipeline.\n\npipe.predict(X_new)\n\narray([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,\n       1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n       1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,\n       1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,\n       1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n       0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n       1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n       1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,\n       0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n       1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n       1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n       0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,\n       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n       0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n       1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,\n       0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n       1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n       0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1])\n\n\nUsing a three-step Pipeline like this is certainly a valid approach. However, I find the original two-step Pipeline easier to write and to read, and thus I prefer the two-step approach.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Fixing common workflow problems</span>"
    ]
  },
  {
    "objectID": "ch08.html#qa-what-methods-can-i-use-with-a-pipeline",
    "href": "ch08.html#qa-what-methods-can-i-use-with-a-pipeline",
    "title": "8¬† Fixing common workflow problems",
    "section": "8.8 Q&A: What methods can I use with a Pipeline?",
    "text": "8.8 Q&A: What methods can I use with a Pipeline?\nThe rules for Pipelines are that all steps other than the final step must be a transformer, and the final step can be a model or a transformer.\n\n\n\n\n\n\nRules for Pipeline steps:\n\nAll steps other than the final step must be a transformer\nFinal step can be a model or a transformer\n\n\n\n\nIf a Pipeline ends in a model, such as our pipe object, you can use the Pipeline‚Äôs fit and predict methods:\n\nIf you run the fit method, all steps before the final one run fit_transform, and the final step runs fit.\nIf you run the predict method, all steps before the final one run transform, and the final step runs predict.\n\n\n\n\n\n\n\nPipeline ends in a model:\n\nfit:\n\nAll steps before the final step run fit_transform\nFinal step runs fit\n\npredict:\n\nAll steps before the final step run transform\nFinal step runs predict\n\n\n\n\n\nIf a Pipeline ends in a transformer, such as our imp_ohe object, you generally use the Pipeline‚Äôs fit_transform and transform methods, but you can also use the fit method:\n\nIf you run the fit_transform method, all steps run fit_transform.\nIf you run the transform method, all steps run transform.\nIf you run the fit method, all steps before the final one run fit_transform, and the final step runs fit.\n\n\n\n\n\n\n\nPipeline ends in a transformer:\n\nfit_transform:\n\nAll steps run fit_transform\n\ntransform:\n\nAll steps run transform\n\nfit:\n\nAll steps before the final step run fit_transform\nFinal step runs fit\n\n\n\n\n\nAlthough this is a lot of information to take in, developing this level of understanding will definitely make it easier for you to test and debug your future Pipelines.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Fixing common workflow problems</span>"
    ]
  },
  {
    "objectID": "ch09.html",
    "href": "ch09.html",
    "title": "9¬† Workflow review #2",
    "section": "",
    "text": "9.1 Recap of our workflow\nIn this chapter, we‚Äôre going to review the workflow that we‚Äôve built so far and also discuss the concept of data leakage.\nTo start, we‚Äôre going to walk through all of the code that‚Äôs necessary to recreate our workflow up to this point. We begin by importing pandas, the three transformer classes we‚Äôre using, one modeling class, and two composition functions.\nimport pandas as pd\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.pipeline import make_pipeline\nNext, we create a list of the six columns we‚Äôre going to select from our data.\ncols = ['Parch', 'Fare', 'Embarked', 'Sex', 'Name', 'Age']\nThen, we read in all of the training data and use it to define our X and y.\ndf = pd.read_csv('http://bit.ly/MLtrain')\nX = df[cols]\ny = df['Survived']\nAnd we read in all of the new data and use it to define X_new.\ndf_new = pd.read_csv('http://bit.ly/MLnewdata')\nX_new = df_new[cols]\nWe create four instances of our transformers, namely two different instances of SimpleImputer and one instance each of OneHotEncoder and CountVectorizer.\nimp = SimpleImputer()\nimp_constant = SimpleImputer(strategy='constant', fill_value='missing')\nohe = OneHotEncoder()\nvect = CountVectorizer()\nWe create a two-step transformer Pipeline of constant value imputation and one-hot encoding.\nimp_ohe = make_pipeline(imp_constant, ohe)\nAnd then we build the ColumnTransformer, which imputes and one-hot encodes Embarked and Sex, vectorizes Name, imputes Age and Fare, and passes through Parch.\nct = make_column_transformer(\n    (imp_ohe, ['Embarked', 'Sex']),\n    (vect, 'Name'),\n    (imp, ['Age', 'Fare']),\n    ('passthrough', ['Parch']))\nWe also create an instance of LogisticRegression.\nlogreg = LogisticRegression(solver='liblinear', random_state=1)\nFinally, we create a two-step modeling Pipeline, fit the Pipeline to X and y, and use the fitted Pipeline to make predictions on X_new.\npipe = make_pipeline(ct, logreg)\npipe.fit(X, y)\npipe.predict(X_new)\n\narray([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,\n       1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n       1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,\n       1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,\n       1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n       0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n       1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n       1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,\n       0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n       1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n       1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n       0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,\n       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n       0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n       1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,\n       0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n       1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n       0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1])\nThat‚Äôs really all the code we need to recreate our workflow from the book so far. As a reminder, there are no calls to fit_transform or transform because all of that functionality is encapsulated by the Pipeline.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Workflow review #2</span>"
    ]
  },
  {
    "objectID": "ch09.html#comparing-columntransformer-and-pipeline",
    "href": "ch09.html#comparing-columntransformer-and-pipeline",
    "title": "9¬† Workflow review #2",
    "section": "9.2 Comparing ColumnTransformer and Pipeline",
    "text": "9.2 Comparing ColumnTransformer and Pipeline\nHere‚Äôs a diagram that illustrates our workflow so far, which we will briefly review.\nLet‚Äôs start with the ColumnTransformer, which received 6 columns of input from the X DataFrame:\n\nIt selected the Embarked and Sex columns and passed them to the transformer Pipeline of SimpleImputer and OneHotEncoder. The SimpleImputer step of that Pipeline imputed a constant value and output 2 columns, and the OneHotEncoder step transformed those 2 columns into 6 columns.\nIt selected the Name column and passed it to the CountVectorizer, which output 1509 columns.\nIt selected the Age and Fare columns and passed them to the SimpleImputer, which imputed the mean and output 2 columns.\nIt selected the Parch column and passed it through unmodified.\nFinally, it stacked the 6 plus 1509 plus 2 plus 1 columns side-by-side, resulting in a total of 1518 columns.\n\nNow let‚Äôs talk about the modeling Pipeline, which has 2 steps:\n\nStep 1 is a ColumnTransformer that received 6 columns of input and transformed them into 1518 columns.\nStep 2 is a LogisticRegression model that received 1518 columns of input and used those columns either for fitting or predicting.\n\n\nWith those examples in mind, we can step back and briefly review the differences between a ColumnTransformer and a Pipeline:\n\nA ColumnTransformer pulls out subsets of columns and transforms them independently, and then stacks the results side-by-side.\nIt only ever does data transformations.\nIt does not have steps, because each subset of columns is transformed independently.\n\nIn contrast:\n\nA Pipeline is a series of steps that occur in order, and the output of each step becomes the input to the next step.\nThe last step of a Pipeline can be a model or a transformer, whereas all other steps must be transformers.\n\n\n\n\n\n\n\nColumnTransformer vs Pipeline:\n\nColumnTransformer:\n\nSelects subsets of columns, transforms them independently, stacks the results side-by-side\nOnly includes transformers\nDoes not have steps (transformers operate in parallel)\n\nPipeline:\n\nSeries of steps that occur in order\nOutput of each step becomes the input to the next step\nLast step is a model or transformer, all other steps are transformers\n\n\n\n\n\nFinally, we can print out the Pipeline to see a similar diagram. You can see that our workflow includes a transformer Pipeline, meaning a Pipeline that ends in a transformer, inside a ColumnTransformer, which is inside of a modeling Pipeline, meaning a Pipeline that ends in a model.\n\npipe\n\nPipelinePipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('pipeline',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer(fill_value='missing',\n                                                                                 strategy='constant')),\n                                                                  ('onehotencoder',\n                                                                   OneHotEncoder())]),\n                                                  ['Embarked', 'Sex']),\n                                                 ('countvectorizer',\n                                                  CountVectorizer(), 'Name'),\n                                                 ('simpleimputer',\n                                                  SimpleImputer(),\n                                                  ['Age', 'Fare']),\n                                                 ('passthrough', 'passthrough',\n                                                  ['Parch'])])),\n                ('logisticregression',\n                 LogisticRegression(random_state=1, solver='liblinear'))])columntransformer: ColumnTransformerColumnTransformer(transformers=[('pipeline',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer(fill_value='missing',\n                                                                strategy='constant')),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder())]),\n                                 ['Embarked', 'Sex']),\n                                ('countvectorizer', CountVectorizer(), 'Name'),\n                                ('simpleimputer', SimpleImputer(),\n                                 ['Age', 'Fare']),\n                                ('passthrough', 'passthrough', ['Parch'])])pipeline['Embarked', 'Sex']SimpleImputerSimpleImputer(fill_value='missing', strategy='constant')OneHotEncoderOneHotEncoder()countvectorizerNameCountVectorizerCountVectorizer()simpleimputer['Age', 'Fare']SimpleImputerSimpleImputer()passthrough['Parch']passthroughpassthroughLogisticRegressionLogisticRegression(random_state=1, solver='liblinear')",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Workflow review #2</span>"
    ]
  },
  {
    "objectID": "ch09.html#why-not-use-pandas-for-transformations",
    "href": "ch09.html#why-not-use-pandas-for-transformations",
    "title": "9¬† Workflow review #2",
    "section": "9.3 Why not use pandas for transformations?",
    "text": "9.3 Why not use pandas for transformations?\nOne question you might be wondering about is why we did all of our data transformations in scikit-learn, when instead we could have done some of them in pandas and then passed the transformed data to scikit-learn. There are four main reasons.\nFirst, CountVectorizer is not available within pandas, and it‚Äôs one of the most useful techniques for encoding text data. You could try doing most of your transformations in pandas and then just use CountVectorizer in scikit-learn, but that adds additional complexity to your workflow, especially if you have to combine the dense matrix output by pandas with the sparse matrix output by CountVectorizer. Thus, your workflow is more efficient when you do all of your transformations within scikit-learn.\nSecond, one-hot encoding is available within pandas using the get_dummies function, but you would probably end up adding the one-hot encoded columns to your DataFrame, which makes the DataFrame wider and more difficult to navigate. By using scikit-learn for one-hot encoding instead, you can leave the source DataFrame in its original format.\nThird, missing value imputation is available within pandas using the fillna function, but that will actually result in data leakage, which I‚Äôll explain in detail in the next lesson. By using scikit-learn for imputation instead, you avoid the problem of data leakage.\nFinally, doing all of your transformations within scikit-learn allows you to cross-validate and tune your entire Pipeline rather than just your model. This will often lead to a better-performing model, which I‚Äôll explain and demonstrate in the next chapter.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n¬†\nscikit-learn\npandas\n\n\n\n\nEncoding text data\nCountVectorizer\nNot available\n\n\nOne-hot encoding\nOneHotEncoder\nget_dummies (results in larger DataFrame)\n\n\nImputing missing values\nSimpleImputer & others\nfillna (results in data leakage)\n\n\nCross-validation and tuning\nYour entire Pipeline\nJust your model",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Workflow review #2</span>"
    ]
  },
  {
    "objectID": "ch09.html#preventing-data-leakage",
    "href": "ch09.html#preventing-data-leakage",
    "title": "9¬† Workflow review #2",
    "section": "9.4 Preventing data leakage",
    "text": "9.4 Preventing data leakage\nIn the previous lesson, I mentioned that imputing missing values in pandas would lead to something called ‚Äúdata leakage‚Äù. So what is data leakage? Data leakage is when you inadvertently include knowledge from the testing data when training a model.\n\n\n\n\n\n\nWhat is data leakage?\n\nInadvertently including knowledge from the testing data when training a model\n\n\n\n\nFirst, I‚Äôll explain why this is problematic, and then I‚Äôll explain why imputing missing values in pandas would cause data leakage.\nIn short, data leakage is problematic because it will cause your model evaluation scores to be less reliable. This may lead you to make bad decisions when tuning hyperparameters, and it will lead you to overestimate how well your model will perform on new data. It‚Äôs hard to know whether data leakage will skew your evaluation scores by a negligible amount or a huge amount, so it‚Äôs best to just avoid data leakage entirely.\n\n\n\n\n\n\nWhy is data leakage problematic?\n\nYour model evaluation scores will be less reliable\nYou might make bad decisions when tuning hyperparameters\nYou will overestimate how well your model will perform on new data\n\n\n\n\nSo why would imputing missing values in pandas cause data leakage? Your model evaluation procedure, such as cross-validation, is supposed to simulate the future, so that you can accurately estimate right now how well your model will perform on new data. But if you impute missing values on your whole dataset in pandas and then pass your dataset to scikit-learn, your model evaluation procedure will no longer be an accurate simulation of reality. That‚Äôs because the imputation values will be based on your entire dataset, meaning both the training portion and the testing portion, whereas the imputation values should just be based on the training portion.\nIn other words, imputation based on the entire dataset is like peeking into the future and then using what you learned from the future during model training, which is definitely not allowed.\nYou might think that one way around this problem would be to split your dataset into training and testing sets and then impute missing values using pandas. That would work if you‚Äôre only planning to use train/test split for model evaluation, but it would not work if you‚Äôre ever planning to use cross-validation. That‚Äôs because during 5-fold cross-validation, the rows contained in the training set will change 5 times, and thus it‚Äôs quite impractical to avoid data leakage if you use pandas for imputation while using cross-validation.\n\n\n\n\n\n\nImputation on the full dataset can cause data leakage:\n\nYour model evaluation procedure is supposed to simulate the future\nImputation based on your full dataset ‚Äúleaks‚Äù information about the future into model training\n\n\n\n\nSo far, we‚Äôve only been talking about data leakage in the context of missing value imputation. But there are other transformations that if done in pandas on the full dataset will also cause data leakage. For example, feature scaling in pandas would lead to data leakage, and even one-hot encoding in pandas would lead to data leakage unless there‚Äôs a known, fixed set of categories. More generally, any transformation which incorporates information about other rows when transforming a row will lead to data leakage if done in pandas.\n\n\n\n\n\n\nOther transformations on the full dataset can also cause data leakage:\n\nFeature scaling\nOne-hot encoding\nAny transformation which incorporates information about other rows\n\n\n\n\nNow that you‚Äôve learned how data transformations in pandas can cause data leakage, I‚Äôll briefly mention three ways in which scikit-learn prevents data leakage:\n\nFirst, scikit-learn transformers have separate fit and transform steps, which allow you to base your data transformations on the training set only, and then apply those transformations to both the training set and the testing set.\nSecond, the fit and predict methods of a Pipeline encapsulate all calls to fit_transform and transform so that they‚Äôre called at the appropriate times.\nThird, cross_val_score splits the data prior to performing data transformations, which I‚Äôll explain in detail in the next chapter.\n\n\n\n\n\n\n\nHow does scikit-learn prevent data leakage?\n\nTransformers have separate fit and transform steps\nPipeline methods call fit_transform and transform at the appropriate times\ncross_val_score splits the data prior to performing transformations",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Workflow review #2</span>"
    ]
  },
  {
    "objectID": "ch10.html",
    "href": "ch10.html",
    "title": "10¬† Evaluating and tuning a Pipeline",
    "section": "",
    "text": "10.1 Evaluating a Pipeline with cross-validation\nIn this chapter, we‚Äôre going to take a deep dive into how to efficiently tune our Pipeline for maximum accuracy.\nLet‚Äôs return to the topic of model evaluation.\nAs you might recall, we used cross-validation way back in chapter 2 to evaluate our most basic model. Since that chapter, we‚Äôve been adding many more features without re-running cross-validation. That‚Äôs because any model evaluation procedure is highly unreliable with only 10 rows of data, so it would have been misleading to run cross-validation and compare the results. But now that we‚Äôre using the full dataset, cross-validation can once again be used.\nWhen we run it, cross_val_score outputs a mean accuracy of 0.811, which we‚Äôll use as the baseline accuracy against which our future Pipelines can be compared.\nfrom sklearn.model_selection import cross_val_score\ncross_val_score(pipe, X, y, cv=5, scoring='accuracy').mean()\n\n0.8114619295712762\nLet‚Äôs talk about what actually happens ‚Äúunder the hood‚Äù when we run the cross_val_score function on a Pipeline:\nOne thing you might have noticed is that cross_val_score splits the data in step 1 before performing the transformations in steps 2 and 3. As a result, the imputation values for Age and Fare and the vocabulary for CountVectorizer are all computed 5 different times. Each time, these values are computed using the training set only, and then applied to both the training and testing sets.\nAlternatively, you could imagine performing all of the transformations first, and then splitting the data. This would be much faster, since the imputation values and the vocabulary would be computed only once on the full dataset.\nSo why does cross_val_score split the data first? Because splitting the data before performing the transformations prevents data leakage, whereas performing the transformations on the full dataset before splitting the data would cause data leakage, since information about the testing set would be ‚Äúleaked‚Äù into the model training process.\nAs we discussed in the previous chapter, this is one way that scikit-learn helps to shield you from data leakage.",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Evaluating and tuning a Pipeline</span>"
    ]
  },
  {
    "objectID": "ch10.html#evaluating-a-pipeline-with-cross-validation",
    "href": "ch10.html#evaluating-a-pipeline-with-cross-validation",
    "title": "10¬† Evaluating and tuning a Pipeline",
    "section": "",
    "text": "First, we‚Äôll import the cross_val_score function from the model_selection module.\nInstead of passing a model to cross_val_score, we can actually pass our entire Pipeline.\nWe also pass it X and y.\nAnd then we specify the number of cross-validation folds. Using 5 or 10 folds has been shown to be a reasonable default choice, and so we‚Äôll choose 5 in order to minimize the computation. 5 folds has actually been the default for cross_val_score since version 0.22, but I like to include it anyway for clarity.\nFinally, we‚Äôll specify the evaluation metric of classification accuracy.\n\n\n\n\n\n\n\n\n\nPipeline accuracy scores:\n\nBaseline (no tuning): 0.811\n\n\n\n\n\n\nIn step 1, cross_val_score splits the data into 5 folds. 4 out of 5 folds (meaning 80% of the data) is set aside for training, and the remaining fold (meaning 20% of the data) is set aside for testing.\nIn step 2, the Pipeline‚Äôs fit method is run on the training portion. Thus the transformations specified in the ColumnTransformer are performed on the training portion, and the transformed training data is used to fit the model.\nIn step 3, the Pipeline‚Äôs predict method is run on the testing portion. Thus the transformations learned during step 2 are applied to the testing portion, the transformed testing data is passed to the fitted model, and the model makes predictions.\nFinally, in step 4, the accuracy of those predictions is calculated.\nSteps 1 through 4 are then repeated 4 more times, and each time a different fold is set aside as the testing portion.\ncross_val_score thus outputs 5 accuracy scores, and we take the mean of those scores.\n\n\n\n\n\n\n\nSteps of 5-fold cross-validation on a Pipeline:\n\nSplit data into 5 folds (A, B, C, D, E)\n\nABCD is training set\nE is testing set\n\nPipeline is fit on training set\n\nABCD is transformed\nModel is fit on transformed data\n\nPipeline makes predictions on testing set\n\nE is transformed (using step 2 transformations)\nModel makes predictions on transformed data\n\nCalculate accuracy of those predictions\nRepeat the steps above 4 more times, with a different testing set each time\nCalculate the mean of the 5 scores\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy does cross_val_score split the data first?\n\nProper cross-validation:\n\nData is split (step 1) before transformations (steps 2 and 3)\nImputation values and vocabulary are computed using training set only\nPrevents data leakage\n\nImproper cross-validation:\n\nTransformations are performed before data is split\nImputation values and vocabulary are computed using full dataset\nCauses data leakage",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Evaluating and tuning a Pipeline</span>"
    ]
  },
  {
    "objectID": "ch10.html#tuning-a-pipeline-with-grid-search",
    "href": "ch10.html#tuning-a-pipeline-with-grid-search",
    "title": "10¬† Evaluating and tuning a Pipeline",
    "section": "10.2 Tuning a Pipeline with grid search",
    "text": "10.2 Tuning a Pipeline with grid search\nNow that we‚Äôve calculated the baseline accuracy for our Pipeline, the next step is to tune the hyperparameters for both the model and the transformers. Recall that we‚Äôve been using the default parameters for most objects in the Pipeline, and so tuning those parameters is likely to result in a more accurate model.\nBefore proceeding, let me briefly explain some terminology. In the field of statistics, ‚Äúhyperparameters‚Äù are values that you set, whereas ‚Äúparameters‚Äù are values learned from the data by the estimator during the fitting process.\nFor example, the C value of logistic regression is called a hyperparameter because it‚Äôs something you can set and optimize, whereas the coefficients of a logistic regression model are called parameters because they‚Äôre learned from the data.\n\n\n\n\n\n\nStatistics terminology:\n\nHyperparameters: Values that you set\n\nExample: C value of logistic regression\n\nParameters: Values learned from the data\n\nExample: Coefficients of logistic regression model\n\n\n\n\n\nIn this book, I‚Äôm generally going to follow scikit-learn‚Äôs conventions as I understand them:\n\nI‚Äôll use the phrase ‚Äúhyperparameter tuning‚Äù to refer to the process of tuning a model or tuning a Pipeline containing a model and transformers.\nAnd I‚Äôll use the phrase ‚Äúparameter‚Äù to refer to anything passed to a class. For example, this includes the C and random_state values passed to the LogisticRegression class, and the strategy value passed to the SimpleImputer class.\n\n\n\n\n\n\n\nscikit-learn terminology:\n\nHyperparameter tuning: Tuning a model or a Pipeline\nParameter: Anything passed to a class\n\nLogisticRegression: C, random_state\nSimpleImputer: strategy\n\n\n\n\n\nWith that being said, we‚Äôre going to use a scikit-learn class called GridSearchCV to perform the hyperparameter tuning. In a grid search, you define which values you want to try for each parameter, and it cross-validates every possible combination of those values.\n\n\n\n\n\n\nHyperparameter tuning with GridSearchCV:\n\nYou define which values to try for each parameter\nIt cross-validates every combination of those values\n\n\n\n\nWe can actually use GridSearchCV to tune the entire Pipeline at once, including both the model and the transformers. This has two huge benefits over just tuning a model:\n\nFirst, it enables you to tune the model and transformer parameters simultaneously, which is important because the best performing combination might be when none of the parameters are set to their default values.\nSecond, this prevents data leakage because the data splitting and transformations will occur within the cross-validation done by GridSearchCV.\n\n\n\n\n\n\n\nBenefits of tuning a Pipeline:\n\nTunes the model and transformers simultaneously\nPrevents data leakage\n\n\n\n\nKeep in mind that if we had instead done the data transformations in pandas, we would have missed out on both of these benefits.",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Evaluating and tuning a Pipeline</span>"
    ]
  },
  {
    "objectID": "ch10.html#tuning-the-model",
    "href": "ch10.html#tuning-the-model",
    "title": "10¬† Evaluating and tuning a Pipeline",
    "section": "10.3 Tuning the model",
    "text": "10.3 Tuning the model\nIn this lesson, we‚Äôre going to tune the model, and then in the next lesson, we‚Äôll also tune the transformers.\nFor the LogisticRegression model, we‚Äôre going to tune two parameters:\n\nThe first parameter is penalty, which is the type of regularization. For this parameter, the default value is 'l2', and we‚Äôre going to try the values 'l1' and 'l2'. And just to be clear, the first character of each of those values is a lowercase ‚ÄúL‚Äù.\nThe second parameter is C, which is the amount of regularization. For this parameter, the default value is 1, and we‚Äôre going to try the values 0.1, 1, and 10.\n\n\n\n\n\n\n\nLogisticRegression tuning parameters:\n\npenalty: Type of regularization\n\n'l1'\n'l2' (default)\n\nC: Amount of regularization\n\n0.1\n1 (default)\n10\n\n\n\n\n\nDeciding which parameters to tune and what values to try requires both research and experience, and unfortunately, it‚Äôs different for every type of model.\nIn order to tune a Pipeline with GridSearchCV, we need to get the names of the Pipeline steps from the named_steps attribute. We‚Äôll tune the logisticregression step in this lesson, and we‚Äôll tune the columntransformer step in the next lesson.\n\npipe.named_steps.keys()\n\ndict_keys(['columntransformer', 'logisticregression'])\n\n\nTo use GridSearchCV, we need to create a dictionary in which each entry represents a parameter and the values we want to try for that parameter. We‚Äôll start by creating an empty dictionary called params, and then we‚Äôll add the two entries.\nFor each dictionary entry, the key is the Pipeline step name, followed by two underscores, followed by the parameter name. Thus the key for the first entry is 'logisticregression__penalty', and the key for the second entry is 'logisticregression__C'.\nUsing two underscores is what allows GridSearchCV to distinguish between the step name and the parameter name. Using a single underscore would be ambiguous, since a step name or parameter name can have an underscore within it.\nThe value for each dictionary entry is a list of the values you want to try for that parameter. Thus the value for the first entry is a list of 'l1' and 'l2', and the value for the second entry is a list of 0.1, 1, and 10.\n\n\n\n\n\n\nParameter dictionary for GridSearchCV:\n\nKey: step__parameter\n\n'logisticregression__penalty'\n'logisticregression__C'\n\nValue: List of values to try\n\n['l1', 'l2']\n[0.1, 1, 10]\n\n\n\n\n\nAfter adding the two entries, we‚Äôll print out the params dictionary just to make sure that it looks correct.\n\nparams = {}\nparams['logisticregression__penalty'] = ['l1', 'l2']\nparams['logisticregression__C'] = [0.1, 1, 10]\nparams\n\n{'logisticregression__penalty': ['l1', 'l2'],\n 'logisticregression__C': [0.1, 1, 10]}\n\n\nNow that we‚Äôve created the parameter dictionary, we can set up the grid search. We import the GridSearchCV class from the model_selection module.\n\nfrom sklearn.model_selection import GridSearchCV\n\nNext, we create an instance of GridSearchCV called grid. We pass it the Pipeline, the parameter dictionary, the number of folds, and the evaluation metric.\nFinally, we run the grid search by fitting the grid object with X and y. Because our scikit-learn configuration is set to display diagrams, we see a diagram of the Pipeline now that the grid search is complete.\n\ngrid = GridSearchCV(pipe, params, cv=5, scoring='accuracy')\ngrid.fit(X, y)\n\nGridSearchCVGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('columntransformer',\n                                        ColumnTransformer(transformers=[('pipeline',\n                                                                         Pipeline(steps=[('simpleimputer',\n                                                                                          SimpleImputer(fill_value='missing',\n                                                                                                        strategy='constant')),\n                                                                                         ('onehotencoder',\n                                                                                          OneHotEncoder())]),\n                                                                         ['Embarked',\n                                                                          'Sex']),\n                                                                        ('countvectorizer',\n                                                                         CountVectorizer(),\n                                                                         'Name'),\n                                                                        ('simpleimputer',\n                                                                         SimpleImputer(),\n                                                                         ['Age',\n                                                                          'Fare']),\n                                                                        ('passthrough',\n                                                                         'passthrough',\n                                                                         ['Parch'])])),\n                                       ('logisticregression',\n                                        LogisticRegression(random_state=1,\n                                                           solver='liblinear'))]),\n             param_grid={'logisticregression__C': [0.1, 1, 10],\n                         'logisticregression__penalty': ['l1', 'l2']},\n             scoring='accuracy')columntransformer: ColumnTransformerColumnTransformer(transformers=[('pipeline',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer(fill_value='missing',\n                                                                strategy='constant')),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder())]),\n                                 ['Embarked', 'Sex']),\n                                ('countvectorizer', CountVectorizer(), 'Name'),\n                                ('simpleimputer', SimpleImputer(),\n                                 ['Age', 'Fare']),\n                                ('passthrough', 'passthrough', ['Parch'])])pipeline['Embarked', 'Sex']SimpleImputerSimpleImputer(fill_value='missing', strategy='constant')OneHotEncoderOneHotEncoder()countvectorizerNameCountVectorizerCountVectorizer()simpleimputer['Age', 'Fare']SimpleImputerSimpleImputer()passthrough['Parch']passthroughpassthroughLogisticRegressionLogisticRegression(random_state=1, solver='liblinear')\n\n\nThe results of the grid search are stored in an attribute called cv_results_, which we‚Äôll convert to a DataFrame. We‚Äôll use a filter to only keep the columns we need, and rename the parameter columns to make them easier to read.\nThere are 6 rows because it ran cross-validation 6 times, which is every possible combination of the 2 values of penalty and the 3 values of C that we specified.\n\nresults = (pd.DataFrame(grid.cv_results_)\n           .filter(regex='param_|mean_test|rank'))\nresults.columns = results.columns.str.split('__').str[-1]\nresults\n\n\n\n\n\n\n\n\nC\npenalty\nmean_test_score\nrank_test_score\n\n\n\n\n0\n0.1\nl1\n0.783385\n6\n\n\n1\n0.1\nl2\n0.788990\n5\n\n\n2\n1\nl1\n0.814814\n2\n\n\n3\n1\nl2\n0.811462\n3\n\n\n4\n10\nl1\n0.818166\n1\n\n\n5\n10\nl2\n0.809234\n4\n\n\n\n\n\n\n\nNotice the rank_test_score column. We‚Äôll use the DataFrame‚Äôs sort_values method to sort the rows by that column in ascending order.\nBy examining the mean_test_score column, we can see that the best parameter combination resulted in a cross-validated accuracy of 0.818, which is higher than our baseline accuracy of 0.811.\nWe can see that the best accuracy occurred when C was 10 and penalty was 'l1', neither of which was the default value for that parameter.\n\nresults.sort_values('rank_test_score')\n\n\n\n\n\n\n\n\nC\npenalty\nmean_test_score\nrank_test_score\n\n\n\n\n4\n10\nl1\n0.818166\n1\n\n\n2\n1\nl1\n0.814814\n2\n\n\n3\n1\nl2\n0.811462\n3\n\n\n5\n10\nl2\n0.809234\n4\n\n\n1\n0.1\nl2\n0.788990\n5\n\n\n0\n0.1\nl1\n0.783385\n6\n\n\n\n\n\n\n\n\n\n\n\n\n\nPipeline accuracy scores:\n\nGrid search (2 parameters): 0.818\nBaseline (no tuning): 0.811",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Evaluating and tuning a Pipeline</span>"
    ]
  },
  {
    "objectID": "ch10.html#tuning-the-transformers",
    "href": "ch10.html#tuning-the-transformers",
    "title": "10¬† Evaluating and tuning a Pipeline",
    "section": "10.4 Tuning the transformers",
    "text": "10.4 Tuning the transformers\nIn the previous lesson, we built a grid search for tuning model parameters and found that the best accuracy occurred when C was 10 and penalty was 'l1'. In this lesson, we‚Äôre going to expand the search to also include transformer parameters.\nWhen expanding the search, you might first think that we should set C to 10 and penalty to 'l1', and then only search the transformer parameters, since that would be the most computationally efficient approach.\nHowever, the better approach is actually to consider all of the values for C and penalty in combination with all of the transformer parameters. That‚Äôs because we‚Äôre searching for the best combination of all parameters, and since each parameter can influence what is optimal for the other parameters, the best combination might use a C value other than 10 or a penalty value other than 'l1'.\n\n\n\n\n\n\nOptions for expanding the grid search:\n\nInitial idea: Set C=10 and penalty='l1', then only search transformer parameters\nBetter approach: Search for best combination of C, penalty, and transformer parameters\n\n\n\n\nAll of that is to say that we‚Äôre going to expand the existing params dictionary to include transformer parameters. And to include transformer parameters, we first need to figure out the transformer names.\nFrom the previous lesson, you might recall that the first step in the Pipeline is named columntransformer (all lowercase). We‚Äôll access that step using the named_steps attribute, which then allows us to examine the named_transformers_ attribute of the ColumnTransformer.\nAs a side note, named_transformers_ ends with an underscore because it‚Äôs set during the fit step, whereas named_steps does not end with an underscore because it‚Äôs set when the Pipeline instance is created.\nAnyway, we can now see the transformer names. We‚Äôre going to tune a single parameter from three of the transformers. Normally I might tune more parameters, but for the sake of brevity I‚Äôm only going to tune three.\n\npipe.named_steps['columntransformer'].named_transformers_\n\n{'pipeline': Pipeline(steps=[('simpleimputer',\n                  SimpleImputer(fill_value='missing', strategy='constant')),\n                 ('onehotencoder', OneHotEncoder())]),\n 'countvectorizer': CountVectorizer(),\n 'simpleimputer': SimpleImputer(),\n 'passthrough': 'passthrough'}\n\n\nThe first parameter we‚Äôre going to tune is the drop parameter of OneHotEncoder, which was added to scikit-learn in version 0.21 and which I discussed in lesson¬†3.6.\nTo add it to the params dictionary, we specify the Pipeline step name, which is columntransformer. Then we specify the transformer name, which is pipeline. Then we specify the step name of the inner Pipeline, which is onehotencoder. Finally we specify the parameter name, which is drop. All of these components are separated by two underscores.\nThe parameter values we‚Äôre going to try are None and 'first'. None is the default, and it means don‚Äôt drop any columns, whereas 'first' means drop the first column of each feature after encoding.\n\n\n\n\n\n\nOneHotEncoder tuning parameter:\n\ndrop: Method for dropping a column of each feature\n\nNone (default)\n'first'\n\n\n\n\n\n\nparams['columntransformer__pipeline__onehotencoder__drop'] = [None,\n                                                              'first']\n\nIf you‚Äôre ever unsure how to specify a parameter for a grid search, you can see all of the Pipeline‚Äôs parameters by using the get_params method followed by the keys method. I‚Äôm converting the output to a list for easier readability. This list is also useful if you prefer to copy and paste the parameter names rather than typing them.\nAs you can see, there are many transformer and model parameters that we‚Äôre not tuning, many of which could be useful to tune given enough time and computational resources.\n\nlist(pipe.get_params().keys())\n\n['memory',\n 'steps',\n 'verbose',\n 'columntransformer',\n 'logisticregression',\n 'columntransformer__n_jobs',\n 'columntransformer__remainder',\n 'columntransformer__sparse_threshold',\n 'columntransformer__transformer_weights',\n 'columntransformer__transformers',\n 'columntransformer__verbose',\n 'columntransformer__pipeline',\n 'columntransformer__countvectorizer',\n 'columntransformer__simpleimputer',\n 'columntransformer__passthrough',\n 'columntransformer__pipeline__memory',\n 'columntransformer__pipeline__steps',\n 'columntransformer__pipeline__verbose',\n 'columntransformer__pipeline__simpleimputer',\n 'columntransformer__pipeline__onehotencoder',\n 'columntransformer__pipeline__simpleimputer__add_indicator',\n 'columntransformer__pipeline__simpleimputer__copy',\n 'columntransformer__pipeline__simpleimputer__fill_value',\n 'columntransformer__pipeline__simpleimputer__missing_values',\n 'columntransformer__pipeline__simpleimputer__strategy',\n 'columntransformer__pipeline__simpleimputer__verbose',\n 'columntransformer__pipeline__onehotencoder__categories',\n 'columntransformer__pipeline__onehotencoder__drop',\n 'columntransformer__pipeline__onehotencoder__dtype',\n 'columntransformer__pipeline__onehotencoder__handle_unknown',\n 'columntransformer__pipeline__onehotencoder__sparse',\n 'columntransformer__countvectorizer__analyzer',\n 'columntransformer__countvectorizer__binary',\n 'columntransformer__countvectorizer__decode_error',\n 'columntransformer__countvectorizer__dtype',\n 'columntransformer__countvectorizer__encoding',\n 'columntransformer__countvectorizer__input',\n 'columntransformer__countvectorizer__lowercase',\n 'columntransformer__countvectorizer__max_df',\n 'columntransformer__countvectorizer__max_features',\n 'columntransformer__countvectorizer__min_df',\n 'columntransformer__countvectorizer__ngram_range',\n 'columntransformer__countvectorizer__preprocessor',\n 'columntransformer__countvectorizer__stop_words',\n 'columntransformer__countvectorizer__strip_accents',\n 'columntransformer__countvectorizer__token_pattern',\n 'columntransformer__countvectorizer__tokenizer',\n 'columntransformer__countvectorizer__vocabulary',\n 'columntransformer__simpleimputer__add_indicator',\n 'columntransformer__simpleimputer__copy',\n 'columntransformer__simpleimputer__fill_value',\n 'columntransformer__simpleimputer__missing_values',\n 'columntransformer__simpleimputer__strategy',\n 'columntransformer__simpleimputer__verbose',\n 'logisticregression__C',\n 'logisticregression__class_weight',\n 'logisticregression__dual',\n 'logisticregression__fit_intercept',\n 'logisticregression__intercept_scaling',\n 'logisticregression__l1_ratio',\n 'logisticregression__max_iter',\n 'logisticregression__multi_class',\n 'logisticregression__n_jobs',\n 'logisticregression__penalty',\n 'logisticregression__random_state',\n 'logisticregression__solver',\n 'logisticregression__tol',\n 'logisticregression__verbose',\n 'logisticregression__warm_start']\n\n\nMoving along, the second parameter we‚Äôre going to tune is the ngram_range parameter of CountVectorizer.\nAgain, we specify the Pipeline step name, then the transformer name, and then the parameter name. Note that these three components are separated by double underscores, but there‚Äôs just a single underscore within ngram_range because that‚Äôs part of the parameter name.\nThe parameter values we‚Äôre going to try are the tuples (1, 1) and (1, 2). (1, 1) is the default, and it creates a single feature from each word. (1, 2) creates features from both single words, known as unigrams, and word pairs, known as bigrams.\n\n\n\n\n\n\nCountVectorizer tuning parameter:\n\nngram_range: Selection of word n-grams to be extracted as features\n\n(1, 1) (default)\n(1, 2)\n\n\n\n\n\n\nparams['columntransformer__countvectorizer__ngram_range'] = [(1, 1),\n                                                             (1, 2)]\n\nThe third parameter we‚Äôre going to tune is the add_indicator parameter of SimpleImputer, which was added to scikit-learn in version 0.21 and which I discussed in lesson¬†7.4.\nOnce again, we specify the Pipeline step name, then the transformer name, and then the parameter name.\nThe parameter values we‚Äôre going to try are False and True. False is the default, and it does not add a missing indicator column, whereas True does add a missing indicator column.\n\n\n\n\n\n\nSimpleImputer tuning parameter:\n\nadd_indicator: Option to add a missing indicator column\n\nFalse (default)\nTrue\n\n\n\n\n\n\nparams['columntransformer__simpleimputer__add_indicator'] = [False, True]\n\nBefore running the grid search, we‚Äôll print out the params dictionary. By multiplying 2 by 3 by 2 by 2 by 2, we can calculate that there are now 48 parameter combinations, and thus the grid search will take about 8 times longer than the previous search.\nAs an aside, if we had used the Pipeline and ColumnTransformer classes instead of the make_pipeline and make_column_transformer functions, we could have customized the step names and transformer names, which would have made these parameter specifications a bit easier to read and write. You can watch lessons¬†4.9 and 4.10 for a review of that topic.\n\nparams\n\n{'logisticregression__penalty': ['l1', 'l2'],\n 'logisticregression__C': [0.1, 1, 10],\n 'columntransformer__pipeline__onehotencoder__drop': [None, 'first'],\n 'columntransformer__countvectorizer__ngram_range': [(1, 1), (1, 2)],\n 'columntransformer__simpleimputer__add_indicator': [False, True]}\n\n\nAnyway, next we‚Äôll recreate the grid object with the new params dictionary, and then we‚Äôll run the grid search.\n\ngrid = GridSearchCV(pipe, params, cv=5, scoring='accuracy')\ngrid.fit(X, y)\n\nGridSearchCVGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('columntransformer',\n                                        ColumnTransformer(transformers=[('pipeline',\n                                                                         Pipeline(steps=[('simpleimputer',\n                                                                                          SimpleImputer(fill_value='missing',\n                                                                                                        strategy='constant')),\n                                                                                         ('onehotencoder',\n                                                                                          OneHotEncoder())]),\n                                                                         ['Embarked',\n                                                                          'Sex']),\n                                                                        ('countvectorizer',\n                                                                         CountVectorizer(),\n                                                                         'Name'),\n                                                                        ('simpleimputer',\n                                                                         SimpleImputer(),\n                                                                         ['Age',\n                                                                          'Fare']),\n                                                                        (...\n                                        LogisticRegression(random_state=1,\n                                                           solver='liblinear'))]),\n             param_grid={'columntransformer__countvectorizer__ngram_range': [(1,\n                                                                              1),\n                                                                             (1,\n                                                                              2)],\n                         'columntransformer__pipeline__onehotencoder__drop': [None,\n                                                                              'first'],\n                         'columntransformer__simpleimputer__add_indicator': [False,\n                                                                             True],\n                         'logisticregression__C': [0.1, 1, 10],\n                         'logisticregression__penalty': ['l1', 'l2']},\n             scoring='accuracy')columntransformer: ColumnTransformerColumnTransformer(transformers=[('pipeline',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer(fill_value='missing',\n                                                                strategy='constant')),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder())]),\n                                 ['Embarked', 'Sex']),\n                                ('countvectorizer', CountVectorizer(), 'Name'),\n                                ('simpleimputer', SimpleImputer(),\n                                 ['Age', 'Fare']),\n                                ('passthrough', 'passthrough', ['Parch'])])pipeline['Embarked', 'Sex']SimpleImputerSimpleImputer(fill_value='missing', strategy='constant')OneHotEncoderOneHotEncoder()countvectorizerNameCountVectorizerCountVectorizer()simpleimputer['Age', 'Fare']SimpleImputerSimpleImputer()passthrough['Parch']passthroughpassthroughLogisticRegressionLogisticRegression(random_state=1, solver='liblinear')\n\n\nNow that the search is complete, we‚Äôll convert the search results into a DataFrame and sort it by the rank_test_score column.\nAs you can see from the mean_test_score column, the best accuracy of 0.828 is an improvement over the previous grid search, which had an accuracy of 0.818. Keep in mind that your exact results may differ based on your scikit-learn version along with other factors. However, there‚Äôs no randomness involved when you set cv to an integer, and so your results will be the same every time you run this grid search.\n\nresults = (pd.DataFrame(grid.cv_results_)\n           .filter(regex='param_|mean_test|rank'))\nresults.columns = results.columns.str.split('__').str[-1]\nresults.sort_values('rank_test_score')\n\n\n\n\n\n\n\n\nngram_range\ndrop\nadd_indicator\nC\npenalty\nmean_test_score\nrank_test_score\n\n\n\n\n34\n(1, 2)\nNone\nTrue\n10\nl1\n0.828253\n1\n\n\n28\n(1, 2)\nNone\nFalse\n10\nl1\n0.824889\n2\n\n\n40\n(1, 2)\nfirst\nFalse\n10\nl1\n0.824889\n2\n\n\n46\n(1, 2)\nfirst\nTrue\n10\nl1\n0.822648\n4\n\n\n16\n(1, 1)\nfirst\nFalse\n10\nl1\n0.820407\n5\n\n\n22\n(1, 1)\nfirst\nTrue\n10\nl1\n0.819296\n6\n\n\n4\n(1, 1)\nNone\nFalse\n10\nl1\n0.818166\n7\n\n\n10\n(1, 1)\nNone\nTrue\n10\nl1\n0.817061\n8\n\n\n20\n(1, 1)\nfirst\nTrue\n1\nl1\n0.814820\n9\n\n\n2\n(1, 1)\nNone\nFalse\n1\nl1\n0.814814\n10\n\n\n44\n(1, 2)\nfirst\nTrue\n1\nl1\n0.813703\n11\n\n\n47\n(1, 2)\nfirst\nTrue\n10\nl2\n0.812598\n12\n\n\n8\n(1, 1)\nNone\nTrue\n1\nl1\n0.812579\n13\n\n\n38\n(1, 2)\nfirst\nFalse\n1\nl1\n0.812579\n14\n\n\n14\n(1, 1)\nfirst\nFalse\n1\nl1\n0.812579\n14\n\n\n26\n(1, 2)\nNone\nFalse\n1\nl1\n0.812567\n16\n\n\n11\n(1, 1)\nNone\nTrue\n10\nl2\n0.811481\n17\n\n\n21\n(1, 1)\nfirst\nTrue\n1\nl2\n0.811468\n18\n\n\n3\n(1, 1)\nNone\nFalse\n1\nl2\n0.811462\n19\n\n\n23\n(1, 1)\nfirst\nTrue\n10\nl2\n0.810363\n20\n\n\n9\n(1, 1)\nNone\nTrue\n1\nl2\n0.810345\n21\n\n\n15\n(1, 1)\nfirst\nFalse\n1\nl2\n0.810332\n22\n\n\n32\n(1, 2)\nNone\nTrue\n1\nl1\n0.810332\n22\n\n\n17\n(1, 1)\nfirst\nFalse\n10\nl2\n0.809234\n24\n\n\n35\n(1, 2)\nNone\nTrue\n10\nl2\n0.809234\n24\n\n\n5\n(1, 1)\nNone\nFalse\n10\nl2\n0.809234\n24\n\n\n29\n(1, 2)\nNone\nFalse\n10\nl2\n0.808104\n27\n\n\n45\n(1, 2)\nfirst\nTrue\n1\nl2\n0.808097\n28\n\n\n41\n(1, 2)\nfirst\nFalse\n10\nl2\n0.806980\n29\n\n\n39\n(1, 2)\nfirst\nFalse\n1\nl2\n0.805844\n30\n\n\n27\n(1, 2)\nNone\nFalse\n1\nl2\n0.805844\n30\n\n\n33\n(1, 2)\nNone\nTrue\n1\nl2\n0.804739\n32\n\n\n31\n(1, 2)\nNone\nTrue\n0.1\nl2\n0.793491\n33\n\n\n7\n(1, 1)\nNone\nTrue\n0.1\nl2\n0.793484\n34\n\n\n19\n(1, 1)\nfirst\nTrue\n0.1\nl2\n0.791243\n35\n\n\n43\n(1, 2)\nfirst\nTrue\n0.1\nl2\n0.790114\n36\n\n\n37\n(1, 2)\nfirst\nFalse\n0.1\nl2\n0.789003\n37\n\n\n25\n(1, 2)\nNone\nFalse\n0.1\nl2\n0.788996\n38\n\n\n1\n(1, 1)\nNone\nFalse\n0.1\nl2\n0.788990\n39\n\n\n13\n(1, 1)\nfirst\nFalse\n0.1\nl2\n0.787885\n40\n\n\n0\n(1, 1)\nNone\nFalse\n0.1\nl1\n0.783385\n41\n\n\n30\n(1, 2)\nNone\nTrue\n0.1\nl1\n0.783385\n41\n\n\n24\n(1, 2)\nNone\nFalse\n0.1\nl1\n0.783385\n41\n\n\n6\n(1, 1)\nNone\nTrue\n0.1\nl1\n0.783385\n41\n\n\n36\n(1, 2)\nfirst\nFalse\n0.1\nl1\n0.777785\n45\n\n\n42\n(1, 2)\nfirst\nTrue\n0.1\nl1\n0.777785\n45\n\n\n12\n(1, 1)\nfirst\nFalse\n0.1\nl1\n0.777785\n45\n\n\n18\n(1, 1)\nfirst\nTrue\n0.1\nl1\n0.777785\n45\n\n\n\n\n\n\n\n\n\n\n\n\n\nPipeline accuracy scores:\n\nGrid search (5 parameters): 0.828\nGrid search (2 parameters): 0.818\nBaseline (no tuning): 0.811\n\n\n\n\nRather than always examining the results DataFrame, we can actually just access the single best score and the set of parameters that resulted in that score via attributes of the grid object.\nIt‚Äôs worth noting that only the drop parameter is using its default value, whereas the other four parameters are not using their default values.\n\ngrid.best_score_\n\n0.828253091456908\n\n\n\ngrid.best_params_\n\n{'columntransformer__countvectorizer__ngram_range': (1, 2),\n 'columntransformer__pipeline__onehotencoder__drop': None,\n 'columntransformer__simpleimputer__add_indicator': True,\n 'logisticregression__C': 10,\n 'logisticregression__penalty': 'l1'}\n\n\nIt‚Äôs hard to say whether this truly is the best set of parameters, because some of the differences in accuracy between parameter combinations may be due to chance, based on which samples happened to appear in each fold. That‚Äôs just a limitation of basic cross-validation, and so all we can say with confidence is that this is a good combination of parameters.",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Evaluating and tuning a Pipeline</span>"
    ]
  },
  {
    "objectID": "ch10.html#using-the-best-pipeline-to-make-predictions",
    "href": "ch10.html#using-the-best-pipeline-to-make-predictions",
    "title": "10¬† Evaluating and tuning a Pipeline",
    "section": "10.5 Using the best Pipeline to make predictions",
    "text": "10.5 Using the best Pipeline to make predictions\nNow that we‚Äôve tuned both the model parameters and the transformer parameters, we want to use those parameters with the Pipeline when making predictions.\nGridSearchCV actually makes this very easy. After locating the best set of parameters, it automatically refits the Pipeline on X and y using the best set of parameters, and it stores that fitted Pipeline as an attribute called best_estimator_. And as you can see, that attribute is indeed a Pipeline object.\n\ntype(grid.best_estimator_)\n\nsklearn.pipeline.Pipeline\n\n\nIf we print out the best_estimator_ attribute and click on the components, we can see that the parameters of this Pipeline match the best parameter set we located in the previous lesson.\n\ngrid.best_estimator_\n\nPipelinePipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('pipeline',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer(fill_value='missing',\n                                                                                 strategy='constant')),\n                                                                  ('onehotencoder',\n                                                                   OneHotEncoder())]),\n                                                  ['Embarked', 'Sex']),\n                                                 ('countvectorizer',\n                                                  CountVectorizer(ngram_range=(1,\n                                                                               2)),\n                                                  'Name'),\n                                                 ('simpleimputer',\n                                                  SimpleImputer(add_indicator=True),\n                                                  ['Age', 'Fare']),\n                                                 ('passthrough', 'passthrough',\n                                                  ['Parch'])])),\n                ('logisticregression',\n                 LogisticRegression(C=10, penalty='l1', random_state=1,\n                                    solver='liblinear'))])columntransformer: ColumnTransformerColumnTransformer(transformers=[('pipeline',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer(fill_value='missing',\n                                                                strategy='constant')),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder())]),\n                                 ['Embarked', 'Sex']),\n                                ('countvectorizer',\n                                 CountVectorizer(ngram_range=(1, 2)), 'Name'),\n                                ('simpleimputer',\n                                 SimpleImputer(add_indicator=True),\n                                 ['Age', 'Fare']),\n                                ('passthrough', 'passthrough', ['Parch'])])pipeline['Embarked', 'Sex']SimpleImputerSimpleImputer(fill_value='missing', strategy='constant')OneHotEncoderOneHotEncoder()countvectorizerNameCountVectorizerCountVectorizer(ngram_range=(1, 2))simpleimputer['Age', 'Fare']SimpleImputerSimpleImputer(add_indicator=True)passthrough['Parch']passthroughpassthroughLogisticRegressionLogisticRegression(C=10, penalty='l1', random_state=1, solver='liblinear')\n\n\nIn order to make predictions using this Pipeline, all we have to do is run the grid object‚Äôs predict method, which calls the predict method of the best_estimator_, and pass it X_new.\n\ngrid.predict(X_new)\n\narray([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,\n       1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n       1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,\n       1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n       1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n       0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n       1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n       1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,\n       0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n       1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,\n       1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n       0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,\n       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n       0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,\n       1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,\n       0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n       1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n       0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1])\n\n\nI just want to emphasize that this Pipeline, with the best set of parameters, was automatically refit to the entire dataset. You always train your model on the entire dataset, meaning all samples for which you know the target value, before using it to make predictions on new data, otherwise you‚Äôre throwing away valuable training data.",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Evaluating and tuning a Pipeline</span>"
    ]
  },
  {
    "objectID": "ch10.html#qa-how-do-i-save-the-best-pipeline-for-future-use",
    "href": "ch10.html#qa-how-do-i-save-the-best-pipeline-for-future-use",
    "title": "10¬† Evaluating and tuning a Pipeline",
    "section": "10.6 Q&A: How do I save the best Pipeline for future use?",
    "text": "10.6 Q&A: How do I save the best Pipeline for future use?\nAfter completing a grid search, you may want to save the Pipeline with the best set of parameters so that you can use it to make predictions later.\nAs we saw in the previous lesson, the Pipeline with the best set of parameters is stored as an attribute of the GridSearchCV object called best_estimator_, so this is the object that we want to save.\n\ntype(grid.best_estimator_)\n\nsklearn.pipeline.Pipeline\n\n\nYou can save a Pipeline to a file using pickle, which is part of the Python standard library.\n\nimport pickle\n\nWe‚Äôll use pickle‚Äôs dump method to save the Pipeline to a file called ‚Äúpipe.pickle‚Äù.\n\nwith open('pipe.pickle', 'wb') as f:\n    pickle.dump(grid.best_estimator_, f)\n\nThen we can use pickle‚Äôs load method to load the Pipeline from the file into an object called pipe_from_pickle.\n\nwith open('pipe.pickle', 'rb') as f:\n    pipe_from_pickle = pickle.load(f)\n\npipe_from_pickle is identical to grid.best_estimator_, and so when we use pipe_from_pickle to make predictions, these predictions are identical to the predictions made by the grid object.\n\npipe_from_pickle.predict(X_new)\n\narray([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,\n       1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n       1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,\n       1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n       1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n       0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n       1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n       1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,\n       0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n       1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,\n       1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n       0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,\n       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n       0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,\n       1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,\n       0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n       1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n       0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1])\n\n\nOne alternative to pickle is joblib, which is usually more efficient than pickle for scikit-learn objects. Although it‚Äôs not part of the Python standard library, joblib has been a dependency of scikit-learn since version 0.21.\n\nimport joblib\n\nJust like pickle, you use joblib‚Äôs dump method to save the Pipeline to a file, which we‚Äôll call ‚Äúpipe.joblib‚Äù.\n\njoblib.dump(grid.best_estimator_, 'pipe.joblib')\n\n['pipe.joblib']\n\n\nThen, we‚Äôll use the load method to load the Pipeline from the file into an object called pipe_from_joblib.\n\npipe_from_joblib = joblib.load('pipe.joblib')\n\nFinally, we‚Äôll use pipe_from_joblib to make predictions.\n\npipe_from_joblib.predict(X_new)\n\narray([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,\n       1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n       1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,\n       1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n       1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n       0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n       1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n       1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,\n       0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n       1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,\n       1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n       0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,\n       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n       0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,\n       1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,\n       0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n       1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n       0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1])\n\n\nTo be clear, pickle and joblib are not limited to Pipelines and can be used with other scikit-learn objects, such as a standalone model object that is not inside a Pipeline.\nThere are a couple warnings to keep in mind when working with pickle and joblib objects:\n\nFirst, the objects may be version-specific and architecture-specific. As such, you should only load them into an identical environment, meaning the same versions of scikit-learn and its dependencies, using the identical computing architecture.\nSecond, these objects can be poisoned with malicious code, and so you should only load objects from a trusted source.\n\n\n\n\n\n\n\nWarnings for pickle and joblib objects:\n\nMay be version-specific and architecture-specific\nCan be poisoned with malicious code\n\n\n\n\nFinally, it‚Äôs worth mentioning that there are alternatives to pickle and joblib such as ONNX and PMML. These formats don‚Äôt capture the full model object, but instead save a representation that can be used to make predictions. One major benefit of these formats is that they are neither environment-specific nor architecture-specific.\n\n\n\n\n\n\nAlternatives to pickle and joblib:\n\nExamples: ONNX, PMML\nSave a model representation for making predictions\nWork across environments and architectures",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Evaluating and tuning a Pipeline</span>"
    ]
  },
  {
    "objectID": "ch10.html#qa-how-do-i-speed-up-a-grid-search",
    "href": "ch10.html#qa-how-do-i-speed-up-a-grid-search",
    "title": "10¬† Evaluating and tuning a Pipeline",
    "section": "10.7 Q&A: How do I speed up a grid search?",
    "text": "10.7 Q&A: How do I speed up a grid search?\nLet‚Äôs recreate the GridSearchCV object, but this time we‚Äôll add the verbose parameter and set it to 1. When we run the search, this parameter will cause two changes to the output:\n\nFirst, it will calculate the number of parameter combinations for you, which is 48. Since this is 5-fold cross-validation, that means the Pipeline will be fit 240 times.\nSecond, it will report back how long the search took, and sometimes it will give you progress updates along the way.\n\n\ngrid = GridSearchCV(pipe, params, cv=5, scoring='accuracy', verbose=1)\ngrid.fit(X, y)\n\nFitting 5 folds for each of 48 candidates, totalling 240 fits\n\n\n[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.5s\n[Parallel(n_jobs=1)]: Done 199 tasks      | elapsed:    2.4s\n\n\nGridSearchCVGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('columntransformer',\n                                        ColumnTransformer(transformers=[('pipeline',\n                                                                         Pipeline(steps=[('simpleimputer',\n                                                                                          SimpleImputer(fill_value='missing',\n                                                                                                        strategy='constant')),\n                                                                                         ('onehotencoder',\n                                                                                          OneHotEncoder())]),\n                                                                         ['Embarked',\n                                                                          'Sex']),\n                                                                        ('countvectorizer',\n                                                                         CountVectorizer(),\n                                                                         'Name'),\n                                                                        ('simpleimputer',\n                                                                         SimpleImputer(),\n                                                                         ['Age',\n                                                                          'Fare']),\n                                                                        (...\n                                        LogisticRegression(random_state=1,\n                                                           solver='liblinear'))]),\n             param_grid={'columntransformer__countvectorizer__ngram_range': [(1,\n                                                                              1),\n                                                                             (1,\n                                                                              2)],\n                         'columntransformer__pipeline__onehotencoder__drop': [None,\n                                                                              'first'],\n                         'columntransformer__simpleimputer__add_indicator': [False,\n                                                                             True],\n                         'logisticregression__C': [0.1, 1, 10],\n                         'logisticregression__penalty': ['l1', 'l2']},\n             scoring='accuracy', verbose=1)columntransformer: ColumnTransformerColumnTransformer(transformers=[('pipeline',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer(fill_value='missing',\n                                                                strategy='constant')),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder())]),\n                                 ['Embarked', 'Sex']),\n                                ('countvectorizer', CountVectorizer(), 'Name'),\n                                ('simpleimputer', SimpleImputer(),\n                                 ['Age', 'Fare']),\n                                ('passthrough', 'passthrough', ['Parch'])])pipeline['Embarked', 'Sex']SimpleImputerSimpleImputer(fill_value='missing', strategy='constant')OneHotEncoderOneHotEncoder()countvectorizerNameCountVectorizerCountVectorizer()simpleimputer['Age', 'Fare']SimpleImputerSimpleImputer()passthrough['Parch']passthroughpassthroughLogisticRegressionLogisticRegression(random_state=1, solver='liblinear')\n\n\nNow, let‚Äôs also add the n_jobs parameter, set it to -1, and re-run the grid search. This instructs scikit-learn to use parallel processing with all of your CPUs to perform the search. If your machine has multiple processors, this will generally be faster, though in this case it took about the same amount of time.\n\ngrid = GridSearchCV(pipe, params, cv=5, scoring='accuracy', verbose=1,\n                    n_jobs=-1)\ngrid.fit(X, y)\n\nFitting 5 folds for each of 48 candidates, totalling 240 fits\n\n\n[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.9s\n[Parallel(n_jobs=-1)]: Done 225 out of 240 | elapsed:    2.5s remaining:    0.2s\n[Parallel(n_jobs=-1)]: Done 240 out of 240 | elapsed:    2.5s finished\n\n\nGridSearchCVGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('columntransformer',\n                                        ColumnTransformer(transformers=[('pipeline',\n                                                                         Pipeline(steps=[('simpleimputer',\n                                                                                          SimpleImputer(fill_value='missing',\n                                                                                                        strategy='constant')),\n                                                                                         ('onehotencoder',\n                                                                                          OneHotEncoder())]),\n                                                                         ['Embarked',\n                                                                          'Sex']),\n                                                                        ('countvectorizer',\n                                                                         CountVectorizer(),\n                                                                         'Name'),\n                                                                        ('simpleimputer',\n                                                                         SimpleImputer(),\n                                                                         ['Age',\n                                                                          'Fare']),\n                                                                        (...\n                                        LogisticRegression(random_state=1,\n                                                           solver='liblinear'))]),\n             n_jobs=-1,\n             param_grid={'columntransformer__countvectorizer__ngram_range': [(1,\n                                                                              1),\n                                                                             (1,\n                                                                              2)],\n                         'columntransformer__pipeline__onehotencoder__drop': [None,\n                                                                              'first'],\n                         'columntransformer__simpleimputer__add_indicator': [False,\n                                                                             True],\n                         'logisticregression__C': [0.1, 1, 10],\n                         'logisticregression__penalty': ['l1', 'l2']},\n             scoring='accuracy', verbose=1)columntransformer: ColumnTransformerColumnTransformer(transformers=[('pipeline',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer(fill_value='missing',\n                                                                strategy='constant')),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder())]),\n                                 ['Embarked', 'Sex']),\n                                ('countvectorizer', CountVectorizer(), 'Name'),\n                                ('simpleimputer', SimpleImputer(),\n                                 ['Age', 'Fare']),\n                                ('passthrough', 'passthrough', ['Parch'])])pipeline['Embarked', 'Sex']SimpleImputerSimpleImputer(fill_value='missing', strategy='constant')OneHotEncoderOneHotEncoder()countvectorizerNameCountVectorizerCountVectorizer()simpleimputer['Age', 'Fare']SimpleImputerSimpleImputer()passthrough['Parch']passthroughpassthroughLogisticRegressionLogisticRegression(random_state=1, solver='liblinear')\n\n\nIf you find it useful to know how long a search takes, but verbose mode is a bit too verbose for you, another option is to remove the verbose parameter and instead prefix the second line with %time. This is known as an IPython line magic, and it will work as long as you‚Äôre using the Jupyter notebook or the IPython interpreter.\nAll this command does is tell you how long a particular line of code took to run. The number to focus on is the wall time.\n\ngrid = GridSearchCV(pipe, params, cv=5, scoring='accuracy', n_jobs=-1)\n%time grid.fit(X, y)\n\nCPU times: user 217 ms, sys: 9.15 ms, total: 226 ms\nWall time: 729 ms\n\n\nGridSearchCVGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('columntransformer',\n                                        ColumnTransformer(transformers=[('pipeline',\n                                                                         Pipeline(steps=[('simpleimputer',\n                                                                                          SimpleImputer(fill_value='missing',\n                                                                                                        strategy='constant')),\n                                                                                         ('onehotencoder',\n                                                                                          OneHotEncoder())]),\n                                                                         ['Embarked',\n                                                                          'Sex']),\n                                                                        ('countvectorizer',\n                                                                         CountVectorizer(),\n                                                                         'Name'),\n                                                                        ('simpleimputer',\n                                                                         SimpleImputer(),\n                                                                         ['Age',\n                                                                          'Fare']),\n                                                                        (...\n                                        LogisticRegression(random_state=1,\n                                                           solver='liblinear'))]),\n             n_jobs=-1,\n             param_grid={'columntransformer__countvectorizer__ngram_range': [(1,\n                                                                              1),\n                                                                             (1,\n                                                                              2)],\n                         'columntransformer__pipeline__onehotencoder__drop': [None,\n                                                                              'first'],\n                         'columntransformer__simpleimputer__add_indicator': [False,\n                                                                             True],\n                         'logisticregression__C': [0.1, 1, 10],\n                         'logisticregression__penalty': ['l1', 'l2']},\n             scoring='accuracy')columntransformer: ColumnTransformerColumnTransformer(transformers=[('pipeline',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer(fill_value='missing',\n                                                                strategy='constant')),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder())]),\n                                 ['Embarked', 'Sex']),\n                                ('countvectorizer', CountVectorizer(), 'Name'),\n                                ('simpleimputer', SimpleImputer(),\n                                 ['Age', 'Fare']),\n                                ('passthrough', 'passthrough', ['Parch'])])pipeline['Embarked', 'Sex']SimpleImputerSimpleImputer(fill_value='missing', strategy='constant')OneHotEncoderOneHotEncoder()countvectorizerNameCountVectorizerCountVectorizer()simpleimputer['Age', 'Fare']SimpleImputerSimpleImputer()passthrough['Parch']passthroughpassthroughLogisticRegressionLogisticRegression(random_state=1, solver='liblinear')\n\n\nMy general recommendation is to set n_jobs to -1 any time you‚Äôre running a grid search, which is what I‚Äôll do for the rest of the book. However, it‚Äôs still a good idea to use %time or verbose mode to confirm that parallel processing is actually reducing the search time on your particular machine.",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Evaluating and tuning a Pipeline</span>"
    ]
  },
  {
    "objectID": "ch10.html#qa-how-do-i-tune-a-pipeline-with-randomized-search",
    "href": "ch10.html#qa-how-do-i-tune-a-pipeline-with-randomized-search",
    "title": "10¬† Evaluating and tuning a Pipeline",
    "section": "10.8 Q&A: How do I tune a Pipeline with randomized search?",
    "text": "10.8 Q&A: How do I tune a Pipeline with randomized search?\nWhen you provide a set of parameter values to GridSearchCV, it will cross-validate every possible combination of those parameters. For example, we know that with this set of parameters, cross-validation will run 48 times.\n\nparams\n\n{'logisticregression__penalty': ['l1', 'l2'],\n 'logisticregression__C': [0.1, 1, 10],\n 'columntransformer__pipeline__onehotencoder__drop': [None, 'first'],\n 'columntransformer__countvectorizer__ngram_range': [(1, 1), (1, 2)],\n 'columntransformer__simpleimputer__add_indicator': [False, True]}\n\n\nLet‚Äôs say that we wanted to try additional C values for logistic regression. I‚Äôll make a copy of the params dictionary called more_params, and then modify the C parameter in this dictionary to have 6 possible values instead of 3.\n\nmore_params = params.copy()\nmore_params['logisticregression__C'] = [0.01, 0.1, 1, 10, 100, 1000]\n\nSince there are twice as many C values, we know that a grid search will take twice as long, meaning it will run cross-validation 96 times. But what if that grid search takes more time than we have available?\nAn alternative method we can use is called randomized search, which is implemented in the RandomizedSearchCV class. We‚Äôll import it from the model_selection module and then create an instance.\nThe API is very similar to GridSearchCV, except that you also specify the number of times it should run using the n_iter parameter. In this case, we‚Äôll set the number of iterations to be 10.\nEach time it runs, it will pick out a set of parameters at random and cross-validate that parameter set. In other words, it does the same thing as GridSearchCV, except that it picks out random combinations of parameters from the parameter dictionary rather than trying every single combination. Because there‚Äôs an element of randomness, we‚Äôll also set the random_state parameter to 1 for reproducibility.\nWe‚Äôll use the fit method to run the search, and because it will only try 10 combinations instead of 96 combinations, it will run about 10 times faster than a grid search would.\n\n\n\n\n\n\nHow to use RandomizedSearchCV:\n\nn_iter: Specify the number of randomly-chosen parameter combinations to cross-validate\nrandom_state: Set to any integer for reproducibility\n\n\n\n\n\nfrom sklearn.model_selection import RandomizedSearchCV\nrand = RandomizedSearchCV(pipe, more_params, cv=5, scoring='accuracy',\n                          n_iter=10, random_state=1, n_jobs=-1)\nrand.fit(X, y)\n\nRandomizedSearchCVRandomizedSearchCV(cv=5,\n                   estimator=Pipeline(steps=[('columntransformer',\n                                              ColumnTransformer(transformers=[('pipeline',\n                                                                               Pipeline(steps=[('simpleimputer',\n                                                                                                SimpleImputer(fill_value='missing',\n                                                                                                              strategy='constant')),\n                                                                                               ('onehotencoder',\n                                                                                                OneHotEncoder())]),\n                                                                               ['Embarked',\n                                                                                'Sex']),\n                                                                              ('countvectorizer',\n                                                                               CountVectorizer(),\n                                                                               'Name'),\n                                                                              ('simpleimputer',\n                                                                               SimpleImputer(),\n                                                                               ['Age',\n                                                                                'Far...\n                   n_jobs=-1,\n                   param_distributions={'columntransformer__countvectorizer__ngram_range': [(1,\n                                                                                             1),\n                                                                                            (1,\n                                                                                             2)],\n                                        'columntransformer__pipeline__onehotencoder__drop': [None,\n                                                                                             'first'],\n                                        'columntransformer__simpleimputer__add_indicator': [False,\n                                                                                            True],\n                                        'logisticregression__C': [0.01, 0.1, 1,\n                                                                  10, 100,\n                                                                  1000],\n                                        'logisticregression__penalty': ['l1',\n                                                                        'l2']},\n                   random_state=1, scoring='accuracy')columntransformer: ColumnTransformerColumnTransformer(transformers=[('pipeline',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer(fill_value='missing',\n                                                                strategy='constant')),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder())]),\n                                 ['Embarked', 'Sex']),\n                                ('countvectorizer', CountVectorizer(), 'Name'),\n                                ('simpleimputer', SimpleImputer(),\n                                 ['Age', 'Fare']),\n                                ('passthrough', 'passthrough', ['Parch'])])pipeline['Embarked', 'Sex']SimpleImputerSimpleImputer(fill_value='missing', strategy='constant')OneHotEncoderOneHotEncoder()countvectorizerNameCountVectorizerCountVectorizer()simpleimputer['Age', 'Fare']SimpleImputerSimpleImputer()passthrough['Parch']passthroughpassthroughLogisticRegressionLogisticRegression(random_state=1, solver='liblinear')\n\n\nBy printing out the results of the search, you can see that it ran 10 times using random combinations of all of those parameters.\n\nresults = (pd.DataFrame(rand.cv_results_)\n           .filter(regex='param_|mean_test|rank'))\nresults.columns = results.columns.str.split('__').str[-1]\nresults\n\n\n\n\n\n\n\n\npenalty\nC\nadd_indicator\ndrop\nngram_range\nmean_test_score\nrank_test_score\n\n\n\n\n0\nl1\n1\nTrue\nfirst\n(1, 1)\n0.814820\n3\n\n\n1\nl2\n10\nFalse\nfirst\n(1, 1)\n0.809234\n7\n\n\n2\nl1\n1000\nTrue\nfirst\n(1, 1)\n0.811437\n5\n\n\n3\nl2\n1000\nFalse\nNone\n(1, 2)\n0.810345\n6\n\n\n4\nl1\n10\nFalse\nfirst\n(1, 2)\n0.824889\n2\n\n\n5\nl1\n0.1\nFalse\nfirst\n(1, 2)\n0.777785\n9\n\n\n6\nl2\n1\nTrue\nNone\n(1, 2)\n0.804739\n8\n\n\n7\nl1\n100\nTrue\nfirst\n(1, 1)\n0.813684\n4\n\n\n8\nl1\n100\nFalse\nfirst\n(1, 2)\n0.827129\n1\n\n\n9\nl2\n0.01\nTrue\nfirst\n(1, 2)\n0.744184\n10\n\n\n\n\n\n\n\nYou might be surprised to know that the best score it found, 0.827, is almost as high as the best score found by our grid search earlier in the chapter, which was 0.828. That being said, we did try additional C values in our randomized search, so the comparison isn‚Äôt entirely fair.\n\nrand.best_score_\n\n0.8271294959512898\n\n\n\n\n\n\n\n\nPipeline accuracy scores:\n\nGrid search (5 parameters): 0.828\nRandomized search (more C values): 0.827\nGrid search (2 parameters): 0.818\nBaseline (no tuning): 0.811\n\n\n\n\nHere‚Äôs the set of parameters that produced that score.\n\nrand.best_params_\n\n{'logisticregression__penalty': 'l1',\n 'logisticregression__C': 100,\n 'columntransformer__simpleimputer__add_indicator': False,\n 'columntransformer__pipeline__onehotencoder__drop': 'first',\n 'columntransformer__countvectorizer__ngram_range': (1, 2)}\n\n\nThere are four things I especially like about using a randomized search instead of a grid search:\nFirst, randomized search will usually find the best result (or almost the best result) in far less time than grid search, which is what we saw above. This is because there are often a lot of parameter combinations that will produce similar results to one another.\nSecond, it‚Äôs easier to control the computational budget of a randomized search. You can test how long a small number of searches takes, and then if you have a certain amount of time available for a search, you can simply choose the number of iterations that can be completed within that time period.\nThird, randomized search gives you the freedom to tune many more model and transformer parameters without worrying that it will take forever. You can try out a ton of different parameters for a short amount of time, and then narrow down which parameters to focus on based on what seems to be working. (We‚Äôll see this in practice in the next chapter.)\nFourth, randomized search will sometimes produce even better results than grid search because you can try a finer grid. For example, let‚Äôs say you were tuning a parameter that allowed continuous values from 0 to 1. If you were using a grid search, you might try the values 0, 0.5, and 1. But if you were using randomized search, you might try the values 0, 0.01, 0.02, and so on. It may turn out that the best value for this parameter is around 0.3, and randomized search could help you to find that out, whereas this grid search would have no chance of finding that out.\n\n\n\n\n\n\nWhy use RandomizedSearchCV instead of GridSearchCV?\n\nSimilar results in far less time\nEasier to control the computational budget\nFreedom to tune many more parameters\nCan use a much finer grid\n\n\n\n\nIf you do need to create a fine grid of numbers for a randomized search, one useful function is NumPy‚Äôs linspace. For example, this code specifies that I want 101 equally spaced values, starting with 0 and ending with 1.\n\nimport numpy as np\nnp.linspace(0, 1, 101)\n\narray([0.  , 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1 ,\n       0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2 , 0.21,\n       0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3 , 0.31, 0.32,\n       0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4 , 0.41, 0.42, 0.43,\n       0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5 , 0.51, 0.52, 0.53, 0.54,\n       0.55, 0.56, 0.57, 0.58, 0.59, 0.6 , 0.61, 0.62, 0.63, 0.64, 0.65,\n       0.66, 0.67, 0.68, 0.69, 0.7 , 0.71, 0.72, 0.73, 0.74, 0.75, 0.76,\n       0.77, 0.78, 0.79, 0.8 , 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87,\n       0.88, 0.89, 0.9 , 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98,\n       0.99, 1.  ])\n\n\nAnother similar function is NumPy‚Äôs logspace. This code specifies that I want 6 values, from 10 to the negative 2nd power through 10 to the 3rd power.\n\nnp.logspace(-2, 3, 6)\n\narray([1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03])\n\n\nIf you‚Äôre comfortable using the SciPy library, you can instead specify continuous parameters for a randomized search using SciPy distributions. However, I find it much easier to just use NumPy‚Äôs linspace and logspace functions.",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Evaluating and tuning a Pipeline</span>"
    ]
  },
  {
    "objectID": "ch10.html#qa-whats-the-target-accuracy-we-are-trying-to-achieve",
    "href": "ch10.html#qa-whats-the-target-accuracy-we-are-trying-to-achieve",
    "title": "10¬† Evaluating and tuning a Pipeline",
    "section": "10.9 Q&A: What‚Äôs the target accuracy we are trying to achieve?",
    "text": "10.9 Q&A: What‚Äôs the target accuracy we are trying to achieve?\nWhen you‚Äôre building and tuning a modeling Pipeline, it‚Äôs natural to wonder how you‚Äôll know when you‚Äôre done. In other words, how good of a model is ‚Äúgood enough‚Äù? There are three ways that I tend to think about this question.\n\n\n\n\n\n\nWhen is a model ‚Äúgood enough‚Äù?\n\nUseful model: Outperforms null accuracy\nBest possible model: Usually impossible to know the theoretical maximum accuracy\nPractical model: Continue improving until you run out of resources\n\n\n\n\nThe first way is to ask the question: What is the minimum accuracy that we need to achieve for our model to be considered useful? In most cases, you want your model to at least outperform null accuracy, which is the accuracy you could achieve by always predicting the most frequent class.\nTo calculate the null accuracy for our training data, we use the value_counts method on y, and set normalize to True in order to display the counts as a percentage. From the results, we can see that class 0 is the most frequent class, and about 61.6% of the y values are class 0.\n\ny.value_counts(normalize=True)\n\n0    0.616162\n1    0.383838\nName: Survived, dtype: float64\n\n\nThus the null accuracy for this problem is 61.6%, since an uninformed model, also known as the null model, could achieve that accuracy simply by predicting class 0 in all cases. In other words, this is the accuracy level that we want to outperform, otherwise the model is not providing any value. Thankfully, all of our Pipelines are outperforming null accuracy by a considerable amount.\n\n\n\n\n\n\nPipeline accuracy scores:\n\nGrid search (5 parameters): 0.828\nRandomized search (more C values): 0.827\nGrid search (2 parameters): 0.818\nBaseline (no tuning): 0.811\nNull model: 0.616\n\n\n\n\nThe second way to think about this question is to ask: What is the maximum accuracy we could eventually reach? For most real problems, it‚Äôs impossible to know how accurate your model could be if you did enough tuning and tried enough models. It‚Äôs also impossible to know how accurate your model could be if you gathered more samples or more features. The main exception to this is if you‚Äôre working on a well-studied research problem, because in that case there may be a state-of-the-art benchmark that everyone is trying to surpass.\nThus in most practical circumstances, you don‚Äôt set a target accuracy. Instead, you work to improve the model until you run out of time, money, or ideas.",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Evaluating and tuning a Pipeline</span>"
    ]
  },
  {
    "objectID": "ch10.html#qa-is-it-okay-that-our-model-includes-thousands-of-features",
    "href": "ch10.html#qa-is-it-okay-that-our-model-includes-thousands-of-features",
    "title": "10¬† Evaluating and tuning a Pipeline",
    "section": "10.10 Q&A: Is it okay that our model includes thousands of features?",
    "text": "10.10 Q&A: Is it okay that our model includes thousands of features?\nThe pipe object is our Pipeline that hasn‚Äôt been tuned by grid search. Recall that you can examine an individual Pipeline step by using the named_steps attribute. In this case, we‚Äôll select the first step, which is our ColumnTransformer.\n\npipe.named_steps['columntransformer']\n\nColumnTransformerColumnTransformer(transformers=[('pipeline',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer(fill_value='missing',\n                                                                strategy='constant')),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder())]),\n                                 ['Embarked', 'Sex']),\n                                ('countvectorizer', CountVectorizer(), 'Name'),\n                                ('simpleimputer', SimpleImputer(),\n                                 ['Age', 'Fare']),\n                                ('passthrough', 'passthrough', ['Parch'])])pipeline['Embarked', 'Sex']SimpleImputerSimpleImputer(fill_value='missing', strategy='constant')OneHotEncoderOneHotEncoder()countvectorizerNameCountVectorizerCountVectorizer()simpleimputer['Age', 'Fare']SimpleImputerSimpleImputer()passthrough['Parch']passthroughpassthrough\n\n\nBy passing X to its fit_transform method, we can see that the ColumnTransformer outputs 1518 feature columns. As we saw in lesson¬†8.4, all except 9 of those features were created from the Name column by CountVectorizer.\n\npipe.named_steps['columntransformer'].fit_transform(X)\n\n&lt;891x1518 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n    with 7328 stored elements in Compressed Sparse Row format&gt;\n\n\nThe cross-validated accuracy of this Pipeline is 0.811, which we‚Äôve been calling the baseline accuracy against which other Pipelines can be compared.\n\ncross_val_score(pipe, X, y, cv=5, scoring='accuracy').mean()\n\n0.8114619295712762\n\n\n\n\n\n\n\n\nPipeline accuracy scores:\n\nGrid search (5 parameters): 0.828\nRandomized search (more C values): 0.827\nGrid search (2 parameters): 0.818\nBaseline (no tuning): 0.811\nNull model: 0.616\n\n\n\n\nSimilarly, we can select the ColumnTransformer from our Pipeline that was tuned by grid search. Notice that the ngram_range for CountVectorizer is (1, 2), meaning CountVectorizer will create features from both unigrams and bigrams in the Name column.\n\ngrid.best_estimator_.named_steps['columntransformer']\n\nColumnTransformerColumnTransformer(transformers=[('pipeline',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer(fill_value='missing',\n                                                                strategy='constant')),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder())]),\n                                 ['Embarked', 'Sex']),\n                                ('countvectorizer',\n                                 CountVectorizer(ngram_range=(1, 2)), 'Name'),\n                                ('simpleimputer',\n                                 SimpleImputer(add_indicator=True),\n                                 ['Age', 'Fare']),\n                                ('passthrough', 'passthrough', ['Parch'])])pipeline['Embarked', 'Sex']SimpleImputerSimpleImputer(fill_value='missing', strategy='constant')OneHotEncoderOneHotEncoder()countvectorizerNameCountVectorizerCountVectorizer(ngram_range=(1, 2))simpleimputer['Age', 'Fare']SimpleImputerSimpleImputer(add_indicator=True)passthrough['Parch']passthroughpassthrough\n\n\nBy using fit_transform, we can see that this ColumnTransformer outputs 3671 feature columns. Again, all except 9 of those features were created from the Name column.\n\ngrid.best_estimator_.named_steps['columntransformer'].fit_transform(X)\n\n&lt;891x3671 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n    with 10191 stored elements in Compressed Sparse Row format&gt;\n\n\nThe cross-validated accuracy of this Pipeline is 0.828.\n\ngrid.best_score_\n\n0.828253091456908\n\n\n\n\n\n\n\n\nPipeline accuracy scores:\n\nGrid search (5 parameters): 0.828\nRandomized search (more C values): 0.827\nGrid search (2 parameters): 0.818\nBaseline (no tuning): 0.811\nNull model: 0.616\n\n\n\n\nFinally, let‚Äôs compare these two Pipelines to a Pipeline that doesn‚Äôt include the Name column at all. First, we‚Äôll create a ColumnTransformer called no_name_ct that excludes Name.\n\nno_name_ct = make_column_transformer(\n    (imp_ohe, ['Embarked', 'Sex']),\n    (imp, ['Age', 'Fare']),\n    ('passthrough', ['Parch']))\n\nAs you can see, this ColumnTransformer only outputs 9 feature columns.\n\nno_name_ct.fit_transform(X).shape\n\n(891, 9)\n\n\nThen, we‚Äôll add no_name_ct to a Pipeline called no_name_pipe and cross-validate it. The accuracy is 0.783, which is significantly lower than the Pipelines that included the Name column. To be fair, this Pipeline hasn‚Äôt been tuned, though honestly there is no hyperparameter tuning we could do to make it perform as well as the Pipelines that included the Name column.\n\nno_name_pipe = make_pipeline(no_name_ct, logreg)\ncross_val_score(no_name_pipe, X, y, cv=5, scoring='accuracy').mean()\n\n0.7833908731404181\n\n\n\n\n\n\n\n\nPipeline accuracy scores:\n\nGrid search (5 parameters): 0.828\nRandomized search (more C values): 0.827\nGrid search (2 parameters): 0.818\nBaseline (no tuning): 0.811\nBaseline excluding Name (no tuning): 0.783\nNull model: 0.616\n\n\n\n\nHere are some conclusions that we can draw from this experiment:\n\nFirst, including the Name column in the Pipeline significantly increased the cross-validated accuracy, which means that adding those thousands of feature columns did not result in overfitting. Instead, it tells us that the Name column contains more predictive signal than noise with respect to the target.\nMore generally, this experiment tells us that having more features than samples does not necessarily result in overfitting.\n\n\n\n\n\n\n\nWhat did we learn?\n\nName column contains more predictive signal than noise\nMore features than samples does not necessarily result in overfitting\n\n\n\n\nIt‚Äôs worth noting that there is additional tuning we could do to CountVectorizer to reduce the number of features it creates. However, there‚Äôs no way to know whether that would increase or decrease the Pipeline‚Äôs accuracy without actually trying it.",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Evaluating and tuning a Pipeline</span>"
    ]
  },
  {
    "objectID": "ch10.html#qa-how-do-i-examine-the-coefficients-of-a-pipeline",
    "href": "ch10.html#qa-how-do-i-examine-the-coefficients-of-a-pipeline",
    "title": "10¬† Evaluating and tuning a Pipeline",
    "section": "10.11 Q&A: How do I examine the coefficients of a Pipeline?",
    "text": "10.11 Q&A: How do I examine the coefficients of a Pipeline?\nRecall that once a grid search is complete, GridSearchCV automatically refits the Pipeline on X and y and stores it as an attribute called best_estimator_. Therefore, we can access the model coefficients by first selecting the logisticregression step and then selecting the coef_ attribute.\n\ngrid.best_estimator_.named_steps['logisticregression'].coef_\n\narray([[ 0.56431161,  0.        , -0.08767203, ...,  0.01408723,\n        -0.43713268, -0.46358519]])\n\n\nIdeally, we would also be able to get the names of the features that correspond to these coefficients by running the get_feature_names method on the ColumnTransformer step. However, get_feature_names only works if all of the underlying transformers have a get_feature_names method, and that is not the case here.\n\ngrid.best_estimator_.named_steps['columntransformer'].get_feature_names()\n\n\n\nAttributeError: Transformer pipeline does not provide get_feature_names\n\n\nInstead, as we saw previously in lesson¬†8.4, you would have to inspect the transformers one-by-one in order to determine the feature names.\n\ngrid.best_estimator_.named_steps['columntransformer'].transformers_\n\n[('pipeline',\n  Pipeline(steps=[('simpleimputer',\n                   SimpleImputer(fill_value='missing', strategy='constant')),\n                  ('onehotencoder', OneHotEncoder())]),\n  ['Embarked', 'Sex']),\n ('countvectorizer', CountVectorizer(ngram_range=(1, 2)), 'Name'),\n ('simpleimputer', SimpleImputer(add_indicator=True), ['Age', 'Fare']),\n ('passthrough', 'passthrough', ['Parch'])]\n\n\nNote that starting in scikit-learn version 1.1, the get_feature_names_out method should work on this ColumnTransformer, since the get_feature_names_out method will be available for all transformers.",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Evaluating and tuning a Pipeline</span>"
    ]
  },
  {
    "objectID": "ch10.html#sec-10-12",
    "href": "ch10.html#sec-10-12",
    "title": "10¬† Evaluating and tuning a Pipeline",
    "section": "10.12 Q&A: Should I split the dataset before tuning the Pipeline?",
    "text": "10.12 Q&A: Should I split the dataset before tuning the Pipeline?\nWhen we perform a grid search, we‚Äôre trying to find the parameters that maximize the cross-validation score on a dataset. Thus, we‚Äôre using the same data to accomplish two separate goals:\n\nFirst, to choose the best parameters for the Pipeline, which are stored in the best_params_ attribute.\nSecond, to estimate the future performance of the Pipelineon new data when using these parameters, which is stored in the best_score_ attribute.\n\n\n\n\n\n\n\nGoals of a grid search:\n\nChoose the best parameters for the Pipeline\nEstimate its performance on new data when using these parameters\n\n\n\n\n\ngrid.best_params_\n\n{'columntransformer__countvectorizer__ngram_range': (1, 2),\n 'columntransformer__pipeline__onehotencoder__drop': None,\n 'columntransformer__simpleimputer__add_indicator': True,\n 'logisticregression__C': 10,\n 'logisticregression__penalty': 'l1'}\n\n\n\ngrid.best_score_\n\n0.828253091456908\n\n\nUsing the same data for these two separate goals actually biases the Pipeline to this dataset and can result in overly optimistic scores.\nIf your main objective is to choose the best parameters, then this process is totally fine. You‚Äôll just have to accept that its actual performance on new data may be lower than the performance estimated by grid search.\nBut if you also need a realistic estimate of the Pipeline‚Äôs performance on new data, then there‚Äôs an alternative process you can use, which I‚Äôll walk you through in this lesson.\n\n\n\n\n\n\nIs it okay to use the same data for both goals?\n\nYes: If your main objective is to choose the best parameters\nNo: If you need a realistic estimate of performance on new data\n\n\n\n\nTo start, we‚Äôll import the train_test_split function from the model_selection module, and use it to split the data into training and testing sets, with 75% of the data as training and 25% of the data as testing. Note that I set the stratify parameter to y so that the class proportions will be approximately equal in the training and testing sets.\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,\n                                                    random_state=1,\n                                                    stratify=y)\n\nNext, we‚Äôll create a new GridSearchCV object called training_grid. When we run the grid search, we‚Äôll only pass it the training set so that the tuning process only takes the training set into account.\n\ntraining_grid = GridSearchCV(pipe, params, cv=5, scoring='accuracy',\n                             n_jobs=-1)\ntraining_grid.fit(X_train, y_train)\n\nGridSearchCVGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('columntransformer',\n                                        ColumnTransformer(transformers=[('pipeline',\n                                                                         Pipeline(steps=[('simpleimputer',\n                                                                                          SimpleImputer(fill_value='missing',\n                                                                                                        strategy='constant')),\n                                                                                         ('onehotencoder',\n                                                                                          OneHotEncoder())]),\n                                                                         ['Embarked',\n                                                                          'Sex']),\n                                                                        ('countvectorizer',\n                                                                         CountVectorizer(),\n                                                                         'Name'),\n                                                                        ('simpleimputer',\n                                                                         SimpleImputer(),\n                                                                         ['Age',\n                                                                          'Fare']),\n                                                                        (...\n                                        LogisticRegression(random_state=1,\n                                                           solver='liblinear'))]),\n             n_jobs=-1,\n             param_grid={'columntransformer__countvectorizer__ngram_range': [(1,\n                                                                              1),\n                                                                             (1,\n                                                                              2)],\n                         'columntransformer__pipeline__onehotencoder__drop': [None,\n                                                                              'first'],\n                         'columntransformer__simpleimputer__add_indicator': [False,\n                                                                             True],\n                         'logisticregression__C': [0.1, 1, 10],\n                         'logisticregression__penalty': ['l1', 'l2']},\n             scoring='accuracy')columntransformer: ColumnTransformerColumnTransformer(transformers=[('pipeline',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer(fill_value='missing',\n                                                                strategy='constant')),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder())]),\n                                 ['Embarked', 'Sex']),\n                                ('countvectorizer', CountVectorizer(), 'Name'),\n                                ('simpleimputer', SimpleImputer(),\n                                 ['Age', 'Fare']),\n                                ('passthrough', 'passthrough', ['Parch'])])pipeline['Embarked', 'Sex']SimpleImputerSimpleImputer(fill_value='missing', strategy='constant')OneHotEncoderOneHotEncoder()countvectorizerNameCountVectorizerCountVectorizer()simpleimputer['Age', 'Fare']SimpleImputerSimpleImputer()passthrough['Parch']passthroughpassthroughLogisticRegressionLogisticRegression(random_state=1, solver='liblinear')\n\n\nHere are the best parameters found by grid search on the training set.\n\ntraining_grid.best_params_\n\n{'columntransformer__countvectorizer__ngram_range': (1, 2),\n 'columntransformer__pipeline__onehotencoder__drop': 'first',\n 'columntransformer__simpleimputer__add_indicator': False,\n 'logisticregression__C': 10,\n 'logisticregression__penalty': 'l2'}\n\n\nWe‚Äôre not actually interested in the best score found during the grid search. Instead, we‚Äôre going to use the best parameters found by the grid search to make predictions for the testing set, and then evaluate the accuracy of those predictions. We can do this by passing the testing set to the training_grid‚Äôs score method.\nThe accuracy it outputs is 0.816, which is a more realistic estimate of how the Pipeline will perform on new data, since the testing set is brand new data that the Pipeline has never seen. However, it‚Äôs still just a single realization of this model, and so it‚Äôs impossible to know how precise this value is.\n\ntraining_grid.score(X_test, y_test)\n\n0.8161434977578476\n\n\n\n\n\n\n\n\nPipeline accuracy scores:\n\nGrid search (5 parameters): 0.828\nRandomized search (more C values): 0.827\nGrid search (2 parameters): 0.818\nGrid search (estimate for new data): 0.816\nBaseline (no tuning): 0.811\nBaseline excluding Name (no tuning): 0.783\nNull model: 0.616\n\n\n\n\nNow that we‚Äôve found the best parameters for the Pipeline and estimated its likely performance on new data, our final step is to actually make predictions on new data. Before making predictions, it‚Äôs critical that we train the Pipeline on all of our data, meaning the entirety of X and y, otherwise we‚Äôre throwing away valuable data.\nIn other words, we can‚Äôt simply use the training_grid‚Äôs predict method since it was only refit on X_train and y_train. Instead, we need to save the Pipeline with the best parameters, which we‚Äôll call best_pipe, and fit it to X and y.\n\nbest_pipe = training_grid.best_estimator_\nbest_pipe.fit(X, y)\n\nPipelinePipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('pipeline',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer(fill_value='missing',\n                                                                                 strategy='constant')),\n                                                                  ('onehotencoder',\n                                                                   OneHotEncoder(drop='first'))]),\n                                                  ['Embarked', 'Sex']),\n                                                 ('countvectorizer',\n                                                  CountVectorizer(ngram_range=(1,\n                                                                               2)),\n                                                  'Name'),\n                                                 ('simpleimputer',\n                                                  SimpleImputer(),\n                                                  ['Age', 'Fare']),\n                                                 ('passthrough', 'passthrough',\n                                                  ['Parch'])])),\n                ('logisticregression',\n                 LogisticRegression(C=10, random_state=1, solver='liblinear'))])columntransformer: ColumnTransformerColumnTransformer(transformers=[('pipeline',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer(fill_value='missing',\n                                                                strategy='constant')),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder(drop='first'))]),\n                                 ['Embarked', 'Sex']),\n                                ('countvectorizer',\n                                 CountVectorizer(ngram_range=(1, 2)), 'Name'),\n                                ('simpleimputer', SimpleImputer(),\n                                 ['Age', 'Fare']),\n                                ('passthrough', 'passthrough', ['Parch'])])pipeline['Embarked', 'Sex']SimpleImputerSimpleImputer(fill_value='missing', strategy='constant')OneHotEncoderOneHotEncoder(drop='first')countvectorizerNameCountVectorizerCountVectorizer(ngram_range=(1, 2))simpleimputer['Age', 'Fare']SimpleImputerSimpleImputer()passthrough['Parch']passthroughpassthroughLogisticRegressionLogisticRegression(C=10, random_state=1, solver='liblinear')\n\n\nNow we can make predictions on new data.\n\nbest_pipe.predict(X_new)\n\narray([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,\n       1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n       1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,\n       1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,\n       1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n       0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n       1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n       1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,\n       0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,\n       1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n       1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n       0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n       0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,\n       1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,\n       0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n       1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n       0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1])\n\n\nIf you decide that you‚Äôre going to follow the process that I‚Äôve just outlined, then there are two guidelines that are important to follow:\nFirst, you should only use the testing set for evaluating Pipeline performance one time. If you keep tuning the Pipeline again and again, each time checking its performance on the testing set, you‚Äôre essentially tuning the Pipeline to the particulars of the testing set. At that point, it no longer functions as an independent data source and thus its performance estimates will become less reliable.\nSecond, it‚Äôs important that you have enough data overall in order for the training and testing sets to both be sufficiently large once the dataset has been split:\n\nIf the training set is too small, then the grid search won‚Äôt have enough data to find the optimal tuning parameters.\nIf the testing set is too small, then it won‚Äôt be able to provide a reliable estimate of Pipeline performance.\n\nBoth of these situations would defeat the purpose of splitting the dataset, and thus this approach is best when you have a large enough dataset. Unfortunately, it‚Äôs difficult to say in the abstract how much data is ‚Äúenough‚Äù, since that depends on the particulars of the dataset and the problem.\n\n\n\n\n\n\nGuidelines for using this process:\n\nOnly use the testing set once:\n\nIf used multiple times, performance estimates will become less reliable\n\nYou must have enough data:\n\nIf training set is too small, grid search won‚Äôt find the optimal parameters\nIf testing set is too small, it won‚Äôt provide a reliable performance estimate",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Evaluating and tuning a Pipeline</span>"
    ]
  },
  {
    "objectID": "ch10.html#qa-what-is-regularization",
    "href": "ch10.html#qa-what-is-regularization",
    "title": "10¬† Evaluating and tuning a Pipeline",
    "section": "10.13 Q&A: What is regularization?",
    "text": "10.13 Q&A: What is regularization?\nEarlier in this chapter, we tuned the regularization parameters of logistic regression. In this lesson, I‚Äôll briefly explain what regularization actually is.\nRegularization is a process that constrains the size of a model‚Äôs coefficients in order to minimize overfitting. Overfitting is when your model fits too closely to patterns in the training data, which causes your model not to perform well when it makes predictions on new data.\nRegularization minimizes overfitting by reducing the variance of the model. Thus if you believe a model is too complex, regularization will reduce the error due to variance more than it increases the error due to bias, resulting in a model that is more likely to generalize to new data.\nIn simpler terms, regularization makes your model a bit less flexible so that it‚Äôs more likely to follow the true patterns in the data and less likely to follow the noise. Regularization is especially useful when you have outliers in the training data, because regularization decreases the influence that outliers have on the model.\n\n\n\n\n\n\nBrief explanation of regularization:\n\nConstrains the size of model coefficients to minimize overfitting\nReduces the variance of an overly complex model to help the model generalize\nDecreases model flexibility so that it follows the true patterns in the data",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Evaluating and tuning a Pipeline</span>"
    ]
  },
  {
    "objectID": "ch11.html",
    "href": "ch11.html",
    "title": "11¬† Comparing linear and non-linear models",
    "section": "",
    "text": "11.1 Trying a random forest model\nSo far, the only model we‚Äôve used in this book is logistic regression. But what if you wanted to try a different model?\nOne great thing about the scikit-learn API is that once you‚Äôve built a workflow, you can easily swap in a different model, usually without making any other changes to your workflow. This is a huge benefit of scikit-learn, since it‚Äôs not possible to know ahead of time which model is going to work best for a given problem and dataset. This is also known as the ‚Äúno free lunch‚Äù theorem.\nIn this chapter, we‚Äôre going to try out the random forest model, which is one of the most well-known models in Machine Learning. Whereas logistic regression is a linear model, random forests is a non-linear model based on decision trees. These two types of models have different overall properties, thus it may turn out that one type is better suited to this particular problem.\nWe start out by importing the RandomForestClassifier class from the ensemble module, and creating an instance called rf. Because there‚Äôs randomness involved in a random forest, we‚Äôll set random_state for reproducibility. And because building a random forest can be computationally expensive, it has its own n_jobs parameter (just like grid search and randomized search), which we‚Äôll set to -1 to enable parallel processing.\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(random_state=1, n_jobs=-1)\nWe‚Äôll create a new Pipeline object called rf_pipe that uses random forests instead of logistic regression.\nrf_pipe = make_pipeline(ct, rf)\nrf_pipe\n\nPipelinePipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('pipeline',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer(fill_value='missing',\n                                                                                 strategy='constant')),\n                                                                  ('onehotencoder',\n                                                                   OneHotEncoder())]),\n                                                  ['Embarked', 'Sex']),\n                                                 ('countvectorizer',\n                                                  CountVectorizer(), 'Name'),\n                                                 ('simpleimputer',\n                                                  SimpleImputer(),\n                                                  ['Age', 'Fare']),\n                                                 ('passthrough', 'passthrough',\n                                                  ['Parch'])])),\n                ('randomforestclassifier',\n                 RandomForestClassifier(n_jobs=-1, random_state=1))])columntransformer: ColumnTransformerColumnTransformer(transformers=[('pipeline',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer(fill_value='missing',\n                                                                strategy='constant')),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder())]),\n                                 ['Embarked', 'Sex']),\n                                ('countvectorizer', CountVectorizer(), 'Name'),\n                                ('simpleimputer', SimpleImputer(),\n                                 ['Age', 'Fare']),\n                                ('passthrough', 'passthrough', ['Parch'])])pipeline['Embarked', 'Sex']SimpleImputerSimpleImputer(fill_value='missing', strategy='constant')OneHotEncoderOneHotEncoder()countvectorizerNameCountVectorizerCountVectorizer()simpleimputer['Age', 'Fare']SimpleImputerSimpleImputer()passthrough['Parch']passthroughpassthroughRandomForestClassifierRandomForestClassifier(n_jobs=-1, random_state=1)\nAnd we can cross-validate it to generate a baseline accuracy, which is 0.811. This accuracy is nearly identical to the baseline accuracy of our logistic regression Pipeline, but it‚Äôs likely that we can improve it through hyperparameter tuning.\ncross_val_score(rf_pipe, X, y, cv=5, scoring='accuracy').mean()\n\n0.811436821291821\nAs an aside, I‚Äôve simplified the Pipeline accuracy scores table to only include the most important scores from the previous chapter. As you might guess, ‚ÄúLR‚Äù stands for logistic regression and ‚ÄúRF‚Äù stands for random forests. And going forward, I‚Äôll always use the term ‚Äúbaseline‚Äù in this table to describe a Pipeline that has not undergone any hyperparameter tuning via grid search or randomized search.",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Comparing linear and non-linear models</span>"
    ]
  },
  {
    "objectID": "ch11.html#trying-a-random-forest-model",
    "href": "ch11.html#trying-a-random-forest-model",
    "title": "11¬† Comparing linear and non-linear models",
    "section": "",
    "text": "Random forest model:\n\nNon-linear model\nBased on decision trees\nDifferent properties from logistic regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPipeline accuracy scores:\n\nGrid search (LR): 0.828\nBaseline (LR): 0.811\nBaseline (RF): 0.811",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Comparing linear and non-linear models</span>"
    ]
  },
  {
    "objectID": "ch11.html#tuning-random-forests-with-randomized-search",
    "href": "ch11.html#tuning-random-forests-with-randomized-search",
    "title": "11¬† Comparing linear and non-linear models",
    "section": "11.2 Tuning random forests with randomized search",
    "text": "11.2 Tuning random forests with randomized search\nWhen tuning random forests, we‚Äôll try tuning the same parameters for the transformers as before, but different parameters for the model. It‚Äôs still important to tune the transformations and the model at the same time, because it may turn out that the best data transformations for a random forest model are different than the best data transformations for a logistic regression model.\nRather than typing a parameters dictionary from scratch, we can start by creating a copy of the params dictionary called rf_params.\n\nrf_params = params.copy()\nrf_params\n\n{'logisticregression__penalty': ['l1', 'l2'],\n 'logisticregression__C': [0.1, 1, 10],\n 'columntransformer__pipeline__onehotencoder__drop': [None, 'first'],\n 'columntransformer__countvectorizer__ngram_range': [(1, 1), (1, 2)],\n 'columntransformer__simpleimputer__add_indicator': [False, True]}\n\n\nThen, we‚Äôll delete the entries from the rf_params dictionary that only apply to logistic regression.\n\ndel rf_params['logisticregression__penalty']\ndel rf_params['logisticregression__C']\nrf_params\n\n{'columntransformer__pipeline__onehotencoder__drop': [None, 'first'],\n 'columntransformer__countvectorizer__ngram_range': [(1, 1), (1, 2)],\n 'columntransformer__simpleimputer__add_indicator': [False, True]}\n\n\nAlternatively, we could have created the rf_params dictionary using a dictionary comprehension that only keeps the entries in params that start with the letters ‚Äúcol‚Äù.\n\nrf_params = {k:v for k, v in params.items() if k.startswith('col')}\nrf_params\n\n{'columntransformer__pipeline__onehotencoder__drop': [None, 'first'],\n 'columntransformer__countvectorizer__ngram_range': [(1, 1), (1, 2)],\n 'columntransformer__simpleimputer__add_indicator': [False, True]}\n\n\nNow we‚Äôre ready to add tuning parameters for the RandomForestClassifier. Random forests has a lot of parameters you can tune, which can make for a computationally expensive grid search if you try to tune all of them. This is compounded by the fact that random forests is comparatively slower to train than logistic regression.\nWhen you‚Äôre not quite sure which parameters to tune or which values to try for those parameters, I would suggest a two-step approach, which is what we‚Äôll use in this chapter:\n\nFirst, use a randomized search with a variety of parameters and values. This allows you to test out a lot of different combinations while still controlling the computational budget. Examine the results of the search to look for trends of what‚Äôs working and what‚Äôs not.\nSecond, use a grid search with a more optimized set of parameters and values, based on what you learned from the randomized search.\n\n\n\n\n\n\n\nTwo-step approach to hyperparameter tuning:\n\nRandomized search: Test a variety of parameters and values, then examine the results for trends\nGrid search: Use an optimized set of parameters and values based on what you learned from step 1\n\n\n\n\nWe‚Äôll start by trying out four parameters from the RandomForestClassifier, which I selected based on research and experience.\nFirst, we‚Äôll confirm that the Pipeline step name is randomforestclassifier (all lowercase).\n\nrf_pipe.named_steps.keys()\n\ndict_keys(['columntransformer', 'randomforestclassifier'])\n\n\nThen we‚Äôll add the four parameters I selected and some reasonable values for those parameters to the rf_params dictionary:\n\nn_estimators is the number of decision trees in the random forest.\nmin_samples_leaf is a way to control overfitting, just like regularization is used to control overfitting in a logistic regression model.\nmax_features and bootstrap affect certain properties of the random forest algorithm.\n\n\n\n\n\n\n\nRandomForestClassifier tuning parameters:\n\nn_estimators: Number of decisions trees in the forest\n\n100 (default)\n300\n500\n700\n\nmin_samples_leaf: Minimum number of samples at a leaf node\n\n1 (default)\n2\n3\n\nmax_features: Number of features to consider when choosing a split\n\n'sqrt' (default)\nNone\n\nbootstrap: Whether bootstrap samples are used when building trees\n\nTrue (default)\nFalse\n\n\n\n\n\n\nrf_params['randomforestclassifier__n_estimators'] = [100, 300, 500, 700]\nrf_params['randomforestclassifier__min_samples_leaf'] = [1, 2, 3]\nrf_params['randomforestclassifier__max_features'] = ['sqrt', None]\nrf_params['randomforestclassifier__bootstrap'] = [True, False]\nrf_params\n\n{'columntransformer__pipeline__onehotencoder__drop': [None, 'first'],\n 'columntransformer__countvectorizer__ngram_range': [(1, 1), (1, 2)],\n 'columntransformer__simpleimputer__add_indicator': [False, True],\n 'randomforestclassifier__n_estimators': [100, 300, 500, 700],\n 'randomforestclassifier__min_samples_leaf': [1, 2, 3],\n 'randomforestclassifier__max_features': ['sqrt', None],\n 'randomforestclassifier__bootstrap': [True, False]}\n\n\nFinally, we‚Äôll create an instance of RandomizedSearchCV called rf_rand, making sure to use the rf_pipe and rf_params objects, and we‚Äôll run 100 iterations of the randomized search.\nNotice that I‚Äôve added a warning to this cell to indicate that it takes a few minutes to run on my local machine, though it may run faster or slower on your machine.\n\n# WARNING: EXTENDED RUNTIME\nrf_rand = RandomizedSearchCV(rf_pipe, rf_params, cv=5,\n                             scoring='accuracy', n_iter=100,\n                             random_state=1, n_jobs=-1)\n%time rf_rand.fit(X, y)\n\nCPU times: user 2.87 s, sys: 346 ms, total: 3.22 s\nWall time: 1min 35s\n\n\nRandomizedSearchCVRandomizedSearchCV(cv=5,\n                   estimator=Pipeline(steps=[('columntransformer',\n                                              ColumnTransformer(transformers=[('pipeline',\n                                                                               Pipeline(steps=[('simpleimputer',\n                                                                                                SimpleImputer(fill_value='missing',\n                                                                                                              strategy='constant')),\n                                                                                               ('onehotencoder',\n                                                                                                OneHotEncoder())]),\n                                                                               ['Embarked',\n                                                                                'Sex']),\n                                                                              ('countvectorizer',\n                                                                               CountVectorizer(),\n                                                                               'Name'),\n                                                                              ('simpleimputer',\n                                                                               SimpleImputer(),\n                                                                               ['Age',\n                                                                                'Far...\n                                        'columntransformer__pipeline__onehotencoder__drop': [None,\n                                                                                             'first'],\n                                        'columntransformer__simpleimputer__add_indicator': [False,\n                                                                                            True],\n                                        'randomforestclassifier__bootstrap': [True,\n                                                                              False],\n                                        'randomforestclassifier__max_features': ['sqrt',\n                                                                                 None],\n                                        'randomforestclassifier__min_samples_leaf': [1,\n                                                                                     2,\n                                                                                     3],\n                                        'randomforestclassifier__n_estimators': [100,\n                                                                                 300,\n                                                                                 500,\n                                                                                 700]},\n                   random_state=1, scoring='accuracy')columntransformer: ColumnTransformerColumnTransformer(transformers=[('pipeline',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer(fill_value='missing',\n                                                                strategy='constant')),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder())]),\n                                 ['Embarked', 'Sex']),\n                                ('countvectorizer', CountVectorizer(), 'Name'),\n                                ('simpleimputer', SimpleImputer(),\n                                 ['Age', 'Fare']),\n                                ('passthrough', 'passthrough', ['Parch'])])pipeline['Embarked', 'Sex']SimpleImputerSimpleImputer(fill_value='missing', strategy='constant')OneHotEncoderOneHotEncoder()countvectorizerNameCountVectorizerCountVectorizer()simpleimputer['Age', 'Fare']SimpleImputerSimpleImputer()passthrough['Parch']passthroughpassthroughRandomForestClassifierRandomForestClassifier(n_jobs=-1, random_state=1)\n\n\nWe see that the best score found during the randomized search is 0.825, which is better than our baseline score of 0.811, but not quite as good as the 0.828 score of our best logistic regression Pipeline.\n\nrf_rand.best_score_\n\n0.8249262444291003\n\n\n\n\n\n\n\n\nPipeline accuracy scores:\n\nGrid search (LR): 0.828\nRandomized search (RF): 0.825\nBaseline (LR): 0.811\nBaseline (RF): 0.811",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Comparing linear and non-linear models</span>"
    ]
  },
  {
    "objectID": "ch11.html#further-tuning-with-grid-search",
    "href": "ch11.html#further-tuning-with-grid-search",
    "title": "11¬† Comparing linear and non-linear models",
    "section": "11.3 Further tuning with grid search",
    "text": "11.3 Further tuning with grid search\nLet‚Äôs now examine the results from the randomized search to look for trends, specifically focusing on the top 20 results. We‚Äôll convert the results to a DataFrame, filter to only keep the columns we need, rename the parameter columns, and then sort by rank_test_score. Keep in mind that it‚Äôs hard to draw any definitive conclusions since this is a randomized search, so what we‚Äôre looking for is just any obvious trends.\n\nresults = (pd.DataFrame(rf_rand.cv_results_)\n           .filter(regex='param_|mean_test|rank'))\nresults.columns = results.columns.str.split('__').str[-1]\nresults.sort_values('rank_test_score').head(20)\n\n\n\n\n\n\n\n\nn_estimators\nmin_samples_leaf\nmax_features\nbootstrap\nadd_indicator\ndrop\nngram_range\nmean_test_score\nrank_test_score\n\n\n\n\n13\n700\n3\nNone\nTrue\nFalse\nfirst\n(1, 1)\n0.824926\n1\n\n\n70\n700\n3\nNone\nTrue\nFalse\nNone\n(1, 1)\n0.822685\n2\n\n\n31\n500\n3\nNone\nTrue\nTrue\nfirst\n(1, 2)\n0.822685\n2\n\n\n45\n300\n2\nNone\nTrue\nFalse\nNone\n(1, 1)\n0.822679\n4\n\n\n33\n300\n3\nNone\nTrue\nFalse\nfirst\n(1, 1)\n0.821562\n5\n\n\n54\n300\n2\nNone\nTrue\nTrue\nNone\n(1, 1)\n0.821549\n6\n\n\n68\n500\n3\nNone\nTrue\nFalse\nNone\n(1, 2)\n0.820444\n7\n\n\n81\n100\n3\nNone\nTrue\nTrue\nfirst\n(1, 2)\n0.820432\n8\n\n\n15\n300\n2\nNone\nTrue\nTrue\nfirst\n(1, 1)\n0.819315\n9\n\n\n94\n500\n2\nNone\nTrue\nTrue\nfirst\n(1, 1)\n0.819308\n10\n\n\n98\n100\n2\nNone\nTrue\nTrue\nfirst\n(1, 1)\n0.819308\n10\n\n\n63\n700\n1\nNone\nTrue\nTrue\nfirst\n(1, 1)\n0.819283\n12\n\n\n18\n700\n2\nNone\nTrue\nTrue\nfirst\n(1, 1)\n0.818191\n13\n\n\n57\n500\n2\nNone\nTrue\nFalse\nNone\n(1, 1)\n0.818185\n14\n\n\n12\n100\n3\nNone\nTrue\nTrue\nNone\n(1, 2)\n0.818185\n14\n\n\n72\n300\n1\nNone\nTrue\nFalse\nNone\n(1, 1)\n0.818178\n16\n\n\n2\n500\n1\nsqrt\nFalse\nFalse\nfirst\n(1, 1)\n0.818160\n17\n\n\n41\n700\n2\nNone\nTrue\nTrue\nNone\n(1, 1)\n0.817067\n18\n\n\n10\n500\n2\nNone\nTrue\nFalse\nfirst\n(1, 2)\n0.817061\n19\n\n\n8\n500\n2\nNone\nTrue\nTrue\nNone\n(1, 2)\n0.817061\n19\n\n\n\n\n\n\n\nStarting with n_estimators, we see that higher numbers are performing better, which is typical for n_estimators. It seems unlikely that 100 will produce the best result, so we‚Äôll exclude that value from our grid search. And since the current best result is at 700, it seems useful to add a value of 900 to our grid search, in case increasing it further is even better.\nDo keep in mind that increasing n_estimators also increases the time needed to train the model. You could consider just setting a single large value for n_estimators rather than searching through multiple values, since larger values will generally produce better results up to a certain point, but I prefer to tune this value when computational resources allow for it.\nThe next parameter to examine is min_samples_leaf. Similar to n_estimators, the lowest value of 1 seems unlikely to produce the best result, so we‚Äôll remove it. The current best result is 3, so we‚Äôll also try the values 4 and 5 in the grid search.\nFor max_features, it‚Äôs clear that None is performing better, so we‚Äôre no longer going to try 'sqrt'.\nFor bootstrap, it‚Äôs clear that True is performing better, so we‚Äôre no longer going to try False.\nAnd finally, there aren‚Äôt any clear trends for the transformer parameters, so we‚Äôll leave those as-is.\n\n\n\n\n\n\nTrends in the randomized search results:\n\nn_estimators:\n\nHigher numbers are performing better\nRemove 100, add 900\n\nmin_samples_leaf:\n\nHigher numbers are performing better\nRemove 1, add 4 and 5\n\nmax_features:\n\nNone is performing better\nRemove 'sqrt'\n\nbootstrap:\n\nTrue is performing better\nRemove False\n\nTransformer parameters:\n\nNo clear trends\nLeave as-is\n\n\n\n\n\nHere are the updated values we‚Äôre going to try. For max_features and bootstrap, you‚Äôll see that we can just pass a list with a single value so that that parameter value will always get set during the search.\n\nrf_params['randomforestclassifier__n_estimators'] = [300, 500, 700, 900]\nrf_params['randomforestclassifier__min_samples_leaf'] = [2, 3, 4, 5]\nrf_params['randomforestclassifier__max_features'] = [None]\nrf_params['randomforestclassifier__bootstrap'] = [True]\nrf_params\n\n{'columntransformer__pipeline__onehotencoder__drop': [None, 'first'],\n 'columntransformer__countvectorizer__ngram_range': [(1, 1), (1, 2)],\n 'columntransformer__simpleimputer__add_indicator': [False, True],\n 'randomforestclassifier__n_estimators': [300, 500, 700, 900],\n 'randomforestclassifier__min_samples_leaf': [2, 3, 4, 5],\n 'randomforestclassifier__max_features': [None],\n 'randomforestclassifier__bootstrap': [True]}\n\n\nAt this point, you could continue to run additional randomized searches in order to study the trends further, but we‚Äôre just going to move on to grid search.\nWe‚Äôll create an instance of GridSearchCV called rf_grid, making sure to use the rf_pipe and rf_params objects, and then run the search.\n\n# WARNING: EXTENDED RUNTIME\nrf_grid = GridSearchCV(rf_pipe, rf_params, cv=5, scoring='accuracy',\n                       n_jobs=-1)\n%time rf_grid.fit(X, y)\n\nCPU times: user 2.15 s, sys: 230 ms, total: 2.38 s\nWall time: 3min 5s\n\n\nGridSearchCVGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('columntransformer',\n                                        ColumnTransformer(transformers=[('pipeline',\n                                                                         Pipeline(steps=[('simpleimputer',\n                                                                                          SimpleImputer(fill_value='missing',\n                                                                                                        strategy='constant')),\n                                                                                         ('onehotencoder',\n                                                                                          OneHotEncoder())]),\n                                                                         ['Embarked',\n                                                                          'Sex']),\n                                                                        ('countvectorizer',\n                                                                         CountVectorizer(),\n                                                                         'Name'),\n                                                                        ('simpleimputer',\n                                                                         SimpleImputer(),\n                                                                         ['Age',\n                                                                          'Fare']),\n                                                                        (...\n                                                                              2)],\n                         'columntransformer__pipeline__onehotencoder__drop': [None,\n                                                                              'first'],\n                         'columntransformer__simpleimputer__add_indicator': [False,\n                                                                             True],\n                         'randomforestclassifier__bootstrap': [True],\n                         'randomforestclassifier__max_features': [None],\n                         'randomforestclassifier__min_samples_leaf': [2, 3, 4,\n                                                                      5],\n                         'randomforestclassifier__n_estimators': [300, 500, 700,\n                                                                  900]},\n             scoring='accuracy')columntransformer: ColumnTransformerColumnTransformer(transformers=[('pipeline',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer(fill_value='missing',\n                                                                strategy='constant')),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder())]),\n                                 ['Embarked', 'Sex']),\n                                ('countvectorizer', CountVectorizer(), 'Name'),\n                                ('simpleimputer', SimpleImputer(),\n                                 ['Age', 'Fare']),\n                                ('passthrough', 'passthrough', ['Parch'])])pipeline['Embarked', 'Sex']SimpleImputerSimpleImputer(fill_value='missing', strategy='constant')OneHotEncoderOneHotEncoder()countvectorizerNameCountVectorizerCountVectorizer()simpleimputer['Age', 'Fare']SimpleImputerSimpleImputer()passthrough['Parch']passthroughpassthroughRandomForestClassifierRandomForestClassifier(n_jobs=-1, random_state=1)\n\n\nThe best score from our grid search is 0.829, which is just a tiny bit higher than the 0.828 score of our best logistic regression Pipeline.\n\nrf_grid.best_score_\n\n0.8294143493817087\n\n\n\n\n\n\n\n\nPipeline accuracy scores:\n\nGrid search (RF): 0.829\nGrid search (LR): 0.828\nRandomized search (RF): 0.825\nBaseline (LR): 0.811\nBaseline (RF): 0.811\n\n\n\n\nThese are the best parameters that were found during the grid search. Again, it‚Äôs hard to say whether this is truly the best set of parameters, but we at least know that it‚Äôs a good set of parameters.\n\nrf_grid.best_params_\n\n{'columntransformer__countvectorizer__ngram_range': (1, 1),\n 'columntransformer__pipeline__onehotencoder__drop': 'first',\n 'columntransformer__simpleimputer__add_indicator': True,\n 'randomforestclassifier__bootstrap': True,\n 'randomforestclassifier__max_features': None,\n 'randomforestclassifier__min_samples_leaf': 4,\n 'randomforestclassifier__n_estimators': 300}",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Comparing linear and non-linear models</span>"
    ]
  },
  {
    "objectID": "ch11.html#qa-how-do-i-tune-two-models-with-a-single-grid-search",
    "href": "ch11.html#qa-how-do-i-tune-two-models-with-a-single-grid-search",
    "title": "11¬† Comparing linear and non-linear models",
    "section": "11.4 Q&A: How do I tune two models with a single grid search?",
    "text": "11.4 Q&A: How do I tune two models with a single grid search?\nSo far, we‚Äôve set up two separate Pipelines called pipe and rf_pipe that each end in a different model. That made it easy to grid search each Pipeline with its own set of relevant parameters. However, you can actually tune two different models using a single grid search if you like.\nTo do this, the first step is to create a new Pipeline using the Pipeline class instead of the make_pipeline function. The reason we‚Äôre doing this is so that we can provide custom names for the steps. In this case, we‚Äôll call the Pipeline object both_pipe, and we‚Äôll call the step names 'preprocessor' and 'classifier'. We‚Äôll set the classifier to be logistic regression, though this is just a placeholder as you‚Äôll see in a minute.\n\nboth_pipe = Pipeline([('preprocessor', ct), ('classifier', logreg)])\n\nNext, we‚Äôll create a new parameter dictionary called params1. For the simplicity of this example, we‚Äôre only going to tune one parameter from the preprocessor step and two parameters from the classifier step.\nAdditionally, we‚Äôre going to add one more entry to the dictionary to indicate that the classifier we want to use with this parameter set is logistic regression. Notice that this is a logistic regression object, not a string, and also notice that we put it in brackets to make it a list. This might seem strange, but it will make more sense in a minute.\n\nparams1 = {}\nparams1['preprocessor__countvectorizer__ngram_range'] = [(1, 1), (1, 2)]\nparams1['classifier__penalty'] = ['l1', 'l2']\nparams1['classifier__C'] = [0.1, 1, 10]\nparams1['classifier'] = [logreg]\nparams1\n\n{'preprocessor__countvectorizer__ngram_range': [(1, 1), (1, 2)],\n 'classifier__penalty': ['l1', 'l2'],\n 'classifier__C': [0.1, 1, 10],\n 'classifier': [LogisticRegression(random_state=1, solver='liblinear')]}\n\n\nNext, we‚Äôll create another parameter dictionary called params2. Again, we‚Äôll tune one parameter from the preprocessor step and two parameters from the classifier step. You‚Äôll notice that the classifier parameters are random forest parameters, not logistic regression parameters.\nJust like above, we‚Äôll add one more entry to the dictionary to indicate that the classifier we want to use with this parameter set is random forests. During the grid search, this will override the logistic regression classifier we specified when creating the Pipeline.\n\nparams2 = {}\nparams2['preprocessor__countvectorizer__ngram_range'] = [(1, 1), (1, 2)]\nparams2['classifier__n_estimators'] = [300, 500]\nparams2['classifier__min_samples_leaf'] = [3, 4]\nparams2['classifier'] = [rf]\nparams2\n\n{'preprocessor__countvectorizer__ngram_range': [(1, 1), (1, 2)],\n 'classifier__n_estimators': [300, 500],\n 'classifier__min_samples_leaf': [3, 4],\n 'classifier': [RandomForestClassifier(n_jobs=-1, random_state=1)]}\n\n\nNext, we‚Äôll create a list called both_params that includes both of these parameter sets.\n\nboth_params = [params1, params2]\nboth_params\n\n[{'preprocessor__countvectorizer__ngram_range': [(1, 1), (1, 2)],\n  'classifier__penalty': ['l1', 'l2'],\n  'classifier__C': [0.1, 1, 10],\n  'classifier': [LogisticRegression(random_state=1, solver='liblinear')]},\n {'preprocessor__countvectorizer__ngram_range': [(1, 1), (1, 2)],\n  'classifier__n_estimators': [300, 500],\n  'classifier__min_samples_leaf': [3, 4],\n  'classifier': [RandomForestClassifier(n_jobs=-1, random_state=1)]}]\n\n\nFinally, we‚Äôll create an instance of GridSearchCV called both_grid, making sure to pass it the both_pipe and both_params objects.\nWhen we run the search, here‚Äôs what will happen:\n\nFirst, it will try every combination of parameters from params1, which is 2 times 2 times 3 times 1, or 12 combinations.\nThen, it will try every combination of parameters from params2, which is 2 times 2 times 2 times 1, or 8 combinations.\n\nThus, it will run a total of 20 times.\n\nboth_grid = GridSearchCV(both_pipe, both_params, cv=5,\n                         scoring='accuracy', n_jobs=-1)\nboth_grid.fit(X, y)\n\nGridSearchCVGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('preprocessor',\n                                        ColumnTransformer(transformers=[('pipeline',\n                                                                         Pipeline(steps=[('simpleimputer',\n                                                                                          SimpleImputer(fill_value='missing',\n                                                                                                        strategy='constant')),\n                                                                                         ('onehotencoder',\n                                                                                          OneHotEncoder())]),\n                                                                         ['Embarked',\n                                                                          'Sex']),\n                                                                        ('countvectorizer',\n                                                                         CountVectorizer(),\n                                                                         'Name'),\n                                                                        ('simpleimputer',\n                                                                         SimpleImputer(),\n                                                                         ['Age',\n                                                                          'Fare']),\n                                                                        ('pass...\n                                                            solver='liblinear')],\n                          'classifier__C': [0.1, 1, 10],\n                          'classifier__penalty': ['l1', 'l2'],\n                          'preprocessor__countvectorizer__ngram_range': [(1, 1),\n                                                                         (1,\n                                                                          2)]},\n                         {'classifier': [RandomForestClassifier(n_jobs=-1,\n                                                                random_state=1)],\n                          'classifier__min_samples_leaf': [3, 4],\n                          'classifier__n_estimators': [300, 500],\n                          'preprocessor__countvectorizer__ngram_range': [(1, 1),\n                                                                         (1,\n                                                                          2)]}],\n             scoring='accuracy')preprocessor: ColumnTransformerColumnTransformer(transformers=[('pipeline',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer(fill_value='missing',\n                                                                strategy='constant')),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder())]),\n                                 ['Embarked', 'Sex']),\n                                ('countvectorizer', CountVectorizer(), 'Name'),\n                                ('simpleimputer', SimpleImputer(),\n                                 ['Age', 'Fare']),\n                                ('passthrough', 'passthrough', ['Parch'])])pipeline['Embarked', 'Sex']SimpleImputerSimpleImputer(fill_value='missing', strategy='constant')OneHotEncoderOneHotEncoder()countvectorizerNameCountVectorizerCountVectorizer()simpleimputer['Age', 'Fare']SimpleImputerSimpleImputer()passthrough['Parch']passthroughpassthroughLogisticRegressionLogisticRegression(C=10, penalty='l1', random_state=1, solver='liblinear')\n\n\nLet‚Äôs take a look at the results. You can see that it ran 12 times with a logistic regression model and 8 times with a random forest model. Also note that when logistic regression model runs, the random forest-related parameters are listed as NaN, and vice versa when the random forest model runs.\n\nresults = (pd.DataFrame(both_grid.cv_results_)\n           .filter(regex='param_.+_|mean_test|rank'))\nresults.columns = results.columns.str.split('__').str[-1]\nresults\n\n\n\n\n\n\n\n\nC\npenalty\nngram_range\nmin_samples_leaf\nn_estimators\nmean_test_score\nrank_test_score\n\n\n\n\n0\n0.1\nl1\n(1, 1)\nNaN\nNaN\n0.783385\n12\n\n\n1\n0.1\nl1\n(1, 2)\nNaN\nNaN\n0.783385\n12\n\n\n2\n0.1\nl2\n(1, 1)\nNaN\nNaN\n0.788990\n10\n\n\n3\n0.1\nl2\n(1, 2)\nNaN\nNaN\n0.788996\n9\n\n\n4\n1\nl1\n(1, 1)\nNaN\nNaN\n0.814814\n3\n\n\n5\n1\nl1\n(1, 2)\nNaN\nNaN\n0.812567\n4\n\n\n6\n1\nl2\n(1, 1)\nNaN\nNaN\n0.811462\n5\n\n\n7\n1\nl2\n(1, 2)\nNaN\nNaN\n0.805844\n8\n\n\n8\n10\nl1\n(1, 1)\nNaN\nNaN\n0.818166\n2\n\n\n9\n10\nl1\n(1, 2)\nNaN\nNaN\n0.824889\n1\n\n\n10\n10\nl2\n(1, 1)\nNaN\nNaN\n0.809234\n6\n\n\n11\n10\nl2\n(1, 2)\nNaN\nNaN\n0.808104\n7\n\n\n12\nNaN\nNaN\n(1, 1)\n3\n300\n0.783378\n14\n\n\n13\nNaN\nNaN\n(1, 2)\n3\n300\n0.723859\n16\n\n\n14\nNaN\nNaN\n(1, 1)\n3\n500\n0.784502\n11\n\n\n15\nNaN\nNaN\n(1, 2)\n3\n500\n0.738478\n15\n\n\n16\nNaN\nNaN\n(1, 1)\n4\n300\n0.701469\n17\n\n\n17\nNaN\nNaN\n(1, 2)\n4\n300\n0.616163\n19\n\n\n18\nNaN\nNaN\n(1, 1)\n4\n500\n0.686900\n18\n\n\n19\nNaN\nNaN\n(1, 2)\n4\n500\n0.616163\n19\n\n\n\n\n\n\n\nAs usual, the best_score_ and best_params_ attributes are still available.\n\nboth_grid.best_score_\n\n0.8248885820099178\n\n\n\nboth_grid.best_params_\n\n{'classifier': LogisticRegression(C=10, penalty='l1', random_state=1, solver='liblinear'),\n 'classifier__C': 10,\n 'classifier__penalty': 'l1',\n 'preprocessor__countvectorizer__ngram_range': (1, 2)}\n\n\nHere are two neat extensions to what we‚Äôve done in this lesson that you could try on your own:\nFirst, since the two models have separate parameter dictionaries, you could theoretically tune different preprocessing parameters for each model. For example, you could tune different CountVectorizer parameters for logistic regression and random forests.\nTaking it one step further, you could actually create two different preprocessor objects and tune them using the same grid search, just like we tuned two different models using the same grid search. That would allow you, for example, to use different encoders when preparing data for your logistic regression and random forest models.\n\n\n\n\n\n\nExtensions of this approach:\n\nTune different preprocessing parameters for each model\nTune two different preprocessor objects",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Comparing linear and non-linear models</span>"
    ]
  },
  {
    "objectID": "ch11.html#qa-how-do-i-tune-two-models-with-a-single-randomized-search",
    "href": "ch11.html#qa-how-do-i-tune-two-models-with-a-single-randomized-search",
    "title": "11¬† Comparing linear and non-linear models",
    "section": "11.5 Q&A: How do I tune two models with a single randomized search?",
    "text": "11.5 Q&A: How do I tune two models with a single randomized search?\nStarting in scikit-learn version 0.22, RandomizedSearchCV can search multiple parameter dictionaries. This allows you to do a randomized search of multiple models, in the same exact way that we did a grid search of multiple models.\nHere‚Äôs an example in which we pass both_pipe and both_params to RandomizedSearchCV and run it for 10 iterations.\n\nboth_rand = RandomizedSearchCV(both_pipe, both_params, cv=5,\n                               scoring='accuracy', n_iter=10,\n                               random_state=1, n_jobs=-1)\nboth_rand.fit(X, y)\n\nRandomizedSearchCVRandomizedSearchCV(cv=5,\n                   estimator=Pipeline(steps=[('preprocessor',\n                                              ColumnTransformer(transformers=[('pipeline',\n                                                                               Pipeline(steps=[('simpleimputer',\n                                                                                                SimpleImputer(fill_value='missing',\n                                                                                                              strategy='constant')),\n                                                                                               ('onehotencoder',\n                                                                                                OneHotEncoder())]),\n                                                                               ['Embarked',\n                                                                                'Sex']),\n                                                                              ('countvectorizer',\n                                                                               CountVectorizer(),\n                                                                               'Name'),\n                                                                              ('simpleimputer',\n                                                                               SimpleImputer(),\n                                                                               ['Age',\n                                                                                'Fare']),...\n                                         'classifier__C': [0.1, 1, 10],\n                                         'classifier__penalty': ['l1', 'l2'],\n                                         'preprocessor__countvectorizer__ngram_range': [(1,\n                                                                                         1),\n                                                                                        (1,\n                                                                                         2)]},\n                                        {'classifier': [RandomForestClassifier(n_jobs=-1,\n                                                                               random_state=1)],\n                                         'classifier__min_samples_leaf': [3, 4],\n                                         'classifier__n_estimators': [300, 500],\n                                         'preprocessor__countvectorizer__ngram_range': [(1,\n                                                                                         1),\n                                                                                        (1,\n                                                                                         2)]}],\n                   random_state=1, scoring='accuracy')preprocessor: ColumnTransformerColumnTransformer(transformers=[('pipeline',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer(fill_value='missing',\n                                                                strategy='constant')),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder())]),\n                                 ['Embarked', 'Sex']),\n                                ('countvectorizer', CountVectorizer(), 'Name'),\n                                ('simpleimputer', SimpleImputer(),\n                                 ['Age', 'Fare']),\n                                ('passthrough', 'passthrough', ['Parch'])])pipeline['Embarked', 'Sex']SimpleImputerSimpleImputer(fill_value='missing', strategy='constant')OneHotEncoderOneHotEncoder()countvectorizerNameCountVectorizerCountVectorizer()simpleimputer['Age', 'Fare']SimpleImputerSimpleImputer()passthrough['Parch']passthroughpassthroughLogisticRegressionLogisticRegression(C=1, penalty='l1', random_state=1, solver='liblinear')\n\n\nIf you examine the results, you‚Äôll find that logistic regression will be chosen more often because we defined more parameter combinations for the logistic regression model. This behavior may change in a future version, such that each model is equally likely to be chosen, regardless of the number of parameter combinations.\n\nresults = (pd.DataFrame(both_rand.cv_results_)\n           .filter(regex='param_.+_|mean_test|rank'))\nresults.columns = results.columns.str.split('__').str[-1]\nresults\n\n\n\n\n\n\n\n\nngram_range\npenalty\nC\nn_estimators\nmin_samples_leaf\nmean_test_score\nrank_test_score\n\n\n\n\n0\n(1, 2)\nl2\n0.1\nNaN\nNaN\n0.788996\n5\n\n\n1\n(1, 1)\nNaN\nNaN\n300\n4\n0.701469\n9\n\n\n2\n(1, 1)\nl2\n1\nNaN\nNaN\n0.811462\n2\n\n\n3\n(1, 1)\nl2\n10\nNaN\nNaN\n0.809234\n3\n\n\n4\n(1, 1)\nl2\n0.1\nNaN\nNaN\n0.788990\n6\n\n\n5\n(1, 1)\nNaN\nNaN\n500\n3\n0.784502\n7\n\n\n6\n(1, 1)\nl1\n1\nNaN\nNaN\n0.814814\n1\n\n\n7\n(1, 2)\nNaN\nNaN\n300\n4\n0.616163\n10\n\n\n8\n(1, 2)\nl2\n1\nNaN\nNaN\n0.805844\n4\n\n\n9\n(1, 2)\nl1\n0.1\nNaN\nNaN\n0.783385\n8",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Comparing linear and non-linear models</span>"
    ]
  },
  {
    "objectID": "ch12.html",
    "href": "ch12.html",
    "title": "12¬† Ensembling multiple models",
    "section": "",
    "text": "12.1 Introduction to ensembling\nSo far, we‚Äôve been trying to choose between two different models, namely logistic regression and random forests. However, you can actually use a process called ensembling to combine multiple models. The goal of ensembling is to produce a combined model, known as an ensemble, that is more accurate than any of the individual models.\nThe process for ensembling is simple:\nThe idea behind ensembling is that if you have a collection of individually imperfect models, the ‚Äúone-off‚Äù errors made by each model are probably not going to be made by the rest of the models. Thus, the errors will be discarded (or at least reduced) when ensembling the models. Another way of saying this is that ensembling produces better predictions because the ensemble has a lower variance than any of the individual models.\nIn this chapter, we‚Äôll ensemble our classification models two different ways, and then we‚Äôll tune the ensemble to try to achieve even better performance.",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Ensembling multiple models</span>"
    ]
  },
  {
    "objectID": "ch12.html#introduction-to-ensembling",
    "href": "ch12.html#introduction-to-ensembling",
    "title": "12¬† Ensembling multiple models",
    "section": "",
    "text": "For a regression problem, you calculate the average of the predictions made by the individual regressors and use that as your prediction.\nFor a classification problem, you can either average the predicted probabilities output by the classifiers, or you can let the classifiers vote on which class to predict. We‚Äôll see examples of this below.\n\n\n\n\n\n\n\nHow to create an ensemble:\n\nRegression: Average the predictions\nClassification: Average the predicted probabilities, or let the classifiers vote on the class\n\n\n\n\n\n\n\n\n\n\n\nWhy does ensembling work?\n\n‚ÄúOne-off‚Äù errors made by each model will be discarded when ensembling\nEnsemble has a lower variance than any individual model",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Ensembling multiple models</span>"
    ]
  },
  {
    "objectID": "ch12.html#ensembling-logistic-regression-and-random-forests",
    "href": "ch12.html#ensembling-logistic-regression-and-random-forests",
    "title": "12¬† Ensembling multiple models",
    "section": "12.2 Ensembling logistic regression and random forests",
    "text": "12.2 Ensembling logistic regression and random forests\nIn this lesson, we‚Äôre going to ensemble logistic regression and random forests. Because their predictions are generated using completely different processes, they‚Äôre likely to make different types of errors and thus they‚Äôre good candidates for ensembling.\nLet‚Äôs see a reminder of their cross-validation scores. Both the logistic regression Pipeline and the random forest Pipeline have an accuracy of 0.811, so our goal with ensembling is to increase this score.\n\nlogreg = LogisticRegression(solver='liblinear', random_state=1)\npipe = make_pipeline(ct, logreg)\ncross_val_score(pipe, X, y, cv=5, scoring='accuracy').mean()\n\n0.8114619295712762\n\n\n\nrf = RandomForestClassifier(random_state=1, n_jobs=-1)\nrf_pipe = make_pipeline(ct, rf)\ncross_val_score(rf_pipe, X, y, cv=5, scoring='accuracy').mean()\n\n0.811436821291821\n\n\n\n\n\n\n\n\nPipeline accuracy scores:\n\nGrid search (RF): 0.829\nGrid search (LR): 0.828\nBaseline (LR): 0.811\nBaseline (RF): 0.811\n\n\n\n\nWe‚Äôll create the ensemble using the VotingClassifier class, which we‚Äôll import from the ensemble module. We‚Äôll create an instance called vc and pass it a list of tuples, in which the first element of the tuple is a name and the second element is a classifier object.\nThe options for the voting parameter are 'soft', in which predicted probabilities are averaged, and 'hard', in which only class predictions are taken into account. We‚Äôll try soft voting first. Also, we‚Äôll set n_jobs to -1 to enable parallel processing.\n\nfrom sklearn.ensemble import VotingClassifier\nvc = VotingClassifier([('clf1', logreg), ('clf2', rf)], voting='soft',\n                      n_jobs=-1)\n\n\n\n\n\n\n\nVoting options for VotingClassifier:\n\n'soft': Average the predicted probabilities\n'hard': Majority vote using class predictions\n\n\n\n\nThen, we‚Äôll create a new Pipeline called vc_pipe in which the VotingClassifier is the second step instead of a model.\n\nvc_pipe = make_pipeline(ct, vc)\nvc_pipe\n\nPipelinePipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('pipeline',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer(fill_value='missing',\n                                                                                 strategy='constant')),\n                                                                  ('onehotencoder',\n                                                                   OneHotEncoder())]),\n                                                  ['Embarked', 'Sex']),\n                                                 ('countvectorizer',\n                                                  CountVectorizer(), 'Name'),\n                                                 ('simpleimputer',\n                                                  SimpleImputer(),\n                                                  ['Age', 'Fare']),\n                                                 ('passthrough', 'passthrough',\n                                                  ['Parch'])])),\n                ('votingclassifier',\n                 VotingClassifier(estimators=[('clf1',\n                                               LogisticRegression(random_state=1,\n                                                                  solver='liblinear')),\n                                              ('clf2',\n                                               RandomForestClassifier(n_jobs=-1,\n                                                                      random_state=1))],\n                                  n_jobs=-1, voting='soft'))])columntransformer: ColumnTransformerColumnTransformer(transformers=[('pipeline',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer(fill_value='missing',\n                                                                strategy='constant')),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder())]),\n                                 ['Embarked', 'Sex']),\n                                ('countvectorizer', CountVectorizer(), 'Name'),\n                                ('simpleimputer', SimpleImputer(),\n                                 ['Age', 'Fare']),\n                                ('passthrough', 'passthrough', ['Parch'])])pipeline['Embarked', 'Sex']SimpleImputerSimpleImputer(fill_value='missing', strategy='constant')OneHotEncoderOneHotEncoder()countvectorizerNameCountVectorizerCountVectorizer()simpleimputer['Age', 'Fare']SimpleImputerSimpleImputer()passthrough['Parch']passthroughpassthroughvotingclassifier: VotingClassifierVotingClassifier(estimators=[('clf1',\n                              LogisticRegression(random_state=1,\n                                                 solver='liblinear')),\n                             ('clf2',\n                              RandomForestClassifier(n_jobs=-1,\n                                                     random_state=1))],\n                 n_jobs=-1, voting='soft')clf1LogisticRegressionLogisticRegression(random_state=1, solver='liblinear')clf2RandomForestClassifierRandomForestClassifier(n_jobs=-1, random_state=1)",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Ensembling multiple models</span>"
    ]
  },
  {
    "objectID": "ch12.html#combining-predicted-probabilities",
    "href": "ch12.html#combining-predicted-probabilities",
    "title": "12¬† Ensembling multiple models",
    "section": "12.3 Combining predicted probabilities",
    "text": "12.3 Combining predicted probabilities\nLet‚Äôs examine how the VotingClassifier makes predictions when using soft voting. We‚Äôll do this by examining the predicted probabilities output by the logistic regression-based Pipeline and the random forest-based Pipeline for the first 3 samples in X_new.\nAs a reminder, the left column is the predicted probability of class 0 (for each sample), and the right column is the predicted probability of class 1.\n\npipe.fit(X, y)\npipe.predict_proba(X_new)[:3]\n\narray([[0.88916549, 0.11083451],\n       [0.14200691, 0.85799309],\n       [0.9190551 , 0.0809449 ]])\n\n\n\nrf_pipe.fit(X, y)\nrf_pipe.predict_proba(X_new)[:3]\n\narray([[0.99, 0.01],\n       [0.24, 0.76],\n       [0.97, 0.03]])\n\n\nNow let‚Äôs use the VotingClassifier to output predicted probabilities. It uses the fit and predict_proba methods, just like any other classifier.\nIf you examine its predicted probabilities, you will see that it‚Äôs simply averaging the two sets of probabilities from logistic regression and random forests. For example, the average of 0.14 and 0.24 is 0.19.\n\nvc_pipe.fit(X, y)\nvc_pipe.predict_proba(X_new)[:3]\n\narray([[0.93958275, 0.06041725],\n       [0.19100345, 0.80899655],\n       [0.94452755, 0.05547245]])\n\n\nIn order to make class predictions for X_new, you use the predict method, which simply chooses whichever class has the higher predicted probability. In this case, it predicted 0, 1, and 0, because those classes had the higher predicted probability.\n\nvc_pipe.predict(X_new)[:3]\n\narray([0, 1, 0])\n\n\nIn the 3 cases we just examined, logistic regression and random forests agreed on the class predictions. Let‚Äôs now examine a case in which the two models disagreed. One example of this is sample 80.\nAs you can see, logistic regression predicted class 0 but without much confidence.\n\npipe.predict_proba(X_new)[80]\n\narray([0.51799634, 0.48200366])\n\n\nRandom forests predicted class 1 with more confidence.\n\nrf_pipe.predict_proba(X_new)[80]\n\narray([0.29, 0.71])\n\n\nWhen VotingClassifier averages the predicted probabilities for this sample, the class 1 value is higher, thus it will predict class 1.\n\nvc_pipe.predict_proba(X_new)[80]\n\narray([0.40399817, 0.59600183])\n\n\nLet‚Äôs move on to cross-validation to see how the VotingClassifier Pipeline with soft voting performs. Its score is 0.818, which is better than either model alone.\n\ncross_val_score(vc_pipe, X, y, cv=5, scoring='accuracy').mean()\n\n0.8181846713953927\n\n\n\n\n\n\n\n\nPipeline accuracy scores:\n\nGrid search (RF): 0.829\nGrid search (LR): 0.828\nBaseline (VC soft voting): 0.818\nBaseline (LR): 0.811\nBaseline (RF): 0.811",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Ensembling multiple models</span>"
    ]
  },
  {
    "objectID": "ch12.html#combining-class-predictions",
    "href": "ch12.html#combining-class-predictions",
    "title": "12¬† Ensembling multiple models",
    "section": "12.4 Combining class predictions",
    "text": "12.4 Combining class predictions\nNow let‚Äôs modify the VotingClassifier to use hard voting, which means that it ignores predicted probabilities and just takes a majority vote based on class predictions.\n\nvc = VotingClassifier([('clf1', logreg), ('clf2', rf)], voting='hard',\n                      n_jobs=-1)\nvc_pipe = make_pipeline(ct, vc)\n\nWhen we cross-validate the Pipeline with hard voting, it performs a bit better, scoring 0.820.\n\ncross_val_score(vc_pipe, X, y, cv=5, scoring='accuracy').mean()\n\n0.8204255853367648\n\n\n\n\n\n\n\n\nPipeline accuracy scores:\n\nGrid search (RF): 0.829\nGrid search (LR): 0.828\nBaseline (VC hard voting): 0.820\nBaseline (VC soft voting): 0.818\nBaseline (LR): 0.811\nBaseline (RF): 0.811\n\n\n\n\nHowever, this result is actually misleading:\n\nWhen you use hard voting with VotingClassifier and there‚Äôs a tie, it always chooses the lowest numbered class. In other words, every time the two models disagreed, it chose class 0.\nThis means that hard voting is performing better than soft voting purely by chance. If the tiebreaking algorithm instead chose the highest numbered class, hard voting would be performing worse than soft voting.\n\n\n\n\n\n\n\nWhy is this result misleading?\n\nIn the case of a tie, hard voting always chooses class 0\nThus hard voting is performing better than soft voting by chance",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Ensembling multiple models</span>"
    ]
  },
  {
    "objectID": "ch12.html#choosing-a-voting-strategy",
    "href": "ch12.html#choosing-a-voting-strategy",
    "title": "12¬† Ensembling multiple models",
    "section": "12.5 Choosing a voting strategy",
    "text": "12.5 Choosing a voting strategy\nThe previous lesson brings up the obvious question: When should you use soft voting, and when should you use hard voting?\n\nSoft voting is preferred if you have an even number of models in the ensemble, and especially if you only have two models.\nSoft voting is preferred if all of your models output well-calibrated predicted probabilities, whereas hard voting is preferred otherwise.\nHard voting will always work, whereas soft voting will only work if all of the models in the ensemble include the predict_proba method. For example, LinearSVC and Perceptron don‚Äôt include the predict_proba method.\n\n\n\n\n\n\n\nSoft voting vs hard voting:\n\nSoft voting:\n\nPreferred if you have an even number of models (especially two)\nPreferred if all models are well-calibrated\nOnly works if all models have the predict_proba method\n\nHard voting:\n\nPreferred if some models are not well-calibrated\nDoes not require the predict_proba method\n\n\n\n\n\nUltimately, you can just try both soft and hard voting and see which works better, keeping in mind that hard voting results can be misleading if you have an even number of classifiers.\nBecause we‚Äôre using an even number of classifiers, we‚Äôll change back to soft voting.\n\nvc = VotingClassifier([('clf1', logreg), ('clf2', rf)], voting='soft',\n                      n_jobs=-1)\nvc_pipe = make_pipeline(ct, vc)",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Ensembling multiple models</span>"
    ]
  },
  {
    "objectID": "ch12.html#tuning-an-ensemble-with-grid-search",
    "href": "ch12.html#tuning-an-ensemble-with-grid-search",
    "title": "12¬† Ensembling multiple models",
    "section": "12.6 Tuning an ensemble with grid search",
    "text": "12.6 Tuning an ensemble with grid search\nLike with any model, we can tune the VotingClassifier‚Äôs hyperparameters using a grid search to try to improve its accuracy. Keep in mind that the best parameters for the VotingClassifier Pipeline might be different than the parameters for either model when they were tuned separately.\nWe‚Äôll start by creating a vc_params dictionary that only includes the ColumnTransformer parameters.\n\nvc_params = {k:v for k, v in params.items() if k.startswith('col')}\nvc_params\n\n{'columntransformer__pipeline__onehotencoder__drop': [None, 'first'],\n 'columntransformer__countvectorizer__ngram_range': [(1, 1), (1, 2)],\n 'columntransformer__simpleimputer__add_indicator': [False, True]}\n\n\nWe can look at the named_steps attribute of vc_pipe to confirm that the second step name is votingclassifier (all lowercase).\n\nvc_pipe.named_steps.keys()\n\ndict_keys(['columntransformer', 'votingclassifier'])\n\n\nAnd you may recall that we assigned the names clf1 and clf2 to our two models within the VotingClassifier.\n\nvc_pipe.named_steps['votingclassifier'].named_estimators\n\n{'clf1': LogisticRegression(random_state=1, solver='liblinear'),\n 'clf2': RandomForestClassifier(n_jobs=-1, random_state=1)}\n\n\nKnowing those names, we can now add some model parameters to vc_params. For simplicity and speed, we‚Äôll just tune a smaller selection of parameters and values.\n\nvc_params['votingclassifier__clf1__penalty'] = ['l1', 'l2']\nvc_params['votingclassifier__clf1__C'] = [1, 10]\nvc_params['votingclassifier__clf2__n_estimators'] = [100, 300]\nvc_params['votingclassifier__clf2__min_samples_leaf'] = [2, 3]\nvc_params\n\n{'columntransformer__pipeline__onehotencoder__drop': [None, 'first'],\n 'columntransformer__countvectorizer__ngram_range': [(1, 1), (1, 2)],\n 'columntransformer__simpleimputer__add_indicator': [False, True],\n 'votingclassifier__clf1__penalty': ['l1', 'l2'],\n 'votingclassifier__clf1__C': [1, 10],\n 'votingclassifier__clf2__n_estimators': [100, 300],\n 'votingclassifier__clf2__min_samples_leaf': [2, 3]}\n\n\nFinally, we‚Äôll create a GridSearchCV object called vc_grid, making sure to use the vc_pipe and vc_params objects, and then run the search.\n\nvc_grid = GridSearchCV(vc_pipe, vc_params, cv=5, scoring='accuracy',\n                       n_jobs=-1)\nvc_grid.fit(X, y)\n\nGridSearchCVGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('columntransformer',\n                                        ColumnTransformer(transformers=[('pipeline',\n                                                                         Pipeline(steps=[('simpleimputer',\n                                                                                          SimpleImputer(fill_value='missing',\n                                                                                                        strategy='constant')),\n                                                                                         ('onehotencoder',\n                                                                                          OneHotEncoder())]),\n                                                                         ['Embarked',\n                                                                          'Sex']),\n                                                                        ('countvectorizer',\n                                                                         CountVectorizer(),\n                                                                         'Name'),\n                                                                        ('simpleimputer',\n                                                                         SimpleImputer(),\n                                                                         ['Age',\n                                                                          'Fare']),\n                                                                        (...\n             param_grid={'columntransformer__countvectorizer__ngram_range': [(1,\n                                                                              1),\n                                                                             (1,\n                                                                              2)],\n                         'columntransformer__pipeline__onehotencoder__drop': [None,\n                                                                              'first'],\n                         'columntransformer__simpleimputer__add_indicator': [False,\n                                                                             True],\n                         'votingclassifier__clf1__C': [1, 10],\n                         'votingclassifier__clf1__penalty': ['l1', 'l2'],\n                         'votingclassifier__clf2__min_samples_leaf': [2, 3],\n                         'votingclassifier__clf2__n_estimators': [100, 300]},\n             scoring='accuracy')columntransformer: ColumnTransformerColumnTransformer(transformers=[('pipeline',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer(fill_value='missing',\n                                                                strategy='constant')),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder())]),\n                                 ['Embarked', 'Sex']),\n                                ('countvectorizer', CountVectorizer(), 'Name'),\n                                ('simpleimputer', SimpleImputer(),\n                                 ['Age', 'Fare']),\n                                ('passthrough', 'passthrough', ['Parch'])])pipeline['Embarked', 'Sex']SimpleImputerSimpleImputer(fill_value='missing', strategy='constant')OneHotEncoderOneHotEncoder()countvectorizerNameCountVectorizerCountVectorizer()simpleimputer['Age', 'Fare']SimpleImputerSimpleImputer()passthrough['Parch']passthroughpassthroughvotingclassifier: VotingClassifierVotingClassifier(estimators=[('clf1',\n                              LogisticRegression(random_state=1,\n                                                 solver='liblinear')),\n                             ('clf2',\n                              RandomForestClassifier(n_jobs=-1,\n                                                     random_state=1))],\n                 n_jobs=-1, voting='soft')clf1LogisticRegressionLogisticRegression(random_state=1, solver='liblinear')clf2RandomForestClassifierRandomForestClassifier(n_jobs=-1, random_state=1)\n\n\nYou can see that the best score has improved significantly, to 0.834.\n\nvc_grid.best_score_\n\n0.833864791915134\n\n\n\n\n\n\n\n\nPipeline accuracy scores:\n\nGrid search (VC soft voting): 0.834\nGrid search (RF): 0.829\nGrid search (LR): 0.828\nBaseline (VC hard voting): 0.820\nBaseline (VC soft voting): 0.818\nBaseline (LR): 0.811\nBaseline (RF): 0.811\n\n\n\n\nHere‚Äôs the best set of parameters that it found.\n\nvc_grid.best_params_\n\n{'columntransformer__countvectorizer__ngram_range': (1, 2),\n 'columntransformer__pipeline__onehotencoder__drop': None,\n 'columntransformer__simpleimputer__add_indicator': True,\n 'votingclassifier__clf1__C': 10,\n 'votingclassifier__clf1__penalty': 'l1',\n 'votingclassifier__clf2__min_samples_leaf': 3,\n 'votingclassifier__clf2__n_estimators': 100}\n\n\nAnd finally, using the tuned grid to make predictions for new data is the same as always.\n\nvc_grid.predict(X_new)\n\narray([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,\n       1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n       1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,\n       1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n       1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n       0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n       1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n       1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,\n       0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n       1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n       1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n       0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,\n       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n       0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,\n       1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,\n       0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n       1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n       0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1])",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Ensembling multiple models</span>"
    ]
  },
  {
    "objectID": "ch12.html#qa-when-should-i-use-ensembling",
    "href": "ch12.html#qa-when-should-i-use-ensembling",
    "title": "12¬† Ensembling multiple models",
    "section": "12.7 Q&A: When should I use ensembling?",
    "text": "12.7 Q&A: When should I use ensembling?\nEnsembling generally improves the performance of your model, and so it‚Äôs a useful technique any time that performance is your highest priority. Keep in mind, however, that ensembling adds more complexity to your process, and the ensemble is also less interpretable than a single model.\n\n\n\n\n\n\nShould you ensemble?\n\nAdvantages:\n\nImproves model performance\n\nDisadvantages:\n\nAdds more complexity\nDecreases interpretability\n\n\n\n\n\nIf you do decide to use ensembling, my advice is to include at least 3 models in the ensemble. It‚Äôs important that all models you include are performing reasonably well on their own. And as mentioned before, it‚Äôs ideal if they generate their predictions using different processes.\n\n\n\n\n\n\nAdvice for ensembling:\n\nInclude at least 3 models\nModels should be performing well on their own\nIdeal if they generate predictions using different processes",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Ensembling multiple models</span>"
    ]
  },
  {
    "objectID": "ch12.html#qa-how-do-i-apply-different-weights-to-the-models-in-an-ensemble",
    "href": "ch12.html#qa-how-do-i-apply-different-weights-to-the-models-in-an-ensemble",
    "title": "12¬† Ensembling multiple models",
    "section": "12.8 Q&A: How do I apply different weights to the models in an ensemble?",
    "text": "12.8 Q&A: How do I apply different weights to the models in an ensemble?\nBy default, each model within an ensemble is given equal weight. However, you can try weighting certain models more than others to give them more ‚Äúvoting power‚Äù when determining the predicted class labels or predicted probabilities.\nFor example, we could give the logistic regression model double the voting power of the random forest model by setting the weights parameter of the VotingClassifier.\n\nvc = VotingClassifier([('clf1', logreg), ('clf2', rf)], voting='soft',\n                      weights=[2, 1], n_jobs=-1)\nvc_pipe = make_pipeline(ct, vc)\n\nLet‚Äôs see how that affects the predicted probabilities. Once again, here are the predicted probabilities output by the logistic regression and random forest models for the first 3 samples in X_new.\n\npipe.predict_proba(X_new)[:3]\n\narray([[0.88916549, 0.11083451],\n       [0.14200691, 0.85799309],\n       [0.9190551 , 0.0809449 ]])\n\n\n\nrf_pipe.predict_proba(X_new)[:3]\n\narray([[0.99, 0.01],\n       [0.24, 0.76],\n       [0.97, 0.03]])\n\n\nAnd here are the predicted probabilities output by the VotingClassifier. As you can see, the predicted probabilities are closer to the ones output by logistic regression because we gave that model twice the weight. For example, 0.94 is closer to 0.92 than it is to 0.97.\n\nvc_pipe.fit(X, y)\nvc_pipe.predict_proba(X_new)[:3]\n\narray([[0.92277699, 0.07722301],\n       [0.17467127, 0.82532873],\n       [0.93603673, 0.06396327]])\n\n\nYou can confirm whether the weights are helping or hurting the ensemble by using cross-validation. In this case, the score is 0.816, which is slightly worse than our baseline VotingClassifier.\n\ncross_val_score(vc_pipe, X, y, cv=5, scoring='accuracy').mean()\n\n0.8159437574540205\n\n\n\n\n\n\n\n\nPipeline accuracy scores:\n\nGrid search (VC soft voting): 0.834\nGrid search (RF): 0.829\nGrid search (LR): 0.828\nBaseline (VC hard voting): 0.820\nBaseline (VC soft voting): 0.818\nBaseline (VC soft voting with LR weighted): 0.816\nBaseline (LR): 0.811\nBaseline (RF): 0.811\n\n\n\n\nYou can also search for the optimal weights using a grid search. Here‚Äôs how you might add that to the vc_params dictionary.\n\nvc_params['votingclassifier__weights'] = [(1, 1), (2, 1), (1, 2)]",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Ensembling multiple models</span>"
    ]
  },
  {
    "objectID": "ch13.html",
    "href": "ch13.html",
    "title": "13¬† Feature selection",
    "section": "",
    "text": "13.1 Introduction to feature selection\nFeature selection is the process of removing uninformative features from your model. These are features that are not helping your model to make better predictions. In other words, uninformative features are adding ‚Äúnoise‚Äù to your model, rather than ‚Äúsignal‚Äù.\nThere are a few reasons you might want to add feature selection to your workflow:\nAs I mentioned at the start of the book, there are many valid methods for feature selection, including human intuition, domain knowledge, and data exploration. In this chapter, we‚Äôre going to do feature selection using automated methods that we can include in our Pipeline.\nThere are three types of automated methods that we‚Äôll cover in this chapter: intrinsic methods, filter methods, and wrapper methods.\nFor the purposes of simplicity and training speed, we‚Äôll use our logistic regression Pipeline as the starting point for the next few chapters. However, everything you‚Äôre learning could also be applied to the random forest Pipeline or the ensemble Pipeline.",
    "crumbs": [
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Feature selection</span>"
    ]
  },
  {
    "objectID": "ch13.html#introduction-to-feature-selection",
    "href": "ch13.html#introduction-to-feature-selection",
    "title": "13¬† Feature selection",
    "section": "",
    "text": "Model accuracy can often be improved by removing uninformative features.\nModels are generally easier to interpret when they include fewer features.\nWhen you have fewer features, models will take less time to train and it may cost less to gather and store the data that is required to train them.\n\n\n\n\n\n\n\nPotential benefits of feature selection:\n\nHigher accuracy\nGreater interpretability\nFaster training\nLower costs\n\n\n\n\n\n\n\n\n\n\n\nFeature selection methods:\n\nHuman intuition\nDomain knowledge\nData exploration\nAutomated methods\n\n\n\n\n\n\n\n\n\n\n\nMethods for automated feature selection:\n\nIntrinsic methods\nFilter methods\nWrapper methods",
    "crumbs": [
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Feature selection</span>"
    ]
  },
  {
    "objectID": "ch13.html#intrinsic-methods-l1-regularization",
    "href": "ch13.html#intrinsic-methods-l1-regularization",
    "title": "13¬† Feature selection",
    "section": "13.2 Intrinsic methods: L1 regularization",
    "text": "13.2 Intrinsic methods: L1 regularization\nAn intrinsic feature selection method is one in which feature selection happens automatically as part of the model building process. These are also known as implicit methods or embedded methods.\n\n\n\n\n\n\nWhat are intrinsic methods?\n\nFeature selection happens automatically during model building\nAlso called: implicit methods, embedded methods\n\n\n\n\nWe‚Äôve actually already used an intrinsic feature selection method in the book. Recall that in chapter 10, we tuned our logistic regression Pipeline using a grid search. Here were the best parameters.\n\ngrid.best_params_\n\n{'columntransformer__countvectorizer__ngram_range': (1, 2),\n 'columntransformer__pipeline__onehotencoder__drop': None,\n 'columntransformer__simpleimputer__add_indicator': True,\n 'logisticregression__C': 10,\n 'logisticregression__penalty': 'l1'}\n\n\nNotice the C and penalty parameters of logistic regression. The L1 penalty is the type of regularization that was used, and the C value indicates the amount of regularization.\n\n\n\n\n\n\nLogisticRegression tuning parameters:\n\npenalty: Type of regularization\nC: Amount of regularization\n\n\n\n\nIn general, regularization shrinks model coefficients in order to minimize overfitting to the training data and improve the model‚Äôs ability to generalize to new data.\nOne notable aspect of L1 regularization in particular is that as the amount of regularization increases, some coefficients will be shrunk all the way to zero, which means they will be excluded from the model. In other words, L1 regularization automatically does feature selection.\n\n\n\n\n\n\nHow does L1 regularization do feature selection?\n\nRegularization shrinks model coefficients to help the model to generalize\nL1 regularization shrinks some coefficients to zero, which removes those features\n\n\n\n\nTo see an example of this, let‚Äôs take a look at the coefficients of the best model found by grid search, which is stored in the best_estimator_ attribute. Notice that the second coefficient is zero, which means that the L1 regularization caused that feature to be removed from the model.\n\ngrid.best_estimator_.named_steps['logisticregression'].coef_\n\narray([[ 0.56431161,  0.        , -0.08767203, ...,  0.01408723,\n        -0.43713268, -0.46358519]])\n\n\nBy checking the shape of the inner array, we can see that there are 3671 features.\n\ngrid.best_estimator_.named_steps['logisticregression'].coef_[0].shape\n\n(3671,)\n\n\nWe can then check how many of the coefficients are zero. It turns out that 3103 coefficients were set to zero, which means that L1 regularization removed those features, leaving only 568 of the features.\nNote that as the amount of regularization increases, more coefficients will be shrunk to zero and thus more features will be removed from the model. In the case of logistic regression, you increase the amount of regularization by decreasing the value of C.\n\nsum(grid.best_estimator_.named_steps['logisticregression'].coef_[0] == 0)\n\n3103\n\n\nLet‚Äôs compare this to our logistic regression Pipeline that was not tuned by grid search. You can see that it uses L2 regularization.\n\npipe.named_steps['logisticregression'].get_params()\n\n{'C': 1.0,\n 'class_weight': None,\n 'dual': False,\n 'fit_intercept': True,\n 'intercept_scaling': 1,\n 'l1_ratio': None,\n 'max_iter': 100,\n 'multi_class': 'auto',\n 'n_jobs': None,\n 'penalty': 'l2',\n 'random_state': 1,\n 'solver': 'liblinear',\n 'tol': 0.0001,\n 'verbose': 0,\n 'warm_start': False}\n\n\nAlthough L2 regularization does shrink coefficients, we can confirm that it does not shrink them all the way to zero, and thus it does not perform feature selection.\nKeep in mind that although L1 regularization produced a better performing model in this situation, that will not always be the case. It‚Äôs a good idea to always try both types of regularization and see which one works better.\n\nsum(pipe.named_steps['logisticregression'].coef_[0] == 0)\n\n0\n\n\nTo wrap up this section, let‚Äôs talk about some advantages and disadvantages of intrinsic feature selection methods:\n\nThe main advantages are speed and simplicity: Since feature selection is implictly performed during model fitting, no additional feature selection process needs to be added to the workflow, which tends to save a lot of computational time.\nThe main disadvantage is that it‚Äôs model-dependent: The model that is best for your particular problem may not perform intrinsic feature selection.\n\n\n\n\n\n\n\nAdvantages and disadvantages of intrinsic methods:\n\nAdvantages:\n\nNo added computation\nNo added steps\n\nDisadvantages:\n\nModel-dependent",
    "crumbs": [
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Feature selection</span>"
    ]
  },
  {
    "objectID": "ch13.html#filter-methods-statistical-test-based-scoring",
    "href": "ch13.html#filter-methods-statistical-test-based-scoring",
    "title": "13¬† Feature selection",
    "section": "13.3 Filter methods: Statistical test-based scoring",
    "text": "13.3 Filter methods: Statistical test-based scoring\nThe next type of feature selection method we‚Äôll cover is filter methods.\nA filter method starts by scoring every single feature to quantify its potential relationship with the target column. Then, the features are ranked by their scores, and only the top scoring features are provided to the model. Thus, they‚Äôre called filter methods because they filter out what they believe to be the least informative features and then pass on the more informative features to the model.\nAs you‚Äôll see in this section, filter methods vary in terms of the processes they use to score the features.\n\n\n\n\n\n\nHow filter methods work:\n\nEach feature is scored by its relationship to the target\nTop scoring features (most informative features) are provided to the model\n\n\n\n\nOur starting point for this section will be the logistic regression Pipeline that has not been tuned by grid search. The reason for this is because we want to tune all of the Pipeline steps simultaneously, rather than tuning the transformers and model first and then adding feature selection.\nIn other words, the presence of a feature selection process may alter the optimal parameters for the transformers and the model, and thus we need to tune all three steps at once. Right now it‚Äôs a two-step Pipeline, but there will be three steps once we add feature selection to the Pipeline.\n\npipe\n\nPipelinePipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('pipeline',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer(fill_value='missing',\n                                                                                 strategy='constant')),\n                                                                  ('onehotencoder',\n                                                                   OneHotEncoder())]),\n                                                  ['Embarked', 'Sex']),\n                                                 ('countvectorizer',\n                                                  CountVectorizer(), 'Name'),\n                                                 ('simpleimputer',\n                                                  SimpleImputer(),\n                                                  ['Age', 'Fare']),\n                                                 ('passthrough', 'passthrough',\n                                                  ['Parch'])])),\n                ('logisticregression',\n                 LogisticRegression(random_state=1, solver='liblinear'))])columntransformer: ColumnTransformerColumnTransformer(transformers=[('pipeline',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer(fill_value='missing',\n                                                                strategy='constant')),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder())]),\n                                 ['Embarked', 'Sex']),\n                                ('countvectorizer', CountVectorizer(), 'Name'),\n                                ('simpleimputer', SimpleImputer(),\n                                 ['Age', 'Fare']),\n                                ('passthrough', 'passthrough', ['Parch'])])pipeline['Embarked', 'Sex']SimpleImputerSimpleImputer(fill_value='missing', strategy='constant')OneHotEncoderOneHotEncoder()countvectorizerNameCountVectorizerCountVectorizer()simpleimputer['Age', 'Fare']SimpleImputerSimpleImputer()passthrough['Parch']passthroughpassthroughLogisticRegressionLogisticRegression(random_state=1, solver='liblinear')\n\n\nLet‚Äôs cross-validate this Pipeline to generate a ‚Äúbaseline‚Äù accuracy that we want to improve upon, which is 0.811.\n\ncross_val_score(pipe, X, y, cv=5, scoring='accuracy').mean()\n\n0.8114619295712762\n\n\n\n\n\n\n\n\nPipeline accuracy scores:\n\nGrid search (VC): 0.834\nGrid search (RF): 0.829\nGrid search (LR): 0.828\nBaseline (VC): 0.818\nBaseline (LR): 0.811\nBaseline (RF): 0.811\n\n\n\n\nThe first filter method we‚Äôll use is SelectPercentile. SelectPercentile scores features using univariate statistical tests:\n\nYou specify a statistical test, and it uses that test to score each feature independently.\nThen, it passes on to the model a certain percentage (that you specify) of the best scoring features.\n\nThus, the assumption behind SelectPercentile is that a statistical test can assess the strength of the relationship between a feature and the target, and that if a feature appears to be independent of the target, then it is uninformative for the purpose of classification.\n\n\n\n\n\n\nHow SelectPercentile works:\n\nScores each feature using the statistical test you specify\nPasses to the model the percentage of features you specify\n\n\n\n\nLet‚Äôs see how SelectPercentile works. After importing SelectPercentile and chi2 from the feature_selection module, we‚Äôll create an instance of SelectPercentile called selection.\nFirst, we pass it the statistical test. In this case we‚Äôre using chi-squared, but other tests are available in scikit-learn.\nThen, we pass it the percentile. We‚Äôre arbitrarily using 50 to keep 50% of the features, but this is a parameter you should tune. And to be clear, lower values for this parameter keep fewer features, so for example a value of 10 would only keep 10% percent of the features.\n\nfrom sklearn.feature_selection import SelectPercentile, chi2\nselection = SelectPercentile(chi2, percentile=50)\n\nNext, we create a Pipeline called fs_pipe in which feature selection is after the ColumnTransformer but before the model. Thus, it will perform feature selection on the transformed features, not the original features.\n\nfs_pipe = make_pipeline(ct, selection, logreg)\nfs_pipe\n\nPipelinePipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('pipeline',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer(fill_value='missing',\n                                                                                 strategy='constant')),\n                                                                  ('onehotencoder',\n                                                                   OneHotEncoder())]),\n                                                  ['Embarked', 'Sex']),\n                                                 ('countvectorizer',\n                                                  CountVectorizer(), 'Name'),\n                                                 ('simpleimputer',\n                                                  SimpleImputer(),\n                                                  ['Age', 'Fare']),\n                                                 ('passthrough', 'passthrough',\n                                                  ['Parch'])])),\n                ('selectpercentile',\n                 SelectPercentile(percentile=50,\n                                  score_func=)),\n                ('logisticregression',\n                 LogisticRegression(random_state=1, solver='liblinear'))])columntransformer: ColumnTransformerColumnTransformer(transformers=[('pipeline',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer(fill_value='missing',\n                                                                strategy='constant')),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder())]),\n                                 ['Embarked', 'Sex']),\n                                ('countvectorizer', CountVectorizer(), 'Name'),\n                                ('simpleimputer', SimpleImputer(),\n                                 ['Age', 'Fare']),\n                                ('passthrough', 'passthrough', ['Parch'])])pipeline['Embarked', 'Sex']SimpleImputerSimpleImputer(fill_value='missing', strategy='constant')OneHotEncoderOneHotEncoder()countvectorizerNameCountVectorizerCountVectorizer()simpleimputer['Age', 'Fare']SimpleImputerSimpleImputer()passthrough['Parch']passthroughpassthroughSelectPercentileSelectPercentile(percentile=50, score_func=)LogisticRegressionLogisticRegression(random_state=1, solver='liblinear')\n\n\nBecause we‚Äôve included feature selection within the Pipeline, we can continue to cross-validate the entire process to see the impact of feature selection on model accuracy. When we run cross-validation on the new Pipeline, the score has improved to 0.819.\n\ncross_val_score(fs_pipe, X, y, cv=5, scoring='accuracy').mean()\n\n0.8193019898311469\n\n\n\n\n\n\n\n\nPipeline accuracy scores:\n\nGrid search (VC): 0.834\nGrid search (RF): 0.829\nGrid search (LR): 0.828\nBaseline (LR with SelectPercentile): 0.819\nBaseline (VC): 0.818\nBaseline (LR): 0.811\nBaseline (RF): 0.811\n\n\n\n\nIt‚Äôs worth noting that there‚Äôs an alternative to SelectPercentile called SelectKBest. SelectKBest is nearly identical, except that you specify a number of features to keep rather than a percentage.\n\n\n\n\n\n\nSelectPercentile vs SelectKBest:\n\nSelectPercentile: Specify percentage of features to keep\nSelectKBest: Specify number of features to keep",
    "crumbs": [
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Feature selection</span>"
    ]
  },
  {
    "objectID": "ch13.html#filter-methods-model-based-scoring",
    "href": "ch13.html#filter-methods-model-based-scoring",
    "title": "13¬† Feature selection",
    "section": "13.4 Filter methods: Model-based scoring",
    "text": "13.4 Filter methods: Model-based scoring\nThe other filter method we‚Äôll use is called SelectFromModel. Whereas SelectPercentile scores features using a statistical test, SelectFromModel actually uses a model to score features:\n\nFirst, you specify a model to use only for feature selection. That model is fit on all of the features, and the coef_ or feature_importances_ attribute of the model is used as the scores.\nThen, it passes on to your prediction model all of the features that score above a certain threshold (that you specify).\n\n\n\n\n\n\n\nHow SelectFromModel works:\n\nScores each feature using the model you specify\n\nModel is fit on all features\nCoefficients or feature importances are used as scores\n\nPasses to the prediction model features that score above a threshold you specify\n\n\n\n\nThus for a model to be used by SelectFromModel, it has to calculate either coefficients or feature importances. Models that can be used by SelectFromModel include logistic regression, linear SVC, and tree-based models.\n\n\n\n\n\n\nModels that can be used by SelectFromModel:\n\nLogistic regression\nLinear SVC\nTree-based models\nAny other model with coefficients or feature importances\n\n\n\n\nTo be clear, SelectFromModel is a filter method (not an intrinsic method) because it‚Äôs filtering which features are passed to your separate prediction model.\nLet‚Äôs see how all of this fits together. We‚Äôre going to start by using logistic regression for feature selection. We‚Äôll create a new instance of logistic regression called logreg_selection that‚Äôs only going to be used for feature selection. It‚Äôs completely separate from the logistic regression model we‚Äôre using to make predictions.\n\nlogreg_selection = LogisticRegression(solver='liblinear', penalty='l1',\n                                      random_state=1)\n\nThen, we‚Äôll import SelectFromModel from the feature_selection module and create an instance called selection.\nFirst, we pass it the model we‚Äôre using for selection. Second, we pass it a threshold. This can be the mean or median of the scores, though you can optionally include a scaling factor (such as 1.5 times mean). All features above this threshold will be passed to the prediction model, thus setting a higher threshold means fewer features will be kept.\n\nfrom sklearn.feature_selection import SelectFromModel\nselection = SelectFromModel(logreg_selection, threshold='mean')\n\nThen, we‚Äôll update fs_pipe to use the new feature selection object. Notice that logistic regression appears twice: one instance is being used only for feature selection, and the other instance is being used only for prediction.\n\nfs_pipe = make_pipeline(ct, selection, logreg)\nfs_pipe\n\nPipelinePipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('pipeline',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer(fill_value='missing',\n                                                                                 strategy='constant')),\n                                                                  ('onehotencoder',\n                                                                   OneHotEncoder())]),\n                                                  ['Embarked', 'Sex']),\n                                                 ('countvectorizer',\n                                                  CountVectorizer(), 'Name'),\n                                                 ('simpleimputer',\n                                                  SimpleImputer(),\n                                                  ['Age', 'Fare']),\n                                                 ('passthrough', 'passthrough',\n                                                  ['Parch'])])),\n                ('selectfrommodel',\n                 SelectFromModel(estimator=LogisticRegression(penalty='l1',\n                                                              random_state=1,\n                                                              solver='liblinear'),\n                                 threshold='mean')),\n                ('logisticregression',\n                 LogisticRegression(random_state=1, solver='liblinear'))])columntransformer: ColumnTransformerColumnTransformer(transformers=[('pipeline',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer(fill_value='missing',\n                                                                strategy='constant')),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder())]),\n                                 ['Embarked', 'Sex']),\n                                ('countvectorizer', CountVectorizer(), 'Name'),\n                                ('simpleimputer', SimpleImputer(),\n                                 ['Age', 'Fare']),\n                                ('passthrough', 'passthrough', ['Parch'])])pipeline['Embarked', 'Sex']SimpleImputerSimpleImputer(fill_value='missing', strategy='constant')OneHotEncoderOneHotEncoder()countvectorizerNameCountVectorizerCountVectorizer()simpleimputer['Age', 'Fare']SimpleImputerSimpleImputer()passthrough['Parch']passthroughpassthroughselectfrommodel: SelectFromModelSelectFromModel(estimator=LogisticRegression(penalty='l1', random_state=1,\n                                             solver='liblinear'),\n                threshold='mean')LogisticRegressionLogisticRegression(penalty='l1', random_state=1, solver='liblinear')LogisticRegressionLogisticRegression(random_state=1, solver='liblinear')\n\n\nWhen we cross-validate the updated Pipeline, the score has improved again, to 0.826.\n\ncross_val_score(fs_pipe, X, y, cv=5, scoring='accuracy').mean()\n\n0.8260121775155358\n\n\n\n\n\n\n\n\nPipeline accuracy scores:\n\nGrid search (VC): 0.834\nGrid search (RF): 0.829\nGrid search (LR): 0.828\nBaseline (LR with SelectFromModel LR): 0.826\nBaseline (LR with SelectPercentile): 0.819\nBaseline (VC): 0.818\nBaseline (LR): 0.811\nBaseline (RF): 0.811\n\n\n\n\nNow let‚Äôs try using a tree-based model with SelectFromModel. We‚Äôll use ExtraTreesClassifier, which is an ensemble of decision trees similar to random forests. After importing it from the ensemble module, we‚Äôll create an instance to use for feature selection called et_selection.\n\nfrom sklearn.ensemble import ExtraTreesClassifier\net_selection = ExtraTreesClassifier(n_estimators=100, random_state=1)\n\nThen, we‚Äôll update both the feature selection object and the Pipeline. Notice that ExtraTreesClassifier has replaced logistic regression as the second step in the Pipeline.\n\nselection = SelectFromModel(et_selection, threshold='mean')\nfs_pipe = make_pipeline(ct, selection, logreg)\nfs_pipe\n\nPipelinePipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('pipeline',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer(fill_value='missing',\n                                                                                 strategy='constant')),\n                                                                  ('onehotencoder',\n                                                                   OneHotEncoder())]),\n                                                  ['Embarked', 'Sex']),\n                                                 ('countvectorizer',\n                                                  CountVectorizer(), 'Name'),\n                                                 ('simpleimputer',\n                                                  SimpleImputer(),\n                                                  ['Age', 'Fare']),\n                                                 ('passthrough', 'passthrough',\n                                                  ['Parch'])])),\n                ('selectfrommodel',\n                 SelectFromModel(estimator=ExtraTreesClassifier(random_state=1),\n                                 threshold='mean')),\n                ('logisticregression',\n                 LogisticRegression(random_state=1, solver='liblinear'))])columntransformer: ColumnTransformerColumnTransformer(transformers=[('pipeline',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer(fill_value='missing',\n                                                                strategy='constant')),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder())]),\n                                 ['Embarked', 'Sex']),\n                                ('countvectorizer', CountVectorizer(), 'Name'),\n                                ('simpleimputer', SimpleImputer(),\n                                 ['Age', 'Fare']),\n                                ('passthrough', 'passthrough', ['Parch'])])pipeline['Embarked', 'Sex']SimpleImputerSimpleImputer(fill_value='missing', strategy='constant')OneHotEncoderOneHotEncoder()countvectorizerNameCountVectorizerCountVectorizer()simpleimputer['Age', 'Fare']SimpleImputerSimpleImputer()passthrough['Parch']passthroughpassthroughselectfrommodel: SelectFromModelSelectFromModel(estimator=ExtraTreesClassifier(random_state=1),\n                threshold='mean')ExtraTreesClassifierExtraTreesClassifier(random_state=1)LogisticRegressionLogisticRegression(random_state=1, solver='liblinear')\n\n\nWhen we cross-validate the updated Pipeline, the resulting score is 0.815, which is not quite as good.\n\ncross_val_score(fs_pipe, X, y, cv=5, scoring='accuracy').mean()\n\n0.8148013307388112\n\n\n\n\n\n\n\n\nPipeline accuracy scores:\n\nGrid search (VC): 0.834\nGrid search (RF): 0.829\nGrid search (LR): 0.828\nBaseline (LR with SelectFromModel LR): 0.826\nBaseline (LR with SelectPercentile): 0.819\nBaseline (VC): 0.818\nBaseline (LR with SelectFromModel ET): 0.815\nBaseline (LR): 0.811\nBaseline (RF): 0.811\n\n\n\n\nAs I mentioned earlier in this chapter, it‚Äôs important to tune the feature selection parameters, the transformer parameters, and the model parameters all at the same time. We‚Äôll do this using a grid search.\nTo start, we‚Äôll make a copy of our params dictionary called fs_params. We want to add a new entry in order to tune the threshold parameter of SelectFromModel. For the dictionary key, we specify the step name, which is selectfrommodel (all lowercase), followed by two underscores, followed by the parameter name. For the values, we‚Äôll pass a list of mean, 1.5 times the mean, and negative infinity, which means don‚Äôt remove any features.\n\nfs_params = params.copy()\nfs_params['selectfrommodel__threshold'] = ['mean', '1.5*mean', -np.inf]\n\nLet‚Äôs review the fs_params dictionary to see everything we‚Äôre tuning.\n\nfs_params\n\n{'logisticregression__penalty': ['l1', 'l2'],\n 'logisticregression__C': [0.1, 1, 10],\n 'columntransformer__pipeline__onehotencoder__drop': [None, 'first'],\n 'columntransformer__countvectorizer__ngram_range': [(1, 1), (1, 2)],\n 'columntransformer__simpleimputer__add_indicator': [False, True],\n 'selectfrommodel__threshold': ['mean', '1.5*mean', -inf]}\n\n\nWe‚Äôll create a new instance of GridSearchCV called fs_grid, and make sure to pass it the fs_pipe and fs_params objects. Then we‚Äôll run the grid search.\n\nfs_grid = GridSearchCV(fs_pipe, fs_params, cv=5, scoring='accuracy',\n                       n_jobs=-1)\nfs_grid.fit(X, y)\n\nGridSearchCVGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('columntransformer',\n                                        ColumnTransformer(transformers=[('pipeline',\n                                                                         Pipeline(steps=[('simpleimputer',\n                                                                                          SimpleImputer(fill_value='missing',\n                                                                                                        strategy='constant')),\n                                                                                         ('onehotencoder',\n                                                                                          OneHotEncoder())]),\n                                                                         ['Embarked',\n                                                                          'Sex']),\n                                                                        ('countvectorizer',\n                                                                         CountVectorizer(),\n                                                                         'Name'),\n                                                                        ('simpleimputer',\n                                                                         SimpleImputer(),\n                                                                         ['Age',\n                                                                          'Fare']),\n                                                                        (...\n             param_grid={'columntransformer__countvectorizer__ngram_range': [(1,\n                                                                              1),\n                                                                             (1,\n                                                                              2)],\n                         'columntransformer__pipeline__onehotencoder__drop': [None,\n                                                                              'first'],\n                         'columntransformer__simpleimputer__add_indicator': [False,\n                                                                             True],\n                         'logisticregression__C': [0.1, 1, 10],\n                         'logisticregression__penalty': ['l1', 'l2'],\n                         'selectfrommodel__threshold': ['mean', '1.5*mean',\n                                                        -inf]},\n             scoring='accuracy')columntransformer: ColumnTransformerColumnTransformer(transformers=[('pipeline',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer(fill_value='missing',\n                                                                strategy='constant')),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder())]),\n                                 ['Embarked', 'Sex']),\n                                ('countvectorizer', CountVectorizer(), 'Name'),\n                                ('simpleimputer', SimpleImputer(),\n                                 ['Age', 'Fare']),\n                                ('passthrough', 'passthrough', ['Parch'])])pipeline['Embarked', 'Sex']SimpleImputerSimpleImputer(fill_value='missing', strategy='constant')OneHotEncoderOneHotEncoder()countvectorizerNameCountVectorizerCountVectorizer()simpleimputer['Age', 'Fare']SimpleImputerSimpleImputer()passthrough['Parch']passthroughpassthroughselectfrommodel: SelectFromModelSelectFromModel(estimator=ExtraTreesClassifier(random_state=1),\n                threshold='mean')ExtraTreesClassifierExtraTreesClassifier(random_state=1)LogisticRegressionLogisticRegression(random_state=1, solver='liblinear')\n\n\nThis search results in a score of 0.832, which is one of our best scores so far.\n\nfs_grid.best_score_\n\n0.8316301550436258\n\n\n\n\n\n\n\n\nPipeline accuracy scores:\n\nGrid search (VC): 0.834\nGrid search (LR with SelectFromModel ET): 0.832\nGrid search (RF): 0.829\nGrid search (LR): 0.828\nBaseline (LR with SelectFromModel LR): 0.826\nBaseline (LR with SelectPercentile): 0.819\nBaseline (VC): 0.818\nBaseline (LR with SelectFromModel ET): 0.815\nBaseline (LR): 0.811\nBaseline (RF): 0.811\n\n\n\n\nBy examining the parameters, we can see that increasing the SelectFromModel threshold and thus keeping fewer features helped the model to perform better.\n\nfs_grid.best_params_\n\n{'columntransformer__countvectorizer__ngram_range': (1, 2),\n 'columntransformer__pipeline__onehotencoder__drop': None,\n 'columntransformer__simpleimputer__add_indicator': False,\n 'logisticregression__C': 10,\n 'logisticregression__penalty': 'l1',\n 'selectfrommodel__threshold': '1.5*mean'}",
    "crumbs": [
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Feature selection</span>"
    ]
  },
  {
    "objectID": "ch13.html#filter-methods-summary",
    "href": "ch13.html#filter-methods-summary",
    "title": "13¬† Feature selection",
    "section": "13.5 Filter methods: Summary",
    "text": "13.5 Filter methods: Summary\nTo wrap up this section, let‚Äôs talk about some advantages and disadvantages of filter methods.\nThe main advantage is that filter methods tend to run very quickly, though it‚Äôs worth noting that some statistical tests used with SelectPercentile and ensemble methods used with SelectFromModel can run quite slowly.\nThe main disadvantage is that there‚Äôs a disconnect between how features are being scored and their predictive value. In other words, the chi-squared scores or coefficient values or feature importance scores are not a perfect measure of whether a particular feature will help a model make more accurate predictions. Thus, it‚Äôs entirely possible for informative features to receive low scores and be removed from a model, and for uninformative features to receive high scores and be kept in a model. One particular case of note is that the feature importance scores generated by tree-based models will be artificially low for any features which are highly correlated, which may result in important features being removed.\nThe other disadvantage of filter methods is that scores are calculated only once. This ignores the fact that as you remove certain features, the importance of other features may change. This drawback will be addressed by wrapper methods, which we‚Äôll discuss in the next lesson.\n\n\n\n\n\n\nAdvantages and disadvantages of filter methods:\n\nAdvantages:\n\nRuns quickly (usually)\n\nDisadvantages:\n\nScores are not always correlated with predictive value\nScores are calculated only once",
    "crumbs": [
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Feature selection</span>"
    ]
  },
  {
    "objectID": "ch13.html#wrapper-methods-recursive-feature-elimination",
    "href": "ch13.html#wrapper-methods-recursive-feature-elimination",
    "title": "13¬† Feature selection",
    "section": "13.6 Wrapper methods: Recursive feature elimination",
    "text": "13.6 Wrapper methods: Recursive feature elimination\nThe final type of feature selection we‚Äôll cover is wrapper methods.\nIn contrast to the filter methods we‚Äôve seen, in which features are scored only once, wrapper methods perform an iterative search in which features are scored multiple times. More specifically, a wrapper method evaluates a subset of features and then uses the results of that evaluation to help it decide which subset to evaluate next, repeating this process until some stopping criteria is met.\n\n\n\n\n\n\nFilter methods vs wrapper methods:\n\nFilter methods: Features are scored once\nWrapper methods: Features are scored multiple times\n\n\n\n\nThe wrapper method we‚Äôll use in this section is Recursive Feature Elimination, which is available in scikit-learn as the RFE class. The way RFE starts is the same as SelectFromModel:\n\nYou specify a model to use only for feature selection.\nThat model is fit on all of the features.\nThe coefficients or feature importances of the model are used as scores.\n\nHowever, this is the point at which SelectFromModel and RFE diverge:\n\nSelectFromModel would now pass to your prediction model all of the features that score above a certain threshold.\nRFE, on the other hand, removes the single worst scoring feature, refits the feature selection model, and recalculates the feature scores. It repeats this process, recursively eliminating one feature at a time, until it reaches the number of features that you specify. Those remaining features are the ones that will be passed to the prediction model.\n\n\n\n\n\n\n\nHow RFE works:\n\nScores each feature using the model you specify\n\nModel is fit on all features\nCoefficients or feature importances are used as scores\n\nRemoves the single worst scoring feature\nRepeats steps 1 and 2 until it reaches the number of features you specify\nPasses the remaining features to the prediction model\n\n\n\n\nIn other words, SelectFromModel will always just score your features a single time, whereas RFE will score your features potentially hundreds or thousands of times, depending on how many features you want to eliminate. That is obviously more computationally expensive, though it may better capture the relationships between features.\n\n\n\n\n\n\nSelectFromModel vs RFE:\n\nSelectFromModel: Scores your features a single time\nRFE: Scores your features many times\n\nMore computationally expensive\nMay better capture the relationships between features\n\n\n\n\n\nLet‚Äôs try using RFE. We start by importing it from the feature_selection module, and then create an instance called selection. We‚Äôre actually going to reuse logreg_selection as our feature selection model.\nWe‚Äôre also going to specify a step size of 10. By default, RFE will remove 1 feature at a time, and will stop once it has eliminated half of the features. Since there are about 1500 features, the default settings would require about 750 model fits. By setting a step size of 10, RFE will remove 10 features at a time, which reduces the amount of computation by a factor of 10.\n\nfrom sklearn.feature_selection import RFE\nselection = RFE(logreg_selection, step=10)\n\nWe‚Äôll update the fs_pipe object to use the new feature selection object.\n\nfs_pipe = make_pipeline(ct, selection, logreg)\nfs_pipe\n\nPipelinePipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('pipeline',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer(fill_value='missing',\n                                                                                 strategy='constant')),\n                                                                  ('onehotencoder',\n                                                                   OneHotEncoder())]),\n                                                  ['Embarked', 'Sex']),\n                                                 ('countvectorizer',\n                                                  CountVectorizer(), 'Name'),\n                                                 ('simpleimputer',\n                                                  SimpleImputer(),\n                                                  ['Age', 'Fare']),\n                                                 ('passthrough', 'passthrough',\n                                                  ['Parch'])])),\n                ('rfe',\n                 RFE(estimator=LogisticRegression(penalty='l1', random_state=1,\n                                                  solver='liblinear'),\n                     step=10)),\n                ('logisticregression',\n                 LogisticRegression(random_state=1, solver='liblinear'))])columntransformer: ColumnTransformerColumnTransformer(transformers=[('pipeline',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer(fill_value='missing',\n                                                                strategy='constant')),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder())]),\n                                 ['Embarked', 'Sex']),\n                                ('countvectorizer', CountVectorizer(), 'Name'),\n                                ('simpleimputer', SimpleImputer(),\n                                 ['Age', 'Fare']),\n                                ('passthrough', 'passthrough', ['Parch'])])pipeline['Embarked', 'Sex']SimpleImputerSimpleImputer(fill_value='missing', strategy='constant')OneHotEncoderOneHotEncoder()countvectorizerNameCountVectorizerCountVectorizer()simpleimputer['Age', 'Fare']SimpleImputerSimpleImputer()passthrough['Parch']passthroughpassthroughrfe: RFERFE(estimator=LogisticRegression(penalty='l1', random_state=1,\n                                 solver='liblinear'),\n    step=10)LogisticRegressionLogisticRegression(penalty='l1', random_state=1, solver='liblinear')LogisticRegressionLogisticRegression(random_state=1, solver='liblinear')\n\n\nWhen we cross-validate it, we see that its score is 0.814, which is barely better than our baseline of 0.811.\n\ncross_val_score(fs_pipe, X, y, cv=5, scoring='accuracy').mean()\n\n0.8136965664427847\n\n\n\n\n\n\n\n\nPipeline accuracy scores:\n\nGrid search (VC): 0.834\nGrid search (LR with SelectFromModel ET): 0.832\nGrid search (RF): 0.829\nGrid search (LR): 0.828\nBaseline (LR with SelectFromModel LR): 0.826\nBaseline (LR with SelectPercentile): 0.819\nBaseline (VC): 0.818\nBaseline (LR with SelectFromModel ET): 0.815\nBaseline (LR with RFE LR): 0.814\nBaseline (LR): 0.811\nBaseline (RF): 0.811\n\n\n\n\nOf course, it‚Äôs possible that the accuracy would improve if we tuned the number of features kept by RFE, or if we tried different models with RFE.\nIt‚Äôs hard to talk about the advantages and disadvantages of wrapper methods in general because of the diversity of wrapper methods. Instead, let‚Äôs wrap up this section by talking about the advantages and disadvantages of RFE specifically:\n\nThe main advantage of RFE is that it recalculates feature scores as features are removed. This is beneficial because as features are removed, the importance of other features may change, which RFE takes into account (whereas filter methods do not).\nHowever, RFE has the same disadvantage as filter methods, in that there‚Äôs a disconnect between how features are being scored and their predictive value. In other words, informative features might be removed and uninformative features might be kept by RFE.\nAnother disadvantage of RFE is that it‚Äôs computationally expensive, especially if you‚Äôre removing a lot of features.\nA final disadvantage of RFE is that it uses a ‚Äúgreedy‚Äù approach to feature selection, which means that it takes whatever action seems best at the time, even if a different action might ultimately lead to better results at the end of the process. There are non-greedy approaches to feature selection, though none of them are currently available in scikit-learn.\n\n\n\n\n\n\n\nAdvantages and disadvantages of RFE:\n\nAdvantages:\n\nCaptures the relationships between features\n\nDisadvantages:\n\nScores are not always correlated with predictive value\nComputationally expensive\nDoes not look ahead when removing features (‚Äúgreedy‚Äù approach)",
    "crumbs": [
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Feature selection</span>"
    ]
  },
  {
    "objectID": "ch13.html#qa-how-do-i-see-which-features-were-selected",
    "href": "ch13.html#qa-how-do-i-see-which-features-were-selected",
    "title": "13¬† Feature selection",
    "section": "13.7 Q&A: How do I see which features were selected?",
    "text": "13.7 Q&A: How do I see which features were selected?\nRecall that the fs_pipe object has three steps: a ColumnTransformer, a feature selector, and a logistic regression model.\n\nfs_pipe.named_steps.keys()\n\ndict_keys(['columntransformer', 'rfe', 'logisticregression'])\n\n\nWe can use slicing to select the ColumnTransformer step, and then we can run fit_transform to see that it outputs 1518 feature columns.\n\nfs_pipe[0].fit_transform(X)\n\n&lt;891x1518 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n    with 7328 stored elements in Compressed Sparse Row format&gt;\n\n\nIf we select the first two steps and then run fit_transform, we can see that the feature selection step reduced the number of feature columns from 1518 to 759. Note that we have to pass both X and y to fit_transform since the feature selection process requires knowledge of the target values.\n\nfs_pipe[0:2].fit_transform(X, y)\n\n&lt;891x759 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n    with 6008 stored elements in Compressed Sparse Row format&gt;\n\n\nIf we then select the feature selection step and run the get_support method, it returns a boolean array which includes a True for every feature which was kept and a False for every feature which was removed.\n\nfs_pipe[1].get_support()\n\narray([ True,  True,  True, ...,  True,  True,  True])\n\n\nAs you can see, that array has 1518 elements, and 759 of those elements are True.\n\nlen(fs_pipe[1].get_support())\n\n1518\n\n\n\nfs_pipe[1].get_support().sum()\n\n759\n\n\nIdeally, you would be able to use this array to filter the list of all features down to a list of the selected features. However, this is not a straightforward process because the get_feature_names method of ColumnTransformer only works if all of the underlying transformers have a get_feature_names method, and that is not the case here.\n\nfs_pipe[0].get_feature_names()\n\n\n\nAttributeError: Transformer pipeline does not provide get_feature_names\n\n\nAs such, you would have to inspect the transformers one-by-one to figure out the 1518 column names, as shown previously in lesson¬†8.4, and then you could filter that list down to the 759 selected features using the boolean array.\nNote that starting in scikit-learn version 1.1, the get_feature_names_out method should work on this ColumnTransformer, since the get_feature_names_out method will be available for all transformers.",
    "crumbs": [
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Feature selection</span>"
    ]
  },
  {
    "objectID": "ch13.html#qa-are-the-selected-features-the-most-important-features",
    "href": "ch13.html#qa-are-the-selected-features-the-most-important-features",
    "title": "13¬† Feature selection",
    "section": "13.8 Q&A: Are the selected features the ‚Äúmost important‚Äù features?",
    "text": "13.8 Q&A: Are the selected features the ‚Äúmost important‚Äù features?\nAfter using any feature selection procedure, it‚Äôs hard to say whether the selected feature set is truly the best set of features, or whether there might be another feature set that performs just as well.\nFor example, one high-performing feature set might include feature A, while another high-performing feature set might exclude feature A but include features B and C which are highly correlated with A. This is especially likely any time the number of features is much greater than the number of samples.\nAs such, feature selection is not an optimal tool for determining feature importance.\n\n\n\n\n\n\nFeature selection vs feature importance:\n\nMultiple sets of features may perform similarly\nEspecially likely if there are many more features than samples (p &gt;&gt; n)\nThus, feature selection does not necessarily determine feature importance",
    "crumbs": [
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Feature selection</span>"
    ]
  },
  {
    "objectID": "ch13.html#qa-is-it-okay-for-feature-selection-to-remove-one-hot-encoded-categories",
    "href": "ch13.html#qa-is-it-okay-for-feature-selection-to-remove-one-hot-encoded-categories",
    "title": "13¬† Feature selection",
    "section": "13.9 Q&A: Is it okay for feature selection to remove one-hot encoded categories?",
    "text": "13.9 Q&A: Is it okay for feature selection to remove one-hot encoded categories?\nThe feature selection process examines each feature independently, which means that it does not know that there are groups of feature columns that originated from the same feature. As a result, feature selection might remove some of the columns that resulted from one-hot encoding a column, and keep others.\nHowever, I don‚Äôt see this as necessarily being problematic. Each one-hot encoded column can be thought of as independent from all others, since it merely represents the presence or absence of a particular value for a categorical column. If the presence or absence of ‚ÄúEmbarked from C‚Äù has a relationship with the target but the presence or absence of ‚ÄúEmbarked from Q‚Äù does not, then I would agree with one of those columns being removed while the other remains.\nThis is similar to how I think of the features output by CountVectorizer: Some text features have a relationship with the target and should be kept, while others do not have a relationship with the target and should be removed.\n\n\n\n\n\n\nFeature selection of one-hot encoded categories:\n\nFeature selection examines each feature column independently (regardless of its ‚Äúorigin‚Äù)\nEach one-hot encoded column is conceptually independent from the others\nThus, it‚Äôs acceptable for feature selection to ignore the origin of each column when removing features",
    "crumbs": [
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Feature selection</span>"
    ]
  },
  {
    "objectID": "ch14.html",
    "href": "ch14.html",
    "title": "14¬† Feature standardization",
    "section": "",
    "text": "14.1 Standardizing numerical features\nSome Machine Learning models benefit from a process called feature standardization. That‚Äôs because the objective function of some models assume that all features are centered around zero and have a variance of the same order of magnitude. If that assumption is incorrect, a given feature might dominate the objective function and the model won‚Äôt be able to learn from all of the features, reducing its performance.\nIn this chapter, we‚Äôll experiment with standardizing our features to see if that improves our model performance.\nWe‚Äôll start with the most common approach, which is to use StandardScaler and only standardize features that were originally numerical. We‚Äôll import it from the preprocessing module and create an instance called scaler using the default parameters. For each feature, it will subtract the mean and divide by the standard deviation, which centers the data around zero and scales it.\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nHere‚Äôs a reminder of our existing ColumnTransformer. Note that our numerical features are Age, Fare, and Parch.\nct = make_column_transformer(\n    (imp_ohe, ['Embarked', 'Sex']),\n    (vect, 'Name'),\n    (imp, ['Age', 'Fare']),\n    ('passthrough', ['Parch']))\nTo scale our numerical features, we‚Äôll make a Pipeline of imputation and scaling called imp_scaler.\nimp_scaler = make_pipeline(imp, scaler)\nThen we‚Äôll replace imp with imp_scaler in our ColumnTransformer, and apply it to Parch as well, which was previously a passthrough column.\nct = make_column_transformer(\n    (imp_ohe, ['Embarked', 'Sex']),\n    (vect, 'Name'),\n    (imp_scaler, ['Age', 'Fare', 'Parch']))\nFinally, we‚Äôll create a Pipeline called scaler_pipe using the updated ColumnTransformer.\nscaler_pipe = make_pipeline(ct, logreg)\nscaler_pipe\n\nPipelinePipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('pipeline-1',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer(fill_value='missing',\n                                                                                 strategy='constant')),\n                                                                  ('onehotencoder',\n                                                                   OneHotEncoder())]),\n                                                  ['Embarked', 'Sex']),\n                                                 ('countvectorizer',\n                                                  CountVectorizer(), 'Name'),\n                                                 ('pipeline-2',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer()),\n                                                                  ('standardscaler',\n                                                                   StandardScaler())]),\n                                                  ['Age', 'Fare', 'Parch'])])),\n                ('logisticregression',\n                 LogisticRegression(random_state=1, solver='liblinear'))])columntransformer: ColumnTransformerColumnTransformer(transformers=[('pipeline-1',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer(fill_value='missing',\n                                                                strategy='constant')),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder())]),\n                                 ['Embarked', 'Sex']),\n                                ('countvectorizer', CountVectorizer(), 'Name'),\n                                ('pipeline-2',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer()),\n                                                 ('standardscaler',\n                                                  StandardScaler())]),\n                                 ['Age', 'Fare', 'Parch'])])pipeline-1['Embarked', 'Sex']SimpleImputerSimpleImputer(fill_value='missing', strategy='constant')OneHotEncoderOneHotEncoder()countvectorizerNameCountVectorizerCountVectorizer()pipeline-2['Age', 'Fare', 'Parch']SimpleImputerSimpleImputer()StandardScalerStandardScaler()LogisticRegressionLogisticRegression(random_state=1, solver='liblinear')\nThe cross-validated accuracy of this Pipeline is 0.810, which is nearly the same as our baseline accuracy.\ncross_val_score(scaler_pipe, X, y, cv=5, scoring='accuracy').mean()\n\n0.8103383340656581\nThat might be surprising, because regularized linear models often benefit from feature standardization. However, our particular logistic regression solver (liblinear) happens to be robust to unscaled data, and thus there was no benefit in this case.\nThe takeaway here is that you shouldn‚Äôt always assume that standardization of numerical features is necessary.",
    "crumbs": [
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Feature standardization</span>"
    ]
  },
  {
    "objectID": "ch14.html#standardizing-numerical-features",
    "href": "ch14.html#standardizing-numerical-features",
    "title": "14¬† Feature standardization",
    "section": "",
    "text": "Why is feature standardization useful?\n\nSome models assume that features are centered around zero and have similar variances\nThose models may perform poorly if that assumption is incorrect\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPipeline accuracy scores:\n\nGrid search (VC): 0.834\nGrid search (LR with SelectFromModel ET): 0.832\nGrid search (RF): 0.829\nGrid search (LR): 0.828\nBaseline (VC): 0.818\nBaseline (LR): 0.811\nBaseline (RF): 0.811\nBaseline (LR with numerical features standardized): 0.810\n\n\n\n\n\n\n\n\n\n\n\n\nWhy didn‚Äôt feature standardization help?\n\nRegularized linear models often benefit from standardization\nHowever, the liblinear solver is robust to unscaled data",
    "crumbs": [
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Feature standardization</span>"
    ]
  },
  {
    "objectID": "ch14.html#standardizing-all-features",
    "href": "ch14.html#standardizing-all-features",
    "title": "14¬† Feature standardization",
    "section": "14.2 Standardizing all features",
    "text": "14.2 Standardizing all features\nIn the previous lesson, we standardized all of the numerical features. An alternative approach is to standardize all features after transformation, even if they were not originally numerical. That‚Äôs what we‚Äôll try in this lesson.\nOur strategy will be to add standardization as the second step in the Pipeline, in between the ColumnTransformer and the model. However, our ColumnTransformer outputs a sparse matrix, and StandardScaler would destroy the sparseness by centering the data, likely resulting in a memory issue.\n\n\n\n\n\n\nWhy not use StandardScaler?\n\nOur ColumnTransformer outputs a sparse matrix\nCentering would cause memory issues by creating a dense matrix\n\n\n\n\nThus, we‚Äôre going to use an alternative scaler called MaxAbsScaler. We‚Äôll import it from the preprocessing module and create an instance called scaler. MaxAbsScaler divides each feature by its maximum value, which scales each feature to the range -1 to 1. Zeros are never changed, and thus sparsity is preserved.\n\nfrom sklearn.preprocessing import MaxAbsScaler\nscaler = MaxAbsScaler()\n\nFirst, we‚Äôll reset our ColumnTransformer so that it doesn‚Äôt include the imp_scaler Pipeline.\n\nct = make_column_transformer(\n    (imp_ohe, ['Embarked', 'Sex']),\n    (vect, 'Name'),\n    (imp, ['Age', 'Fare']),\n    ('passthrough', ['Parch']))\n\nThen, we‚Äôll update the scaler_pipe object to use MaxAbsScaler as the second step.\n\nscaler_pipe = make_pipeline(ct, scaler, logreg)\nscaler_pipe\n\nPipelinePipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('pipeline',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer(fill_value='missing',\n                                                                                 strategy='constant')),\n                                                                  ('onehotencoder',\n                                                                   OneHotEncoder())]),\n                                                  ['Embarked', 'Sex']),\n                                                 ('countvectorizer',\n                                                  CountVectorizer(), 'Name'),\n                                                 ('simpleimputer',\n                                                  SimpleImputer(),\n                                                  ['Age', 'Fare']),\n                                                 ('passthrough', 'passthrough',\n                                                  ['Parch'])])),\n                ('maxabsscaler', MaxAbsScaler()),\n                ('logisticregression',\n                 LogisticRegression(random_state=1, solver='liblinear'))])columntransformer: ColumnTransformerColumnTransformer(transformers=[('pipeline',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer(fill_value='missing',\n                                                                strategy='constant')),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder())]),\n                                 ['Embarked', 'Sex']),\n                                ('countvectorizer', CountVectorizer(), 'Name'),\n                                ('simpleimputer', SimpleImputer(),\n                                 ['Age', 'Fare']),\n                                ('passthrough', 'passthrough', ['Parch'])])pipeline['Embarked', 'Sex']SimpleImputerSimpleImputer(fill_value='missing', strategy='constant')OneHotEncoderOneHotEncoder()countvectorizerNameCountVectorizerCountVectorizer()simpleimputer['Age', 'Fare']SimpleImputerSimpleImputer()passthrough['Parch']passthroughpassthroughMaxAbsScalerMaxAbsScaler()LogisticRegressionLogisticRegression(random_state=1, solver='liblinear')\n\n\nWhen we cross-validate it, the accuracy is 0.811, which is exactly the same as our baseline accuracy.\nI‚Äôm not surprised at this result, because MaxAbsScaler has no effect on the columns output by OneHotEncoder and only a tiny effect on the columns output by CountVectorizer, and thus our approach in this lesson is mostly just affecting the numerical columns, which is what we did in the previous lesson.\n\ncross_val_score(scaler_pipe, X, y, cv=5, scoring='accuracy').mean()\n\n0.8114556525014123\n\n\n\n\n\n\n\n\nPipeline accuracy scores:\n\nGrid search (VC): 0.834\nGrid search (LR with SelectFromModel ET): 0.832\nGrid search (RF): 0.829\nGrid search (LR): 0.828\nBaseline (VC): 0.818\nBaseline (LR): 0.811\nBaseline (LR with all features standardized): 0.811\nBaseline (RF): 0.811\nBaseline (LR with numerical features standardized): 0.810\n\n\n\n\nAlthough we didn‚Äôt see any benefits from standardization, there are certainly cases in which it will help. If you do try out standardization, my suggestion is to try out both of the approaches that we used in this chapter and use whichever one works better.",
    "crumbs": [
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Feature standardization</span>"
    ]
  },
  {
    "objectID": "ch14.html#qa-how-do-i-see-what-scaling-was-applied-to-each-feature",
    "href": "ch14.html#qa-how-do-i-see-what-scaling-was-applied-to-each-feature",
    "title": "14¬† Feature standardization",
    "section": "14.3 Q&A: How do I see what scaling was applied to each feature?",
    "text": "14.3 Q&A: How do I see what scaling was applied to each feature?\nIf you‚Äôre interested in seeing what scaling was applied to each feature, you can fit the Pipeline and then examine the scale_ attribute of the maxabsscaler step.\nFor example, the last three entries in the array correspond to the scaling for Age, Fare, and Parch. These are simply the maximum values of Age, Fare, and Parch in X. As a reminder, this is the scaling that will be applied to the features in X_new when making predictions.\n\nscaler_pipe.fit(X, y)\nscaler_pipe.named_steps['maxabsscaler'].scale_\n\narray([  1.    ,   1.    ,   1.    , ...,  80.    , 512.3292,   6.    ])\n\n\nAnd as you might expect, the scale_ attribute is also available when using StandardScaler.",
    "crumbs": [
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Feature standardization</span>"
    ]
  },
  {
    "objectID": "ch14.html#qa-how-do-i-turn-off-feature-standardization-within-a-grid-search",
    "href": "ch14.html#qa-how-do-i-turn-off-feature-standardization-within-a-grid-search",
    "title": "14¬† Feature standardization",
    "section": "14.4 Q&A: How do I turn off feature standardization within a grid search?",
    "text": "14.4 Q&A: How do I turn off feature standardization within a grid search?\nAlthough grid search is usually just used to tune parameter values, you can actually use grid search to turn on and off particular Pipeline steps. Thus you could use a grid search to decide whether or not feature standardization should be included.\nTo demonstrate this, let‚Äôs create a small dictionary called scaler_params.\nThe first entry tunes the C parameter of logistic regression, just like we‚Äôve done before. The dictionary key is the step name, then two underscores, then the parameter name. The dictionary values are the possible values for that parameter.\nThe second entry is different: Rather than tuning the parameter of a Pipeline step, we‚Äôre tuning the Pipeline step itself. In this case, the dictionary key is simply the step name assigned by make_pipeline. The possible values are 'passthrough', which means skip this Pipeline step, or a MaxAbsScaler instance, which means keep MaxAbsScaler in the Pipeline.\n\nscaler_params = {}\nscaler_params['logisticregression__C'] = [0.1, 1, 10]\nscaler_params['maxabsscaler'] = ['passthrough', MaxAbsScaler()]\n\nWe‚Äôll create scaler_grid using the scaler_pipe and scaler_params objects, and then run the grid search as normal.\n\nscaler_grid = GridSearchCV(scaler_pipe, scaler_params, cv=5,\n                           scoring='accuracy', n_jobs=-1)\nscaler_grid.fit(X, y)\n\nGridSearchCVGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('columntransformer',\n                                        ColumnTransformer(transformers=[('pipeline',\n                                                                         Pipeline(steps=[('simpleimputer',\n                                                                                          SimpleImputer(fill_value='missing',\n                                                                                                        strategy='constant')),\n                                                                                         ('onehotencoder',\n                                                                                          OneHotEncoder())]),\n                                                                         ['Embarked',\n                                                                          'Sex']),\n                                                                        ('countvectorizer',\n                                                                         CountVectorizer(),\n                                                                         'Name'),\n                                                                        ('simpleimputer',\n                                                                         SimpleImputer(),\n                                                                         ['Age',\n                                                                          'Fare']),\n                                                                        ('passthrough',\n                                                                         'passthrough',\n                                                                         ['Parch'])])),\n                                       ('maxabsscaler', MaxAbsScaler()),\n                                       ('logisticregression',\n                                        LogisticRegression(random_state=1,\n                                                           solver='liblinear'))]),\n             n_jobs=-1,\n             param_grid={'logisticregression__C': [0.1, 1, 10],\n                         'maxabsscaler': ['passthrough', MaxAbsScaler()]},\n             scoring='accuracy')columntransformer: ColumnTransformerColumnTransformer(transformers=[('pipeline',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer(fill_value='missing',\n                                                                strategy='constant')),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder())]),\n                                 ['Embarked', 'Sex']),\n                                ('countvectorizer', CountVectorizer(), 'Name'),\n                                ('simpleimputer', SimpleImputer(),\n                                 ['Age', 'Fare']),\n                                ('passthrough', 'passthrough', ['Parch'])])pipeline['Embarked', 'Sex']SimpleImputerSimpleImputer(fill_value='missing', strategy='constant')OneHotEncoderOneHotEncoder()countvectorizerNameCountVectorizerCountVectorizer()simpleimputer['Age', 'Fare']SimpleImputerSimpleImputer()passthrough['Parch']passthroughpassthroughMaxAbsScalerMaxAbsScaler()LogisticRegressionLogisticRegression(random_state=1, solver='liblinear')\n\n\nAs you can see in the results, each C value was tried once with the MaxAbsScaler and once without.\n\nresults = (pd.DataFrame(scaler_grid.cv_results_)\n           .filter(regex='param_|mean_test|rank'))\nresults.columns = results.columns.str.split('__').str[-1]\nresults\n\n\n\n\n\n\n\n\nC\nparam_maxabsscaler\nmean_test_score\nrank_test_score\n\n\n\n\n0\n0.1\npassthrough\n0.788990\n5\n\n\n1\n0.1\nMaxAbsScaler()\n0.788971\n6\n\n\n2\n1\npassthrough\n0.811462\n2\n\n\n3\n1\nMaxAbsScaler()\n0.811456\n3\n\n\n4\n10\npassthrough\n0.809234\n4\n\n\n5\n10\nMaxAbsScaler()\n0.819308\n1\n\n\n\n\n\n\n\nWhen using grid search with this limited set of parameters, the best results came from using a C value of 10 and including the MaxAbsScaler step.\n\nscaler_grid.best_params_\n\n{'logisticregression__C': 10, 'maxabsscaler': MaxAbsScaler()}",
    "crumbs": [
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Feature standardization</span>"
    ]
  },
  {
    "objectID": "ch14.html#qa-which-models-benefit-from-standardization",
    "href": "ch14.html#qa-which-models-benefit-from-standardization",
    "title": "14¬† Feature standardization",
    "section": "14.5 Q&A: Which models benefit from standardization?",
    "text": "14.5 Q&A: Which models benefit from standardization?\nFeature standardization tends to be useful any time a model considers the distance between features, such as K-Nearest Neighbors and Support Vector Machines.\nFeature standardization also tends to be useful for any models that incorporate regularization, such as a linear regression or logistic regression model with an L1 or L2 penalty, though we saw earlier in the chapter that this doesn‚Äôt apply to all solvers.\nNotably, feature standardization will not benefit any tree-based models such as random forests.\n\n\n\n\n\n\nWhen is feature standardization likely to be useful?\n\nUseful:\n\nDistance-based models (KNN, SVM)\nRegularized models (linear or logistic regression with L1/L2)\n\nNot useful:\n\nTree-based models (random forests)",
    "crumbs": [
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Feature standardization</span>"
    ]
  },
  {
    "objectID": "ch15.html",
    "href": "ch15.html",
    "title": "15¬† Feature engineering with custom transformers",
    "section": "",
    "text": "15.1 Why not use pandas for feature engineering?\nLet‚Äôs say that you need to create some custom features for your model. This is usually because you believe that your model could learn more from a particular feature if the feature was represented in a different way or combined with another feature.\nOften, feature engineering is done using pandas on the original dataset, and then the updated dataset is passed to scikit-learn. However, you can actually do feature engineering within scikit-learn using custom transformers, and in this chapter I‚Äôll show you how.\nIt‚Äôs a bit more work to do feature engineering within scikit-learn, but it provides some considerable benefits. All of your data transformations can be included in a Pipeline, which means they can be tuned using a grid search, they can be applied to new data without any extra work, and when done correctly, there‚Äôs no possibility of data leakage.",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Feature engineering with custom transformers</span>"
    ]
  },
  {
    "objectID": "ch15.html#why-not-use-pandas-for-feature-engineering",
    "href": "ch15.html#why-not-use-pandas-for-feature-engineering",
    "title": "15¬† Feature engineering with custom transformers",
    "section": "",
    "text": "Options for feature engineering:\n\npandas: Create features on original dataset, pass updated dataset to scikit-learn\nscikit-learn: Create features using custom transformers\n\nRequires more work\nAll transformations can be included in a Pipeline",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Feature engineering with custom transformers</span>"
    ]
  },
  {
    "objectID": "ch15.html#transformer-1-rounding-numerical-values",
    "href": "ch15.html#transformer-1-rounding-numerical-values",
    "title": "15¬† Feature engineering with custom transformers",
    "section": "15.2 Transformer 1: Rounding numerical values",
    "text": "15.2 Transformer 1: Rounding numerical values\nTo start, we‚Äôre going to redefine df as a 10-row dataset. This will make it much easier for you to see how custom transformers work.\n\ndf = pd.read_csv('http://bit.ly/MLtrain', nrows=10)\ndf\n\n\n\n\n\n\n\n\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n5\n0\n3\nMoran, Mr. James\nmale\nNaN\n0\n0\n330877\n8.4583\nNaN\nQ\n\n\n6\n0\n1\nMcCarthy, Mr. Timothy J\nmale\n54.0\n0\n0\n17463\n51.8625\nE46\nS\n\n\n7\n0\n3\nPalsson, Master. Gosta Leonard\nmale\n2.0\n3\n1\n349909\n21.0750\nNaN\nS\n\n\n8\n1\n3\nJohnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)\nfemale\n27.0\n0\n2\n347742\n11.1333\nNaN\nS\n\n\n9\n1\n2\nNasser, Mrs. Nicholas (Adele Achem)\nfemale\n14.0\n1\n0\n237736\n30.0708\nNaN\nC\n\n\n\n\n\n\n\nFirst, let‚Äôs pretend we believe Fare would be a better feature if we rounded it up to the next integer. We can do this using NumPy‚Äôs built-in ceil function, which is short for ceiling. Notice that each value in Fare has now been rounded up.\nIn general, NumPy functions are a good choice for custom transformations because both scikit-learn and pandas use NumPy under-the-hood.\nNotice that we passed it a 2D object, namely a one-column DataFrame, and it returned a 2D object, again a one-column DataFrame. This will turn out to be a useful characteristic for custom transformations, as you‚Äôll see later in this chapter.\n\nnp.ceil(df[['Fare']])\n\n\n\n\n\n\n\n\nFare\n\n\n\n\n0\n8.0\n\n\n1\n72.0\n\n\n2\n8.0\n\n\n3\n54.0\n\n\n4\n9.0\n\n\n5\n9.0\n\n\n6\n52.0\n\n\n7\n22.0\n\n\n8\n12.0\n\n\n9\n31.0\n\n\n\n\n\n\n\nIn order to do this transformation within scikit-learn, we need to convert the ceil function into a scikit-learn transformer using the FunctionTransformer class, which we‚Äôll import from the preprocessing module.\nWe simply pass the ceil function to FunctionTransformer, and it returns a transformer object, which we‚Äôll call ceiling. Note that if you‚Äôre using a version of scikit-learn prior to 0.22, you should also include the argument validate=False any time you‚Äôre using FunctionTransformer.\n\nfrom sklearn.preprocessing import FunctionTransformer\nceiling = FunctionTransformer(np.ceil)\n\nAnyway, because ceiling is a transformer, you can pass Fare to its fit_transform method, which performs the same transformation as before. This is the simplest example of feature engineering within scikit-learn.\n\nceiling.fit_transform(df[['Fare']])\n\n\n\n\n\n\n\n\nFare\n\n\n\n\n0\n8.0\n\n\n1\n72.0\n\n\n2\n8.0\n\n\n3\n54.0\n\n\n4\n9.0\n\n\n5\n9.0\n\n\n6\n52.0\n\n\n7\n22.0\n\n\n8\n12.0\n\n\n9\n31.0\n\n\n\n\n\n\n\nLike any transformer, ceiling can be included in a ColumnTransformer. For the moment, we‚Äôll create a ColumnTransformer instance that only includes the ceiling transformer, though we‚Äôll add more transformers throughout the chapter.\nFinally, we‚Äôll pass df to its fit_transform method, which confirms that it works. As we‚Äôve seen throughout this book, ColumnTransformer always outputs a NumPy array or a sparse matrix, and in this case it outputs a NumPy array.\n\nct = make_column_transformer(\n    (ceiling, ['Fare']))\nct.fit_transform(df)\n\narray([[ 8.],\n       [72.],\n       [ 8.],\n       [54.],\n       [ 9.],\n       [ 9.],\n       [52.],\n       [22.],\n       [12.],\n       [31.]])",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Feature engineering with custom transformers</span>"
    ]
  },
  {
    "objectID": "ch15.html#transformer-2-clipping-numerical-values",
    "href": "ch15.html#transformer-2-clipping-numerical-values",
    "title": "15¬† Feature engineering with custom transformers",
    "section": "15.3 Transformer 2: Clipping numerical values",
    "text": "15.3 Transformer 2: Clipping numerical values\nLet‚Äôs look at the 10-row dataset again.\n\ndf\n\n\n\n\n\n\n\n\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n5\n0\n3\nMoran, Mr. James\nmale\nNaN\n0\n0\n330877\n8.4583\nNaN\nQ\n\n\n6\n0\n1\nMcCarthy, Mr. Timothy J\nmale\n54.0\n0\n0\n17463\n51.8625\nE46\nS\n\n\n7\n0\n3\nPalsson, Master. Gosta Leonard\nmale\n2.0\n3\n1\n349909\n21.0750\nNaN\nS\n\n\n8\n1\n3\nJohnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)\nfemale\n27.0\n0\n2\n347742\n11.1333\nNaN\nS\n\n\n9\n1\n2\nNasser, Mrs. Nicholas (Adele Achem)\nfemale\n14.0\n1\n0\n237736\n30.0708\nNaN\nC\n\n\n\n\n\n\n\nFor our second transformation, let‚Äôs pretend we believe Age would be a better feature if we limited the values to the range 5 to 60, such that all values below 5 are rounded up to 5, and all values above 60 are rounded down to 60. We can do this using NumPy‚Äôs built-in clip function. Note that it has two required arguments, a_min and a_max, which define the limits.\nIn this case, the only value that changed was the row with index 7, in which a 2 became a 5.\n\nnp.clip(df[['Age']], a_min=5, a_max=60)\n\n\n\n\n\n\n\n\nAge\n\n\n\n\n0\n22.0\n\n\n1\n38.0\n\n\n2\n26.0\n\n\n3\n35.0\n\n\n4\n35.0\n\n\n5\nNaN\n\n\n6\n54.0\n\n\n7\n5.0\n\n\n8\n27.0\n\n\n9\n14.0\n\n\n\n\n\n\n\nWe‚Äôll convert NumPy‚Äôs clip function to a transformer called clip, though this time we need to pass the a_min and a_max arguments to the FunctionTransformer‚Äôs kw_args parameter.\n\nclip = FunctionTransformer(np.clip, kw_args={'a_min':5, 'a_max':60})\n\nWe‚Äôll check that the clip transformer works by running the fit_transform method and passing it the Age column.\n\nclip.fit_transform(df[['Age']])\n\n\n\n\n\n\n\n\nAge\n\n\n\n\n0\n22.0\n\n\n1\n38.0\n\n\n2\n26.0\n\n\n3\n35.0\n\n\n4\n35.0\n\n\n5\nNaN\n\n\n6\n54.0\n\n\n7\n5.0\n\n\n8\n27.0\n\n\n9\n14.0\n\n\n\n\n\n\n\nFinally, we‚Äôll add the clip transformer to the ColumnTransformer, and confirm that the ColumnTransformer still works as expected, which it does.\n\nct = make_column_transformer(\n    (ceiling, ['Fare']),\n    (clip, ['Age']))\nct.fit_transform(df)\n\narray([[ 8., 22.],\n       [72., 38.],\n       [ 8., 26.],\n       [54., 35.],\n       [ 9., 35.],\n       [ 9., nan],\n       [52., 54.],\n       [22.,  5.],\n       [12., 27.],\n       [31., 14.]])",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Feature engineering with custom transformers</span>"
    ]
  },
  {
    "objectID": "ch15.html#transformer-3-extracting-string-values",
    "href": "ch15.html#transformer-3-extracting-string-values",
    "title": "15¬† Feature engineering with custom transformers",
    "section": "15.4 Transformer 3: Extracting string values",
    "text": "15.4 Transformer 3: Extracting string values\nLet‚Äôs look at the dataset again.\n\ndf\n\n\n\n\n\n\n\n\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n5\n0\n3\nMoran, Mr. James\nmale\nNaN\n0\n0\n330877\n8.4583\nNaN\nQ\n\n\n6\n0\n1\nMcCarthy, Mr. Timothy J\nmale\n54.0\n0\n0\n17463\n51.8625\nE46\nS\n\n\n7\n0\n3\nPalsson, Master. Gosta Leonard\nmale\n2.0\n3\n1\n349909\n21.0750\nNaN\nS\n\n\n8\n1\n3\nJohnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)\nfemale\n27.0\n0\n2\n347742\n11.1333\nNaN\nS\n\n\n9\n1\n2\nNasser, Mrs. Nicholas (Adele Achem)\nfemale\n14.0\n1\n0\n237736\n30.0708\nNaN\nC\n\n\n\n\n\n\n\nFor our third transformation, let‚Äôs assume that the first letter of Cabin indicates the deck they were staying on, which we believe might be predictive. Our first thought might be to use the Series string slice method to extract the first character.\nThis works, but notice that the output is a 1D object, namely a pandas Series. This is problematic because a function (once transformed) must return 2D output in order to be used in a ColumnTransformer.\n\ndf['Cabin'].str.slice(0, 1)\n\n0    NaN\n1      C\n2    NaN\n3      C\n4    NaN\n5    NaN\n6      E\n7    NaN\n8    NaN\n9    NaN\nName: Cabin, dtype: object\n\n\nTo resolve this, we‚Äôll use the DataFrame apply method with a lambda. It still extracts the first character, but notice that it now returns 2D output. Also notice that it accepts 2D input, which means that it will be able to operate on multiple columns if we like.\n\ndf[['Cabin']].apply(lambda x: x.str.slice(0, 1))\n\n\n\n\n\n\n\n\nCabin\n\n\n\n\n0\nNaN\n\n\n1\nC\n\n\n2\nNaN\n\n\n3\nC\n\n\n4\nNaN\n\n\n5\nNaN\n\n\n6\nE\n\n\n7\nNaN\n\n\n8\nNaN\n\n\n9\nNaN\n\n\n\n\n\n\n\nSince this doesn‚Äôt already exist as a standalone function, our next step is to convert this operation into a custom function called first_letter. This function would work, but notice that it requires the input to be a pandas DataFrame, since apply is a DataFrame method. That may be problematic since ColumnTransformers accept both DataFrames and NumPy arrays as input. As such, it‚Äôs better to write a function that will work regardless of whether the input is a DataFrame or a NumPy array.\n\ndef first_letter(df):\n    return df.apply(lambda x: x.str.slice(0, 1))\n\nWe can revise the function to work with both DataFrames and NumPy arrays by converting the input to a DataFrame explicitly before using the apply method.\n\ndef first_letter(df):\n    return pd.DataFrame(df).apply(lambda x: x.str.slice(0, 1))\n\nWe‚Äôll check that the function works. Again, notice that it accepts 2D input and returns 2D output, which is what we want.\n\nfirst_letter(df[['Cabin']])\n\n\n\n\n\n\n\n\nCabin\n\n\n\n\n0\nNaN\n\n\n1\nC\n\n\n2\nNaN\n\n\n3\nC\n\n\n4\nNaN\n\n\n5\nNaN\n\n\n6\nE\n\n\n7\nNaN\n\n\n8\nNaN\n\n\n9\nNaN\n\n\n\n\n\n\n\nThen, we‚Äôll convert the function to a transformer called letter, and check that it works.\n\nletter = FunctionTransformer(first_letter)\nletter.fit_transform(df[['Cabin']])\n\n\n\n\n\n\n\n\nCabin\n\n\n\n\n0\nNaN\n\n\n1\nC\n\n\n2\nNaN\n\n\n3\nC\n\n\n4\nNaN\n\n\n5\nNaN\n\n\n6\nE\n\n\n7\nNaN\n\n\n8\nNaN\n\n\n9\nNaN\n\n\n\n\n\n\n\nFinally, we‚Äôll add the letter transformer to the ColumnTransformer, and check that it works, which it does.\n\nct = make_column_transformer(\n    (ceiling, ['Fare']),\n    (clip, ['Age']),\n    (letter, ['Cabin']))\nct.fit_transform(df)\n\narray([[8.0, 22.0, nan],\n       [72.0, 38.0, 'C'],\n       [8.0, 26.0, nan],\n       [54.0, 35.0, 'C'],\n       [9.0, 35.0, nan],\n       [9.0, nan, nan],\n       [52.0, 54.0, 'E'],\n       [22.0, 5.0, nan],\n       [12.0, 27.0, nan],\n       [31.0, 14.0, nan]], dtype=object)",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Feature engineering with custom transformers</span>"
    ]
  },
  {
    "objectID": "ch15.html#rules-for-transformer-functions",
    "href": "ch15.html#rules-for-transformer-functions",
    "title": "15¬† Feature engineering with custom transformers",
    "section": "15.5 Rules for transformer functions",
    "text": "15.5 Rules for transformer functions\nWe‚Äôve been talking about input and output shapes, so let me summarize the rules for functions that will be used in a ColumnTransformer:\nFirst, your function isn‚Äôt required to accept 2D input, but it‚Äôs better if it accepts 2D input. This enables your function (once transformed) to accept multiple columns in the ColumnTransformer.\nSecond, your function is required to return 2D output in order to be used in a ColumnTransformer. Thus if your function returns a pandas object, it should return a DataFrame (not a Series). And if your function returns a 1D array, you should reshape the array to be 2D before returning it. We‚Äôll see an example of this in the next lesson.\n\n\n\n\n\n\nInput and output of transformer functions:\n\nInput:\n\n1D is allowed\n2D is preferred (can accept multiple columns)\n\nOutput:\n\n2D is required",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Feature engineering with custom transformers</span>"
    ]
  },
  {
    "objectID": "ch15.html#transformer-4-combining-two-features",
    "href": "ch15.html#transformer-4-combining-two-features",
    "title": "15¬† Feature engineering with custom transformers",
    "section": "15.6 Transformer 4: Combining two features",
    "text": "15.6 Transformer 4: Combining two features\nLet‚Äôs look at the dataset one more time.\n\ndf\n\n\n\n\n\n\n\n\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n5\n0\n3\nMoran, Mr. James\nmale\nNaN\n0\n0\n330877\n8.4583\nNaN\nQ\n\n\n6\n0\n1\nMcCarthy, Mr. Timothy J\nmale\n54.0\n0\n0\n17463\n51.8625\nE46\nS\n\n\n7\n0\n3\nPalsson, Master. Gosta Leonard\nmale\n2.0\n3\n1\n349909\n21.0750\nNaN\nS\n\n\n8\n1\n3\nJohnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)\nfemale\n27.0\n0\n2\n347742\n11.1333\nNaN\nS\n\n\n9\n1\n2\nNasser, Mrs. Nicholas (Adele Achem)\nfemale\n14.0\n1\n0\n237736\n30.0708\nNaN\nC\n\n\n\n\n\n\n\nFor our fourth transformation, let‚Äôs say we believe that a passenger‚Äôs total number of family members aboard, meaning SibSp plus Parch, is more predictive than either feature individually.\nTo create this feature, we can use the DataFrame‚Äôs sum method over the 1 axis. However, this outputs a 1D object, which will not work with a ColumnTransformer.\n\ndf[['SibSp', 'Parch']].sum(axis=1)\n\n0    1\n1    1\n2    0\n3    1\n4    0\n5    0\n6    0\n7    4\n8    2\n9    1\ndtype: int64\n\n\nTo resolve this issue, we can use NumPy‚Äôs reshape method. Since it‚Äôs a NumPy method, we need to first convert the DataFrame into a NumPy array, then use NumPy‚Äôs sum method, then chain the reshape method on the end to convert the output into a 2D object. If you‚Äôre not familiar with the reshape method, this notation specifies that the second dimension should be 1 and the first dimension should be inferred.\n\nnp.array(df[['SibSp', 'Parch']]).sum(axis=1).reshape(-1, 1)\n\narray([[1],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [4],\n       [2],\n       [1]])\n\n\nNext, we‚Äôll convert this operation into a function called sum_cols, and this function will work regardless of whether the input is a DataFrame or a NumPy array.\n\ndef sum_cols(df):\n    return np.array(df).sum(axis=1).reshape(-1, 1)\n\nWe‚Äôll check that the function works.\n\nsum_cols(df[['SibSp', 'Parch']])\n\narray([[1],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [4],\n       [2],\n       [1]])\n\n\nSince the function works, we‚Äôll convert the function to a transformer called total, and check that it also works.\n\ntotal = FunctionTransformer(sum_cols)\ntotal.fit_transform(df[['SibSp', 'Parch']])\n\narray([[1],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [4],\n       [2],\n       [1]])\n\n\nFinally, we‚Äôll add the total transformer to the ColumnTransformer, specify that it should be applied to the SibSp and Parch columns, and then check that it all works together, which it does.\n\nct = make_column_transformer(\n    (ceiling, ['Fare']),\n    (clip, ['Age']),\n    (letter, ['Cabin']),\n    (total, ['SibSp', 'Parch']))\nct.fit_transform(df)\n\narray([[8.0, 22.0, nan, 1],\n       [72.0, 38.0, 'C', 1],\n       [8.0, 26.0, nan, 0],\n       [54.0, 35.0, 'C', 1],\n       [9.0, 35.0, nan, 0],\n       [9.0, nan, nan, 0],\n       [52.0, 54.0, 'E', 0],\n       [22.0, 5.0, nan, 4],\n       [12.0, 27.0, nan, 2],\n       [31.0, 14.0, nan, 1]], dtype=object)",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Feature engineering with custom transformers</span>"
    ]
  },
  {
    "objectID": "ch15.html#sec-15-7",
    "href": "ch15.html#sec-15-7",
    "title": "15¬† Feature engineering with custom transformers",
    "section": "15.7 Revising the transformers",
    "text": "15.7 Revising the transformers\nAt this point, we‚Äôve built four custom transformers and tested them out on 10 rows. Now, we want to apply them to our entire dataset, along with all of our other transformations.\nBefore we start, let‚Äôs review our original ColumnTransformer.\n\nct = make_column_transformer(\n    (imp_ohe, ['Embarked', 'Sex']),\n    (vect, 'Name'),\n    (imp, ['Age', 'Fare']),\n    ('passthrough', ['Parch']))\n\nLet‚Äôs compare that with the ColumnTransformer with our custom transformations.\n\nct = make_column_transformer(\n    (ceiling, ['Fare']),\n    (clip, ['Age']),\n    (letter, ['Cabin']),\n    (total, ['SibSp', 'Parch']))\n\nIn order to combine the custom transformations with our original transformations, there are a few things we‚Äôll need to handle:\n\nFirst, Cabin and SibSp weren‚Äôt used in our original ColumnTransformer.\nSecond, Fare and Age both have missing values.\nThird, the first letter of Cabin is obviously non-numeric, and it has missing values.\n\n\n\n\n\n\n\nIssues to handle when updating the ColumnTransformer:\n\nCabin and SibSp weren‚Äôt originally included\nFare and Age have missing values\nCabin is non-numeric and has missing values\n\n\n\n\nTo start, we‚Äôll define df as the entire dataset.\n\ndf = pd.read_csv('http://bit.ly/MLtrain')\n\nThen, we‚Äôll add Cabin and SibSp to the list of columns, and update X and X_new to include these columns. That solves issue number one.\n\ncols = ['Parch', 'Fare', 'Embarked', 'Sex', 'Name', 'Age', 'Cabin',\n        'SibSp']\nX = df[cols]\nX_new = df_new[cols]\n\nNext, let‚Äôs handle issue number two, which is the missing values in Fare and Age. Remember how we did imputation before one-hot encoding for Embarked and Sex? We‚Äôll do something similar for Fare and Age:\n\nFor Fare, we‚Äôll create a Pipeline called imp_ceiling that does imputation before ceiling.\nFor Age, we‚Äôll create a Pipeline called imp_clip that does imputation before clipping.\n\n\nimp_ceiling = make_pipeline(imp, ceiling)\nimp_clip = make_pipeline(imp, clip)\n\nFinally, we‚Äôll tackle the most complicated issue, which is issue number three. Let‚Äôs slice the first letter of Cabin using the string slice method and then take the value_counts of the result to see why:\n\nFirst, it contains missing values, so imputation will be required.\nSecond, letters are strings, so one-hot encoding will be required.\n\n\nX['Cabin'].str.slice(0, 1).value_counts(dropna=False)\n\nNaN    687\nC       59\nB       47\nD       33\nE       32\nA       15\nF       13\nG        4\nT        1\nName: Cabin, dtype: int64\n\n\nIn addition, notice that the G and T categories are quite rare, which can cause problems with cross-validation. Why would this be?\nFor any rare category, it‚Äôs possible for all values of that category to show up in the same testing fold during cross-validation. If that happens, the rare category won‚Äôt be learned by the OneHotEncoder during the fit step, and will be treated as an unknown category during the transform step. By default, the OneHotEncoder will error when it encounters an unknown category, and thus cross-validation will also throw an error.\n\n\n\n\n\n\nWhy are rare categories problematic for cross-validation?\n\nRare category values may all show up in the same testing fold\nThe rare category won‚Äôt be learned during fit and will be treated as an unknown category\nOneHotEncoder will error when it encounters an unknown category\n\n\n\n\nAlthough the problem is complicated, the solution is simple, which is to set the handle_unknown parameter to 'ignore', which we learned about back in lesson¬†3.5.\n\nohe_ignore = OneHotEncoder(handle_unknown='ignore')\n\nTo resolve all of these issues with the Cabin column, we‚Äôll create a three-step Pipeline. Step 1 is the letter transformer, which extracts the first letter. Step 2 is imp_constant, which imputes the constant value ‚Äúmissing‚Äù. Step 3 is ohe_ignore, which one-hot encodes the results.\n\nletter_imp_ohe = make_pipeline(letter, imp_constant, ohe_ignore)\n\nNow we‚Äôre finally ready to update our primary ColumnTransformer:\n\nEmbarked, Sex, and Name are transformed exactly as they were previously.\nFor Fare, we‚Äôll use imp_ceiling instead of imp.\nFor Age, we‚Äôll use imp_clip instead of imp.\nFor Cabin, we‚Äôll use letter_imp_ohe.\nFor SibSp and Parch, we‚Äôll use total.\n\n\nct = make_column_transformer(\n    (imp_ohe, ['Embarked', 'Sex']),\n    (vect, 'Name'),\n    (imp_ceiling, ['Fare']),\n    (imp_clip, ['Age']),\n    (letter_imp_ohe, ['Cabin']),\n    (total, ['SibSp', 'Parch']))\n\nWe‚Äôll check that it works by passing X to the fit_transform method.\n\nct.fit_transform(X)\n\n&lt;891x1527 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n    with 8360 stored elements in Compressed Sparse Row format&gt;\n\n\nThen we‚Äôll update the Pipeline to use the new ColumnTransformer.\n\npipe = make_pipeline(ct, logreg)\npipe\n\nPipelinePipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('pipeline-1',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer(fill_value='missing',\n                                                                                 strategy='constant')),\n                                                                  ('onehotencoder',\n                                                                   OneHotEncoder())]),\n                                                  ['Embarked', 'Sex']),\n                                                 ('countvectorizer',\n                                                  CountVectorizer(), 'Name'),\n                                                 ('pipeline-2',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer()),\n                                                                  ('functiontr...\n                                                                   FunctionTransformer(func=)),\n                                                                  ('simpleimputer',\n                                                                   SimpleImputer(fill_value='missing',\n                                                                                 strategy='constant')),\n                                                                  ('onehotencoder',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  ['Cabin']),\n                                                 ('functiontransformer',\n                                                  FunctionTransformer(func=),\n                                                  ['SibSp', 'Parch'])])),\n                ('logisticregression',\n                 LogisticRegression(random_state=1, solver='liblinear'))])columntransformer: ColumnTransformerColumnTransformer(transformers=[('pipeline-1',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer(fill_value='missing',\n                                                                strategy='constant')),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder())]),\n                                 ['Embarked', 'Sex']),\n                                ('countvectorizer', CountVectorizer(), 'Name'),\n                                ('pipeline-2',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer()),\n                                                 ('functiontransformer',\n                                                  FunctionTransformer(func=&lt;...\n                                ('pipeline-4',\n                                 Pipeline(steps=[('functiontransformer',\n                                                  FunctionTransformer(func=)),\n                                                 ('simpleimputer',\n                                                  SimpleImputer(fill_value='missing',\n                                                                strategy='constant')),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder(handle_unknown='ignore'))]),\n                                 ['Cabin']),\n                                ('functiontransformer',\n                                 FunctionTransformer(func=),\n                                 ['SibSp', 'Parch'])])pipeline-1['Embarked', 'Sex']SimpleImputerSimpleImputer(fill_value='missing', strategy='constant')OneHotEncoderOneHotEncoder()countvectorizerNameCountVectorizerCountVectorizer()pipeline-2['Fare']SimpleImputerSimpleImputer()FunctionTransformerFunctionTransformer(func=)pipeline-3['Age']SimpleImputerSimpleImputer()FunctionTransformerFunctionTransformer(func=,\n                    kw_args={'a_max': 60, 'a_min': 5})pipeline-4['Cabin']FunctionTransformerFunctionTransformer(func=)SimpleImputerSimpleImputer(fill_value='missing', strategy='constant')OneHotEncoderOneHotEncoder(handle_unknown='ignore')functiontransformer['SibSp', 'Parch']FunctionTransformerFunctionTransformer(func=)LogisticRegressionLogisticRegression(random_state=1, solver='liblinear')\n\n\nWhen we cross-validate the Pipeline, the accuracy is 0.827, which is higher than our baseline accuracy of 0.811. And it‚Äôs very likely that its accuracy could be further improved through hyperparameter tuning.\n\ncross_val_score(pipe, X, y, cv=5, scoring='accuracy').mean()\n\n0.8271483271608812\n\n\n\n\n\n\n\n\nPipeline accuracy scores:\n\nGrid search (VC): 0.834\nGrid search (LR with SelectFromModel ET): 0.832\nGrid search (RF): 0.829\nGrid search (LR): 0.828\nBaseline (LR with more features): 0.827\nBaseline (VC): 0.818\nBaseline (LR): 0.811\nBaseline (RF): 0.811\n\n\n\n\nFinally, we‚Äôll fit the Pipeline to X and y and use it to make predictions for X_new.\n\npipe.fit(X, y)\npipe.predict(X_new)\n\narray([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,\n       1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n       1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,\n       1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n       1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n       0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n       0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n       1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,\n       0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n       1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n       1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n       0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,\n       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,\n       0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n       1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,\n       0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n       1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,\n       0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1])",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Feature engineering with custom transformers</span>"
    ]
  },
  {
    "objectID": "ch15.html#qa-how-do-i-fix-incorrect-data-types-within-a-pipeline",
    "href": "ch15.html#qa-how-do-i-fix-incorrect-data-types-within-a-pipeline",
    "title": "15¬† Feature engineering with custom transformers",
    "section": "15.8 Q&A: How do I fix incorrect data types within a Pipeline?",
    "text": "15.8 Q&A: How do I fix incorrect data types within a Pipeline?\nLet‚Äôs say that you have the following DataFrame.\n\ndemo = pd.DataFrame({'A': ['10', '20', '30'],\n                     'B': ['40', '50', '60'],\n                     'C': [70, 80, 90],\n                     'D': ['x', 'y', 'z']})\ndemo\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\n10\n40\n70\nx\n\n\n1\n20\n50\n80\ny\n\n\n2\n30\n60\n90\nz\n\n\n\n\n\n\n\nIt may look like the first three columns are all integers, but columns A and B are actually object columns because the numbers are being stored as strings, which is a common problem in datasets.\n\ndemo.dtypes\n\nA    object\nB    object\nC     int64\nD    object\ndtype: object\n\n\nThese data types need to be fixed in order for a scikit-learn model to understand them. Within pandas, you can do this using the DataFrame method astype.\n\ndemo[['A', 'B']].astype('int')\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n10\n40\n\n\n1\n20\n50\n\n\n2\n30\n60\n\n\n\n\n\n\n\nAs you can see, this changes the data types of columns A and B to integer.\n\ndemo[['A', 'B']].astype('int').dtypes\n\nA    int64\nB    int64\ndtype: object\n\n\nTo incorporate this into a scikit-learn Pipeline, we would first define a custom function called make_integer that converts DataFrame columns to integers.\n\ndef make_integer(df):\n    return pd.DataFrame(df).astype('int')\n\nThen, we can convert the function to a transformer called integer, and check that it works.\n\ninteger = FunctionTransformer(make_integer)\ninteger.fit_transform(demo[['A', 'B']])\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n10\n40\n\n\n1\n20\n50\n\n\n2\n30\n60\n\n\n\n\n\n\n\nAnd you can see that the data types are now integer.\n\ninteger.fit_transform(demo[['A', 'B']]).dtypes\n\nA    int64\nB    int64\ndtype: object\n\n\nWhat I‚Äôve shown so far is a reasonable strategy, and it will work in many cases. However, let‚Äôs modify the demo DataFrame slightly.\n\ndemo.loc[2, 'B'] = ''\n\nAs you can see, the value 60 has been replaced by an empty string.\n\ndemo\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\n10\n40\n70\nx\n\n\n1\n20\n50\n80\ny\n\n\n2\n30\n\n90\nz\n\n\n\n\n\n\n\nIf we try to use our integer transformer on the revised DataFrame, it will error because the astype method doesn‚Äôt know how to handle an empty string.\n\ninteger.fit_transform(demo[['A', 'B']])\n\n\n\nValueError: invalid literal for int() with base 10: ''\n\n\nAn alternative strategy is to use the pandas function to_numeric and apply it to each of the columns. As you can see, to_numeric replaced the empty string with NaN.\n\ndemo[['A', 'B']].apply(pd.to_numeric)\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n10\n40.0\n\n\n1\n20\n50.0\n\n\n2\n30\nNaN\n\n\n\n\n\n\n\nColumn A is now an integer column, and column B is now a float column since integer columns don‚Äôt currently support NumPy‚Äôs NaN value.\n\ndemo[['A', 'B']].apply(pd.to_numeric).dtypes\n\nA      int64\nB    float64\ndtype: object\n\n\nIf we wanted to use this in the Pipeline, we would create a custom function called make_number.\n\ndef make_number(df):\n    return pd.DataFrame(df).apply(pd.to_numeric)\n\nThen we would convert the function to a transformer called number, and check that it works.\n\nnumber = FunctionTransformer(make_number)\nnumber.fit_transform(demo[['A', 'B']])\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n10\n40.0\n\n\n1\n20\n50.0\n\n\n2\n30\nNaN\n\n\n\n\n\n\n\nYou could use either the integer or number transformers in a ColumnTransformer, and both would allow you to fix data types within a Pipeline. However, there are some advantages to using the number transformer, since the to_numeric function includes a few different options for error handling, and it will handle the conversion of both integers and floating point numbers.",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Feature engineering with custom transformers</span>"
    ]
  },
  {
    "objectID": "ch15.html#qa-how-do-i-create-features-from-datetime-data",
    "href": "ch15.html#qa-how-do-i-create-features-from-datetime-data",
    "title": "15¬† Feature engineering with custom transformers",
    "section": "15.9 Q&A: How do I create features from datetime data?",
    "text": "15.9 Q&A: How do I create features from datetime data?\nTo demonstrate how to create date-based features, I‚Äôm going to read a tiny dataset of reported UFO sightings into a DataFrame.\n\nufo = pd.read_csv('http://bit.ly/ufosample', parse_dates=['Date'])\nufo\n\n\n\n\n\n\n\n\nDate\nCity\nState\nShape\n\n\n\n\n0\n2023-08-04\nLexington\nSC\nCylinder\n\n\n1\n2023-08-04\nAlpharetta\nGA\nOrb\n\n\n2\n2023-08-05\nBaltimore\nMD\nLight\n\n\n3\n2023-08-05\nHelena\nMT\nRectangle\n\n\n4\n2023-08-06\nWilmington\nNC\nFireball\n\n\n5\n2023-08-06\nBrooklyn\nNY\nStar\n\n\n\n\n\n\n\nBecause we parsed the Date column during file reading, it uses the special datetime data type.\n\nufo.dtypes\n\nDate     datetime64[ns]\nCity             object\nState            object\nShape            object\ndtype: object\n\n\nBecause of the datetime data type, we can access various properties of the Date column using the dt accessor. For example, we can easily access the day of the month using the day attribute.\n\nufo['Date'].dt.day\n\n0    4\n1    4\n2    5\n3    5\n4    6\n5    6\nName: Date, dtype: int64\n\n\nLet‚Äôs say that you wanted to use day of month as a feature. The first step would be to create a custom function like we‚Äôve done previously, and we‚Äôll call it day_of_month.\n\ndef day_of_month(df):\n    return df.apply(lambda x: x.dt.day)\n\nWe can see that this function works. However, it‚Äôs not quite optimal because it assumes that the data structure being passed in is a DataFrame with the datetime data type.\n\nday_of_month(ufo[['Date']])\n\n\n\n\n\n\n\n\nDate\n\n\n\n\n0\n4\n\n\n1\n4\n\n\n2\n5\n\n\n3\n5\n\n\n4\n6\n\n\n5\n6\n\n\n\n\n\n\n\nLet‚Äôs make the function more robust by enabling it to handle NumPy arrays and by doing the datetime conversion within the function.\nHere‚Äôs one option. As you can see, it converts the object to a DataFrame, and then it converts each column to datetime format before accessing the day attribute.\n\ndef day_of_month(df):\n    return pd.DataFrame(df).apply(lambda x: pd.to_datetime(x).dt.day)\n\nAnother option is to convert all columns to datetime format within the DataFrame constructor rather than during the apply method.\n\ndef day_of_month(df):\n    return pd.DataFrame(df, dtype=np.datetime64).apply(lambda x:\n                                                       x.dt.day)\n\nTo check that it works, we‚Äôll read in the UFO dataset again, but this time we‚Äôll leave the Date column as an object column.\n\nufo = pd.read_csv('http://bit.ly/ufosample')\nufo.dtypes\n\nDate     object\nCity     object\nState    object\nShape    object\ndtype: object\n\n\nWe can see that the day_of_month function works, even though Date is an object column.\n\nday_of_month(ufo[['Date']])\n\n\n\n\n\n\n\n\nDate\n\n\n\n\n0\n4\n\n\n1\n4\n\n\n2\n5\n\n\n3\n5\n\n\n4\n6\n\n\n5\n6\n\n\n\n\n\n\n\nFinally, we‚Äôll convert the day_of_month function into a transformer called day, and check that it works as well.\n\nday = FunctionTransformer(day_of_month)\nday.fit_transform(ufo[['Date']])\n\n\n\n\n\n\n\n\nDate\n\n\n\n\n0\n4\n\n\n1\n4\n\n\n2\n5\n\n\n3\n5\n\n\n4\n6\n\n\n5\n6",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Feature engineering with custom transformers</span>"
    ]
  },
  {
    "objectID": "ch15.html#qa-how-do-i-create-feature-interactions",
    "href": "ch15.html#qa-how-do-i-create-feature-interactions",
    "title": "15¬† Feature engineering with custom transformers",
    "section": "15.10 Q&A: How do I create feature interactions?",
    "text": "15.10 Q&A: How do I create feature interactions?\nWhen you believe there‚Äôs an interaction between two or more features, one common technique is to create ‚Äúinteraction terms‚Äù or ‚Äúfeature interactions‚Äù that your model can learn from. This is generally done by multiplying the values of each pair of features, and using those as new features.\nCreating interaction features is useful when the combined impact of a pair of features is different from the impact of the features when considered independently. For example, let‚Äôs pretend that features A and B each have a small positive impact on the target, but when combined, they have a much larger positive impact on the target than you would expect. In that case, it would be useful to create the interaction feature of A times B.\n\n\n\n\n\n\nWhen are feature interactions useful?\n\nWhen the combined impact of features is different from their independent impacts\nExample:\n\nA and B (individually) each have a small positive impact\nA and B (combined) has a larger positive impact than expected\n\n\n\n\n\nLet‚Äôs see how we can create feature interactions in scikit-learn. We‚Äôll assume that we‚Äôve decided to create interactions between Fare, SibSp, and Parch. Here are the first three rows and last three rows of each of those features.\n\nX[['Fare', 'SibSp', 'Parch']].to_numpy()\n\narray([[ 7.25  ,  1.    ,  0.    ],\n       [71.2833,  1.    ,  0.    ],\n       [ 7.925 ,  0.    ,  0.    ],\n       ...,\n       [23.45  ,  1.    ,  2.    ],\n       [30.    ,  0.    ,  0.    ],\n       [ 7.75  ,  0.    ,  0.    ]])\n\n\nOur first step is to import the PolynomialFeatures class from the preprocessing module, and then create an instance called poly. We‚Äôll set the include_bias parameter to False to avoid creating a column of ones in the result, and we‚Äôll set the interaction_only parameter to True to avoid creating the square of each feature.\n\nfrom sklearn.preprocessing import PolynomialFeatures\npoly = PolynomialFeatures(include_bias=False, interaction_only=True)\n\nWhen we run the fit_transform method and pass it those three columns, it outputs six columns:\n\nThe first three columns of the output are the original three columns: Fare, SibSp, and Parch.\nThe next three columns are our interaction terms: Fare times SibSp, Fare times Parch, and SibSp times Parch.\n\n\npoly.fit_transform(X[['Fare', 'SibSp', 'Parch']])\n\narray([[ 7.25  ,  1.    ,  0.    ,  7.25  ,  0.    ,  0.    ],\n       [71.2833,  1.    ,  0.    , 71.2833,  0.    ,  0.    ],\n       [ 7.925 ,  0.    ,  0.    ,  0.    ,  0.    ,  0.    ],\n       ...,\n       [23.45  ,  1.    ,  2.    , 23.45  , 46.9   ,  2.    ],\n       [30.    ,  0.    ,  0.    ,  0.    ,  0.    ,  0.    ],\n       [ 7.75  ,  0.    ,  0.    ,  0.    ,  0.    ,  0.    ]])\n\n\n\n\n\n\n\n\nOutput columns:\n\nFare\nSibSp\nParch\nFare * SibSp\nFare * Parch\nSibSp * Parch\n\n\n\n\nIf we wanted to include these feature interactions in our model, we would simply include poly as one of the transformers in our ColumnTransformer.\nOne obvious question is: How should you choose which feature interactions to create?\n\nIdeally, you would use expert knowledge of the system in question to guide your decision of which interactions to create.\nIf that‚Äôs not available, then you can explore the data to decide which interactions to create.\nFinally, if you have a small number of features, then you could simply create all possible interactions and then use feature selection to remove the interactions which are not useful.\n\nWith a large number of features, it‚Äôs simply not practical to create all possible interactions, plus you run an increased risk of false positive feature interactions. These are interactions which appear to have a relationship with the target, but that relationship is actually just occurring due to random chance.\n\n\n\n\n\n\nHow to choose feature interactions:\n\nUse expert knowledge\nExplore the data\nCreate all possible interactions\n\nNot practical with a large number of features\nIncreases risk of false positives\n\n\n\n\n\nAnother point worth noting is that tree-based models can learn feature interactions on their own through recursive splitting. That means that if you‚Äôre using a tree-based model as your prediction model, then you don‚Äôt need to manually create feature interactions.\nFinally, it‚Äôs worth noting that even though linear models can‚Äôt explicitly learn feature interactions, they can sometimes replace the information supplied by the interaction terms, in which case the interaction terms are unnecessary. As such, you should always evaluate the model with interaction terms against the model without interaction terms, and only include the interaction terms if they‚Äôre improving the model‚Äôs performance.\n\n\n\n\n\n\nWhen are feature interactions not useful?\n\nTree-based models can learn feature interactions on their own\nLinear models can sometimes replace the information supplied by interaction terms\nConclusion: Evaluate the model with and without interaction terms",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Feature engineering with custom transformers</span>"
    ]
  },
  {
    "objectID": "ch15.html#qa-how-do-i-save-a-pipeline-with-custom-transformers",
    "href": "ch15.html#qa-how-do-i-save-a-pipeline-with-custom-transformers",
    "title": "15¬† Feature engineering with custom transformers",
    "section": "15.11 Q&A: How do I save a Pipeline with custom transformers?",
    "text": "15.11 Q&A: How do I save a Pipeline with custom transformers?\nIf you save a Pipeline using pickle or joblib, and the Pipeline includes custom transformers, then the saved Pipeline can only be loaded into a new environment if the functions it depends upon are defined in the new environment.\nFor example, let‚Äôs import pickle and use it to save our current Pipeline.\n\nimport pickle\nwith open('pipe.pickle', 'wb') as f:\n    pickle.dump(pipe, f)\n\nLet‚Äôs pretend that we‚Äôre in a brand new environment and we wanted to make predictions for X_new using our saved Pipeline. Because the Pipeline includes custom transformers which use the first_letter and sum_cols functions, those two functions need to be defined in the new environment.\n\ndef first_letter(df):\n    return pd.DataFrame(df).apply(lambda x: x.str.slice(0, 1))\n\n\ndef sum_cols(df):\n    return np.array(df).sum(axis=1).reshape(-1, 1)\n\nAnd because those functions depend on pandas and NumPy, then these libraries also need to be imported in the new environment.\n\nimport pandas as pd\nimport numpy as np\n\nNow we can load our saved Pipeline into the pipe_from_pickle object.\n\nwith open('pipe.pickle', 'rb') as f:\n    pipe_from_pickle = pickle.load(f)\n\nWe also need to create the X_new object in our environment.\n\ncols = ['Parch', 'Fare', 'Embarked', 'Sex', 'Name', 'Age', 'Cabin',\n        'SibSp']\ndf_new = pd.read_csv('http://bit.ly/MLnewdata')\nX_new = df_new[cols]\n\nFinally, we can make predictions using the saved Pipeline.\n\npipe_from_pickle.predict(X_new)\n\narray([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,\n       1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n       1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,\n       1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n       1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n       0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n       0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n       1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,\n       0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n       1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n       1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n       0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,\n       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,\n       0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n       1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,\n       0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n       1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,\n       0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1])\n\n\nIf that process seems too burdensome, you can actually simplify the process by using a Python library called cloudpickle. cloudpickle extends the functionality of pickle to allow you to save user-defined functions.\nAll you have to do is to install cloudpickle using pip or conda, import it, and then save the Pipeline using cloudpickle instead of pickle. Notice that the cloudpickle code is exactly the same as the pickle code, except you use the dump function from cloudpickle instead of from pickle.\n\nimport cloudpickle\nwith open('pipe.pickle', 'wb') as f:\n    cloudpickle.dump(pipe, f)\n\nThen, in your new environment, you‚Äôll be able to load the saved Pipeline using pickle and use it to make predictions without having to define the custom functions in that environment.\n\nwith open('pipe.pickle', 'rb') as f:\n    pipe_from_pickle = pickle.load(f)\n\n\npipe_from_pickle.predict(X_new)\n\narray([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,\n       1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n       1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,\n       1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n       1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n       0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n       0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n       1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,\n       0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n       1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n       1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n       0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,\n       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,\n       0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n       1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,\n       0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n       1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,\n       0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1])",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Feature engineering with custom transformers</span>"
    ]
  },
  {
    "objectID": "ch15.html#qa-can-functiontransformer-be-used-with-any-transformation",
    "href": "ch15.html#qa-can-functiontransformer-be-used-with-any-transformation",
    "title": "15¬† Feature engineering with custom transformers",
    "section": "15.12 Q&A: Can FunctionTransformer be used with any transformation?",
    "text": "15.12 Q&A: Can FunctionTransformer be used with any transformation?\nFunctionTransformer should only be used with stateless transformations. A transformation is considered stateless if it doesn‚Äôt learn any information during the fit step.\nFor example, all of our custom transformations in this chapter were stateless: rounding up to the next integer, limiting values to a range, extracting the first letter, and adding two columns. They‚Äôre considered stateless because they didn‚Äôt learn anything about the training data that later needed to be applied to testing data. In other words, they work exactly the same on the testing data regardless of what the training data looked like.\n\n\n\n\n\n\nStateless transformations:\n\nceiling: Rounding up to the next integer\nclip: Limiting values to a range\nletter: Extracting the first letter\ntotal: Adding two columns\n\n\n\n\nThis is in contrast to stateful transformations, which do learn information from the fit step that needs to be applied to both training and testing data. We‚Äôve seen many stateful transformations in this book:\n\nOneHotEncoder learns the categories from the training data, and those same categories need to be applied to the testing data.\nCountVectorizer learns the vocabulary from the training data, and that vocabulary needs to be used when building the document-term matrix for the testing data.\nSimpleImputer learns the value to impute from the training data, and that value is applied to the testing data.\nMaxAbsScaler learns the scale of each feature from the training data, and that scaling is applied to the testing data.\n\nFunctionTransformer should never be used to implement stateful transformations. Depending on the situation, you would either run into an error or you would silently cause data leakage. In that case, you would need to write your own class in order to create a proper transformer.\n\n\n\n\n\n\nStateful transformations:\n\nOneHotEncoder: fit learns the categories\nCountVectorizer: fit learns the vocabulary\nSimpleImputer: fit learns the value to impute\nMaxAbsScaler: fit learns the scale of each feature",
    "crumbs": [
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Feature engineering with custom transformers</span>"
    ]
  },
  {
    "objectID": "ch16.html",
    "href": "ch16.html",
    "title": "16¬† Workflow review #3",
    "section": "",
    "text": "16.1 Recap of our workflow\nIn this chapter, we‚Äôre going to do one final review of the core workflow that we built throughout this book, including all of the features that we developed in the previous chapter.\nWe begin by importing pandas and NumPy, the four transformer classes we‚Äôre using, one modeling class, and two composition functions.\nimport pandas as pd\nimport numpy as np\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, FunctionTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.pipeline import make_pipeline\nNext, we create a list of the eight columns we‚Äôre going to select from our data.\ncols = ['Parch', 'Fare', 'Embarked', 'Sex', 'Name', 'Age', 'Cabin',\n        'SibSp']\nThen, we read in all of our training data and use it to define our X and y.\ndf = pd.read_csv('http://bit.ly/MLtrain')\nX = df[cols]\ny = df['Survived']\nAnd we read in all of the new data and use it to define X_new.\ndf_new = pd.read_csv('http://bit.ly/MLnewdata')\nX_new = df_new[cols]\nWe create five instances of our transformers, namely two different instances of SimpleImputer, two different instances of OneHotEncoder, and one instance of CountVectorizer.\nimp = SimpleImputer()\nimp_constant = SimpleImputer(strategy='constant', fill_value='missing')\nohe = OneHotEncoder()\nohe_ignore = OneHotEncoder(handle_unknown='ignore')\nvect = CountVectorizer()\nWe define two custom functions that will be used for feature engineering.\ndef first_letter(df):\n    return pd.DataFrame(df).apply(lambda x: x.str.slice(0, 1))\ndef sum_cols(df):\n    return np.array(df).sum(axis=1).reshape(-1, 1)\nWe convert two NumPy functions and our two custom functions into transformers.\nceiling = FunctionTransformer(np.ceil)\nclip = FunctionTransformer(np.clip, kw_args={'a_min':5, 'a_max':60})\nletter = FunctionTransformer(first_letter)\ntotal = FunctionTransformer(sum_cols)\nWe create four Pipelines that combine the various transformers.\nimp_ohe = make_pipeline(imp_constant, ohe)\nimp_ceiling = make_pipeline(imp, ceiling)\nimp_clip = make_pipeline(imp, clip)\nletter_imp_ohe = make_pipeline(letter, imp_constant, ohe_ignore)\nAnd then we build the ColumnTransformer, which imputes and one-hot encodes Embarked and Sex, vectorizes Name, imputes and takes the ceiling of Fare, imputes and clips Age, imputes and one-hot encodes the first letter of Cabin, and adds SibSp and Parch.\nct = make_column_transformer(\n    (imp_ohe, ['Embarked', 'Sex']),\n    (vect, 'Name'),\n    (imp_ceiling, ['Fare']),\n    (imp_clip, ['Age']),\n    (letter_imp_ohe, ['Cabin']),\n    (total, ['SibSp', 'Parch']))\nWe also create an instance of LogisticRegression.\nlogreg = LogisticRegression(solver='liblinear', random_state=1)\nWe create a two-step modeling Pipeline and fit the Pipeline to X and y.\npipe = make_pipeline(ct, logreg)\npipe.fit(X, y)\npipe\n\nPipelinePipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('pipeline-1',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer(fill_value='missing',\n                                                                                 strategy='constant')),\n                                                                  ('onehotencoder',\n                                                                   OneHotEncoder())]),\n                                                  ['Embarked', 'Sex']),\n                                                 ('countvectorizer',\n                                                  CountVectorizer(), 'Name'),\n                                                 ('pipeline-2',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer()),\n                                                                  ('functiontr...\n                                                                   FunctionTransformer(func=)),\n                                                                  ('simpleimputer',\n                                                                   SimpleImputer(fill_value='missing',\n                                                                                 strategy='constant')),\n                                                                  ('onehotencoder',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  ['Cabin']),\n                                                 ('functiontransformer',\n                                                  FunctionTransformer(func=),\n                                                  ['SibSp', 'Parch'])])),\n                ('logisticregression',\n                 LogisticRegression(random_state=1, solver='liblinear'))])columntransformer: ColumnTransformerColumnTransformer(transformers=[('pipeline-1',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer(fill_value='missing',\n                                                                strategy='constant')),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder())]),\n                                 ['Embarked', 'Sex']),\n                                ('countvectorizer', CountVectorizer(), 'Name'),\n                                ('pipeline-2',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer()),\n                                                 ('functiontransformer',\n                                                  FunctionTransformer(func=&lt;...\n                                ('pipeline-4',\n                                 Pipeline(steps=[('functiontransformer',\n                                                  FunctionTransformer(func=)),\n                                                 ('simpleimputer',\n                                                  SimpleImputer(fill_value='missing',\n                                                                strategy='constant')),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder(handle_unknown='ignore'))]),\n                                 ['Cabin']),\n                                ('functiontransformer',\n                                 FunctionTransformer(func=),\n                                 ['SibSp', 'Parch'])])pipeline-1['Embarked', 'Sex']SimpleImputerSimpleImputer(fill_value='missing', strategy='constant')OneHotEncoderOneHotEncoder()countvectorizerNameCountVectorizerCountVectorizer()pipeline-2['Fare']SimpleImputerSimpleImputer()FunctionTransformerFunctionTransformer(func=)pipeline-3['Age']SimpleImputerSimpleImputer()FunctionTransformerFunctionTransformer(func=,\n                    kw_args={'a_max': 60, 'a_min': 5})pipeline-4['Cabin']FunctionTransformerFunctionTransformer(func=)SimpleImputerSimpleImputer(fill_value='missing', strategy='constant')OneHotEncoderOneHotEncoder(handle_unknown='ignore')functiontransformer['SibSp', 'Parch']FunctionTransformerFunctionTransformer(func=)LogisticRegressionLogisticRegression(random_state=1, solver='liblinear')\nFinally, we use the fitted Pipeline to make predictions on X_new.\npipe.predict(X_new)\n\narray([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,\n       1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n       1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,\n       1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n       1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n       0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n       0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n       1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,\n       0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n       1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n       1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n       0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,\n       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,\n       0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n       1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,\n       0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n       1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,\n       0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1])\nKeep in mind that there are many other steps you can incorporate into this workflow in order to potentially improve performance, including hyperparameter tuning, trying a different model, ensembling, feature selection, feature standardization, and additional feature engineering.",
    "crumbs": [
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Workflow review #3</span>"
    ]
  },
  {
    "objectID": "ch16.html#whats-the-role-of-pandas",
    "href": "ch16.html#whats-the-role-of-pandas",
    "title": "16¬† Workflow review #3",
    "section": "16.2 What‚Äôs the role of pandas?",
    "text": "16.2 What‚Äôs the role of pandas?\nIf we can do all of our data transformations in scikit-learn, then you might be left wondering: What‚Äôs the role of pandas?\n\nFirst is data exploration and visualization. A deep understanding of your dataset will help you with many steps of the Machine Learning workflow, especially selecting which features to use and deciding how you might want to transform your features.\nSecond is testing out data transformations for Machine Learning. If I‚Äôm thinking about building a custom transformer in scikit-learn, I first create it using pandas to make sure that it works.\nFinally, if your goal is anything other than Machine Learning, then all of your data transformations should still be executed using pandas.\n\nThe bottom line is that pandas still has a huge role in the data science workflow. However, if your goal is Machine Learning, then it‚Äôs best to shift as much of your workflow as possible to scikit-learn.\n\n\n\n\n\n\nUses for pandas in the data science workflow:\n\nAll projects: Data exploration and visualization\nML projects: Testing out data transformations\nNon-ML projects: Executing data transformations",
    "crumbs": [
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Workflow review #3</span>"
    ]
  },
  {
    "objectID": "ch17.html",
    "href": "ch17.html",
    "title": "17¬† High-cardinality categorical features",
    "section": "",
    "text": "17.1 Recap of nominal and ordinal features\nLet‚Äôs talk about categorical features. There are two types of categorical features that we‚Äôve covered in the book:\nSo far, here‚Äôs the advice that I‚Äôve given for encoding nominal and ordinal features:\nLet‚Äôs do a quick recap as to why OneHotEncoder is the preferred approach for a nominal feature, using Embarked as an example.\nEmbarked has three categories, so OneHotEncoder would output 3 features. From each of the three features, the model can learn the relationship between the target value and whether or not a given passenger embarked at that port. For example, the model might learn from the first feature that passengers who embarked at C have a higher survival rate than passengers who didn‚Äôt embark at C.\nIf you were to instead use OrdinalEncoder with Embarked, it would output 1 feature. This is problematic because it would imply an ordering of the categories that doesn‚Äôt inherently exist. For example, if passengers who embarked at C and S had high survival rates, and passengers who embarked at Q had low survival rates, there‚Äôs no way for a linear model to learn this relationship if Embarked is encoded as a single feature.\ndf['Embarked'].value_counts()\n\nS    644\nC    168\nQ     77\nName: Embarked, dtype: int64\nIn this chapter, we‚Äôre going to explore whether this advice still holds when you have high-cardinality categorical features, which are categorical features with lots of unique values.",
    "crumbs": [
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>High-cardinality categorical features</span>"
    ]
  },
  {
    "objectID": "ch17.html#recap-of-nominal-and-ordinal-features",
    "href": "ch17.html#recap-of-nominal-and-ordinal-features",
    "title": "17¬† High-cardinality categorical features",
    "section": "",
    "text": "A nominal feature has categories that are unordered, such as Embarked and Sex.\nAn ordinal feature has categories with an inherent logical ordering, such as Pclass.\n\n\n\n\n\n\n\nTypes of categorical features:\n\nNominal: Unordered categories\n\nEmbarked\nSex\n\nOrdinal: Ordered categories\n\nPclass\n\n\n\n\n\n\n\nFor a nominal feature, you should use OneHotEncoder, and it will output one column for each category.\nFor an ordinal feature that is already encoded numerically, you should leave it as-is.\nAnd for an ordinal feature that is encoded as strings, you should use OrdinalEncoder, and it will output a single column using the category ordering that you define.\n\n\n\n\n\n\n\nAdvice for encoding categorical data:\n\nNominal feature: Use OneHotEncoder\nOrdinal feature stored as numbers: Leave as-is\nOrdinal feature stored as strings: Use OrdinalEncoder\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy use OneHotEncoder for Embarked?\n\nOneHotEncoder:\n\nOutputs 3 features\nModel can learn the relationship between each feature and the target value\n\nOrdinalEncoder:\n\nOutputs 1 feature\nImplies an ordering that doesn‚Äôt inherently exist\nLinear model can‚Äôt necessarily learn the relationships in the data",
    "crumbs": [
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>High-cardinality categorical features</span>"
    ]
  },
  {
    "objectID": "ch17.html#preparing-the-census-dataset",
    "href": "ch17.html#preparing-the-census-dataset",
    "title": "17¬† High-cardinality categorical features",
    "section": "17.2 Preparing the census dataset",
    "text": "17.2 Preparing the census dataset\nWe‚Äôll use a new dataset for this chapter, namely a dataset of US census data from 1994.\nWe‚Äôll read the dataset into a new DataFrame called census.\n\ncensus = pd.read_csv('http://bit.ly/censusdataset')\n\nWe‚Äôre only going to use the categorical features, which we can explore by using the DataFrame describe method.\n\ncensus.describe(include='object')\n\n\n\n\n\n\n\n\nworkclass\neducation\nmarital-status\noccupation\nrelationship\nrace\nsex\nnative-country\nclass\n\n\n\n\ncount\n48842\n48842\n48842\n48842\n48842\n48842\n48842\n48842\n48842\n\n\nunique\n9\n16\n7\n15\n6\n5\n2\n42\n2\n\n\ntop\nPrivate\nHS-grad\nMarried-civ-spouse\nProf-specialty\nHusband\nWhite\nMale\nUnited-States\n&lt;=50K\n\n\nfreq\n33906\n15784\n22379\n6172\n19716\n41762\n32650\n43832\n37155\n\n\n\n\n\n\n\nFrom the row labeled ‚Äúunique‚Äù, you can see that education, occupation, and native-country all have more than 10 unique values. There‚Äôs no hard-and-fast rule for what counts as a high-cardinality feature, but all three of these could be considered high-cardinality since they have a lot of unique values.\nYou can‚Äôt tell from this display, but these 8 features are all nominal features, with the exception of education since it does have a logical ordering. However, we‚Äôre going to be treating education as nominal for this experiment.\n\n\n\n\n\n\nCategorical features in census dataset:\n\nHigh-cardinality (3 of 8): education, occupation, native-country\nNominal (7 of 8): All except education\n\n\n\n\nThe column labeled ‚Äúclass‚Äù is actually our target. This column indicates whether the person has an income of more or less than fifty thousand dollars a year.\nWe can view the class proportions by normalizing the output of value_counts.\n\ncensus['class'].value_counts(normalize=True)\n\n &lt;=50K    0.760718\n &gt;50K     0.239282\nName: class, dtype: float64\n\n\nWhen defining our X DataFrame, which I‚Äôm calling census_X, we‚Äôre only going to use the 8 categorical columns, which I‚Äôve listed out manually. And we‚Äôll use class as our y Series, which I‚Äôm calling census_y.\n\ncensus_cols = ['workclass', 'education', 'marital-status', 'occupation',\n               'relationship', 'race', 'sex', 'native-country']\ncensus_X = census[census_cols]\ncensus_y = census['class']",
    "crumbs": [
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>High-cardinality categorical features</span>"
    ]
  },
  {
    "objectID": "ch17.html#setting-up-the-encoders",
    "href": "ch17.html#setting-up-the-encoders",
    "title": "17¬† High-cardinality categorical features",
    "section": "17.3 Setting up the encoders",
    "text": "17.3 Setting up the encoders\nWe‚Äôre going to be testing the effectiveness of both OneHotEncoder and OrdinalEncoder with these 8 features. For this experiment, we would normally just create instances using the default arguments.\n\nohe = OneHotEncoder()\noe = OrdinalEncoder()\n\nNotice that we created an instance of OrdinalEncoder without defining the category ordering. This is because we‚Äôre treating all of the features as nominal, and as such there is no logical ordering.\nAs a result, OrdinalEncoder would simply learn the categories for each feature in alphabetical order, which we can confirm by fitting the OrdinalEncoder and checking the categories_ attribute.\n\noe.fit(census_X).categories_\n\n[array([' ?', ' Federal-gov', ' Local-gov', ' Never-worked', ' Private',\n        ' Self-emp-inc', ' Self-emp-not-inc', ' State-gov', ' Without-pay'],\n       dtype=object),\n array([' 10th', ' 11th', ' 12th', ' 1st-4th', ' 5th-6th', ' 7th-8th',\n        ' 9th', ' Assoc-acdm', ' Assoc-voc', ' Bachelors', ' Doctorate',\n        ' HS-grad', ' Masters', ' Preschool', ' Prof-school',\n        ' Some-college'], dtype=object),\n array([' Divorced', ' Married-AF-spouse', ' Married-civ-spouse',\n        ' Married-spouse-absent', ' Never-married', ' Separated',\n        ' Widowed'], dtype=object),\n array([' ?', ' Adm-clerical', ' Armed-Forces', ' Craft-repair',\n        ' Exec-managerial', ' Farming-fishing', ' Handlers-cleaners',\n        ' Machine-op-inspct', ' Other-service', ' Priv-house-serv',\n        ' Prof-specialty', ' Protective-serv', ' Sales', ' Tech-support',\n        ' Transport-moving'], dtype=object),\n array([' Husband', ' Not-in-family', ' Other-relative', ' Own-child',\n        ' Unmarried', ' Wife'], dtype=object),\n array([' Amer-Indian-Eskimo', ' Asian-Pac-Islander', ' Black', ' Other',\n        ' White'], dtype=object),\n array([' Female', ' Male'], dtype=object),\n array([' ?', ' Cambodia', ' Canada', ' China', ' Columbia', ' Cuba',\n        ' Dominican-Republic', ' Ecuador', ' El-Salvador', ' England',\n        ' France', ' Germany', ' Greece', ' Guatemala', ' Haiti',\n        ' Holand-Netherlands', ' Honduras', ' Hong', ' Hungary', ' India',\n        ' Iran', ' Ireland', ' Italy', ' Jamaica', ' Japan', ' Laos',\n        ' Mexico', ' Nicaragua', ' Outlying-US(Guam-USVI-etc)', ' Peru',\n        ' Philippines', ' Poland', ' Portugal', ' Puerto-Rico',\n        ' Scotland', ' South', ' Taiwan', ' Thailand', ' Trinadad&Tobago',\n        ' United-States', ' Vietnam', ' Yugoslavia'], dtype=object)]\n\n\nThat being said, we‚Äôre actually going to run into a problem with encoding due to our highest cardinality feature, native-country. Let‚Äôs take a look at it and see why.\n\ncensus_X['native-country'].value_counts()\n\n United-States                 43832\n Mexico                          951\n ?                               857\n Philippines                     295\n Germany                         206\n Puerto-Rico                     184\n Canada                          182\n El-Salvador                     155\n India                           151\n Cuba                            138\n England                         127\n China                           122\n South                           115\n Jamaica                         106\n Italy                           105\n Dominican-Republic              103\n Japan                            92\n Guatemala                        88\n Poland                           87\n Vietnam                          86\n Columbia                         85\n Haiti                            75\n Portugal                         67\n Taiwan                           65\n Iran                             59\n Nicaragua                        49\n Greece                           49\n Peru                             46\n Ecuador                          45\n France                           38\n Ireland                          37\n Thailand                         30\n Hong                             30\n Cambodia                         28\n Trinadad&Tobago                  27\n Laos                             23\n Yugoslavia                       23\n Outlying-US(Guam-USVI-etc)       23\n Scotland                         21\n Honduras                         20\n Hungary                          19\n Holand-Netherlands                1\nName: native-country, dtype: int64\n\n\nYou can see that one of the categories appears only once in the dataset. As we talked about in lesson¬†15.7, rare category values can cause problems with cross-validation.\nIn this case, it will definitely create a problem, because that sample is guaranteed to appear in the test fold but not a training fold during one of the runs of cross-validation. That will cause an error for both OneHotEncoder and OrdinalEncoder.\n\n\n\n\n\n\nResolving the cross-validation error caused by rare categories:\n\nOneHotEncoder: Set handle_unknown to 'ignore'\nOrdinalEncoder (before version 0.24): Define the categories in advance\nOrdinalEncoder (starting in version 0.24): Set handle_unknown to 'use_encoded_value'\n\n\n\n\nIn the case of OneHotEncoder, the solution is simply to set the handle_unknown parameter to 'ignore'.\n\nohe_ignore = OneHotEncoder(handle_unknown='ignore')\n\nStarting in scikit-learn version 0.24, OrdinalEncoder has a similar handle_unknown parameter that could be used for this situation.\nBut for now, the best solution is to define the categories in advance for each feature using a list comprehension. The list comprehension iterates through the feature columns, extracts the unique values from each column, and stores the result in a list called cats.\n\ncats = [census_X[col].unique() for col in census_X[census_cols]]\ncats\n\n[array([' Private', ' Local-gov', ' ?', ' Self-emp-not-inc',\n        ' Federal-gov', ' State-gov', ' Self-emp-inc', ' Without-pay',\n        ' Never-worked'], dtype=object),\n array([' 11th', ' HS-grad', ' Assoc-acdm', ' Some-college', ' 10th',\n        ' Prof-school', ' 7th-8th', ' Bachelors', ' Masters', ' Doctorate',\n        ' 5th-6th', ' Assoc-voc', ' 9th', ' 12th', ' 1st-4th',\n        ' Preschool'], dtype=object),\n array([' Never-married', ' Married-civ-spouse', ' Widowed', ' Divorced',\n        ' Separated', ' Married-spouse-absent', ' Married-AF-spouse'],\n       dtype=object),\n array([' Machine-op-inspct', ' Farming-fishing', ' Protective-serv', ' ?',\n        ' Other-service', ' Prof-specialty', ' Craft-repair',\n        ' Adm-clerical', ' Exec-managerial', ' Tech-support', ' Sales',\n        ' Priv-house-serv', ' Transport-moving', ' Handlers-cleaners',\n        ' Armed-Forces'], dtype=object),\n array([' Own-child', ' Husband', ' Not-in-family', ' Unmarried', ' Wife',\n        ' Other-relative'], dtype=object),\n array([' Black', ' White', ' Asian-Pac-Islander', ' Other',\n        ' Amer-Indian-Eskimo'], dtype=object),\n array([' Male', ' Female'], dtype=object),\n array([' United-States', ' ?', ' Peru', ' Guatemala', ' Mexico',\n        ' Dominican-Republic', ' Ireland', ' Germany', ' Philippines',\n        ' Thailand', ' Haiti', ' El-Salvador', ' Puerto-Rico', ' Vietnam',\n        ' South', ' Columbia', ' Japan', ' India', ' Cambodia', ' Poland',\n        ' Laos', ' England', ' Cuba', ' Taiwan', ' Italy', ' Canada',\n        ' Portugal', ' China', ' Nicaragua', ' Honduras', ' Iran',\n        ' Scotland', ' Jamaica', ' Ecuador', ' Yugoslavia', ' Hungary',\n        ' Hong', ' Greece', ' Trinadad&Tobago',\n        ' Outlying-US(Guam-USVI-etc)', ' France', ' Holand-Netherlands'],\n       dtype=object)]\n\n\nThen, we can pass the cats list to the categories parameter of OrdinalEncoder when creating an instance. This solves our problem since no unknown categories will ever appear during cross-validation.\n\noe_cats = OrdinalEncoder(categories=cats)",
    "crumbs": [
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>High-cardinality categorical features</span>"
    ]
  },
  {
    "objectID": "ch17.html#encoding-nominal-features-for-a-linear-model",
    "href": "ch17.html#encoding-nominal-features-for-a-linear-model",
    "title": "17¬† High-cardinality categorical features",
    "section": "17.4 Encoding nominal features for a linear model",
    "text": "17.4 Encoding nominal features for a linear model\nNow that we‚Äôve set up our OneHotEncoder, called ohe_ignore, and our OrdinalEncoder, called oe_cats, let‚Äôs see what happens when we pass census_X to fit_transform and then check the shape.\nAs expected, the OneHotEncoder creates a lot of columns due to the high-cardinality features, whereas the OrdinalEncoder creates only one column for each of the eight features.\n\nohe_ignore.fit_transform(census_X).shape\n\n(48842, 102)\n\n\n\noe_cats.fit_transform(census_X).shape\n\n(48842, 8)\n\n\nNow let‚Äôs actually test the advice that I‚Äôve given, which is that OneHotEncoder should be used for nominal features, to see if this advice still holds for high-cardinality features.\nThe simplest method for doing this is to create two Pipelines. One of them uses OneHotEncoder and the other uses OrdinalEncoder, and both end in a logistic regression model.\n\nohe_logreg = make_pipeline(ohe_ignore, logreg)\noe_logreg = make_pipeline(oe_cats, logreg)\n\nWe‚Äôll cross-validate each Pipeline using all features and then compare the accuracies. We‚Äôll also time the operations to see if there are significant differences.\n\n%time cross_val_score(ohe_logreg, census_X, census_y, cv=5, \\\n                      scoring='accuracy').mean()\n\nCPU times: user 580 ms, sys: 5.33 ms, total: 586 ms\nWall time: 588 ms\n\n\n0.8329920571424309\n\n\n\n%time cross_val_score(oe_logreg, census_X, census_y, cv=5, \\\n                      scoring='accuracy').mean()\n\nCPU times: user 506 ms, sys: 3.26 ms, total: 509 ms\nWall time: 509 ms\n\n\n0.7547398152859307\n\n\nThe two Pipelines take around the same amount of time to run, but the accuracy of the OneHotEncoder Pipeline is 0.833, which is significantly better than the 0.755 accuracy of the OrdinalEncoder Pipeline. This would suggest that at least for a linear model like logistic regression, OneHotEncoder should be used for nominal features, even when the features have high cardinality.\n\n\n\n\n\n\nPipeline accuracy when encoding nominal features:\n\nLinear model:\n\nOneHotEncoder: 0.833\nOrdinalEncoder: 0.755",
    "crumbs": [
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>High-cardinality categorical features</span>"
    ]
  },
  {
    "objectID": "ch17.html#encoding-nominal-features-for-a-non-linear-model",
    "href": "ch17.html#encoding-nominal-features-for-a-non-linear-model",
    "title": "17¬† High-cardinality categorical features",
    "section": "17.5 Encoding nominal features for a non-linear model",
    "text": "17.5 Encoding nominal features for a non-linear model\nLet‚Äôs now do the same comparison as the previous lesson, except this time we‚Äôll use random forests, which is a tree-based non-linear model that we used in chapter 11.\nFirst, we‚Äôll create two more Pipelines. One uses OneHotEncoder and the other uses OrdinalEncoder, and both end in a random forest model.\n\nohe_rf = make_pipeline(ohe_ignore, rf)\noe_rf = make_pipeline(oe_cats, rf)\n\nThen, we‚Äôll cross-validate each Pipeline using all features.\n\n%time cross_val_score(ohe_rf, census_X, census_y, cv=5, \\\n                      scoring='accuracy').mean()\n\nCPU times: user 38.3 s, sys: 336 ms, total: 38.6 s\nWall time: 5.75 s\n\n\n0.8260513856992514\n\n\n\n%time cross_val_score(oe_rf, census_X, census_y, cv=5, \\\n                      scoring='accuracy').mean()\n\nCPU times: user 6.17 s, sys: 299 ms, total: 6.47 s\nWall time: 1.47 s\n\n\n0.8245362761024548\n\n\nWe can see that the accuracies are about the same for the OneHotEncoder Pipeline (0.826) and the OrdinalEncoder Pipeline (0.825), even though we were using the OrdinalEncoder on nominal features, which would normally be considered improper.\nHow can this be? Well, because of how decision trees recursively split features, the random forest model can approximately learn the relationships present in categorical features even when they‚Äôre encoded as single columns with OrdinalEncoder.\nIt‚Äôs also worth noting that the OrdinalEncoder Pipeline is significantly faster than the OneHotEncoder Pipeline due to the much smaller feature set created by the OrdinalEncoder.\n\n\n\n\n\n\nPipeline accuracy when encoding nominal features:\n\nLinear model:\n\nOneHotEncoder: 0.833\nOrdinalEncoder: 0.755\n\nNon-linear model:\n\nOneHotEncoder: 0.826\nOrdinalEncoder: 0.825",
    "crumbs": [
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>High-cardinality categorical features</span>"
    ]
  },
  {
    "objectID": "ch17.html#combining-the-encodings",
    "href": "ch17.html#combining-the-encodings",
    "title": "17¬† High-cardinality categorical features",
    "section": "17.6 Combining the encodings",
    "text": "17.6 Combining the encodings\nOne final variation that we can try is to use the OneHotEncoder for all features except for education. And since education is actually an ordinal feature, we can use the OrdinalEncoder with it and define the category ordering.\nHere are the education categories.\n\ncensus_X['education'].unique()\n\narray([' 11th', ' HS-grad', ' Assoc-acdm', ' Some-college', ' 10th',\n       ' Prof-school', ' 7th-8th', ' Bachelors', ' Masters', ' Doctorate',\n       ' 5th-6th', ' Assoc-voc', ' 9th', ' 12th', ' 1st-4th',\n       ' Preschool'], dtype=object)\n\n\nWe‚Äôll manually define the category ordering, from ‚ÄúPreschool‚Äù through ‚ÄúDoctorate‚Äù, and then create an instance of OrdinalEncoder using that ordering.\n\ncats = [[' Preschool', ' 1st-4th', ' 5th-6th', ' 7th-8th', ' 9th',\n         ' 10th', ' 11th', ' 12th', ' HS-grad', ' Some-college',\n         ' Assoc-voc', ' Assoc-acdm', ' Bachelors', ' Masters',\n         ' Prof-school', ' Doctorate']]\noe_cats = OrdinalEncoder(categories=cats)\n\nThen we‚Äôll create a ColumnTransformer that applies the OrdinalEncoder to education, and applies the OneHotEncoder to all other features.\n\nct = make_column_transformer(\n    (oe_cats, ['education']),\n    remainder=ohe_ignore)\n\nWhen we pass census_X to the fit_transform, it creates 87 feature columns, compared to the 102 columns that were created when we only used the OneHotEncoder.\n\nct.fit_transform(census_X).shape\n\n(48842, 87)\n\n\nFinally, we‚Äôll create two Pipelines. Both of them start with the same ColumnTransformer, but one ends with logistic regression while the other ends with random forests.\n\noe_ohe_logreg = make_pipeline(ct, logreg)\noe_ohe_rf = make_pipeline(ct, rf)\n\nWhen we cross-validate the first Pipeline, the accuracy is 0.832, which is nearly the same as the 0.833 achieved by the logistic regression Pipeline that used OneHotEncoder for all features.\n\n%time cross_val_score(oe_ohe_logreg, census_X, census_y, cv=5, \\\n                      scoring='accuracy').mean()\n\nCPU times: user 611 ms, sys: 6.72 ms, total: 618 ms\nWall time: 618 ms\n\n\n0.8315588308601922\n\n\nWhen we cross-validate the second Pipeline, the accuracy is 0.825, which is nearly the same as the 0.826 achieved by the random forest Pipeline that used OneHotEncoder for all features.\n\n%time cross_val_score(oe_ohe_rf, census_X, census_y, cv=5, \\\n                      scoring='accuracy').mean()\n\nCPU times: user 38.5 s, sys: 316 ms, total: 38.8 s\nWall time: 5.73 s\n\n\n0.8251300537921482\n\n\nIn summary, encoding the education feature with OrdinalEncoder and the seven other features with OneHotEncoder performed basically the same as encoding all eight features with OneHotEncoder. However, it‚Äôs certainly possible that the OrdinalEncoder could provide a benefit under other circumstances.\n\n\n\n\n\n\nPipeline accuracy when encoding nominal features:\n\nLinear model:\n\nOneHotEncoder: 0.833\nOrdinalEncoder: 0.755\nOneHotEncoder (7 features) + OrdinalEncoder (education): 0.832\n\nNon-linear model:\n\nOneHotEncoder: 0.826\nOrdinalEncoder: 0.825\nOneHotEncoder (7 features) + OrdinalEncoder (education): 0.825",
    "crumbs": [
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>High-cardinality categorical features</span>"
    ]
  },
  {
    "objectID": "ch17.html#best-practices-for-encoding",
    "href": "ch17.html#best-practices-for-encoding",
    "title": "17¬† High-cardinality categorical features",
    "section": "17.7 Best practices for encoding",
    "text": "17.7 Best practices for encoding\nLet‚Äôs summarize what we‚Äôve learned in this chapter:\n\nIf you have nominal features, and you‚Äôre using a linear model, you should definitely use OneHotEncoder, regardless of whether the features have high cardinality.\nIf you have nominal features, and you‚Äôre using a non-linear model, you can try using OneHotEncoder, and you can try using OrdinalEncoder without defining the category ordering, and then see which option performs better.\nIf you have ordinal features, regardless of the type of model, you can try using OneHotEncoder, and you can try using OrdinalEncoder while defining the category ordering, and then see which option performs better.\n\nIn all cases, keep in mind that if the features have high cardinality, OrdinalEncoder is likely to run significantly faster than OneHotEncoder, which may or may not matter in your particular case.\n\n\n\n\n\n\nSummary of best practices:\n\nNominal features, linear model:\n\nOneHotEncoder\n\nNominal features, non-linear model:\n\nOneHotEncoder\nOrdinalEncoder\n\nDon‚Äôt define category ordering\nMuch faster than OneHotEncoder (if features have high cardinality)\n\n\nOrdinal features:\n\nOneHotEncoder\nOrdinalEncoder\n\nDefine category ordering\nMuch faster than OneHotEncoder (if features have high cardinality)",
    "crumbs": [
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>High-cardinality categorical features</span>"
    ]
  },
  {
    "objectID": "ch18.html",
    "href": "ch18.html",
    "title": "18¬† Class imbalance",
    "section": "",
    "text": "18.1 Introduction to class imbalance\nOne of the common issues you might run into when working on a classification problem is known as ‚Äúclass imbalance‚Äù. This refers to a situation in which the classes are not equally represented in the dataset.\nClass imbalance is inherent to many domains. For example, if we were classifying fraudulent transactions in a credit card dataset, there would naturally be class imbalance since the vast majority of transactions are not going to be fraudulent.\nClass imbalance can occur in both binary classification problems, meaning there are 2 classes, and multiclass problems, meaning there are more than 2 classes. In binary classification, the class that has more samples is called the ‚Äúmajority class‚Äù, and the class that has fewer samples is called the ‚Äúminority class‚Äù.\nSo why does class imbalance even matter? In brief, Machine Learning models often have a harder time predicting the minority class because there are fewer examples of this class to learn from. In other words, your model won‚Äôt be able to learn as much about the patterns in the minority class, and thus it may have a hard time differentiating between the classes.\nKeep in mind that some class imbalance is present in most datasets. For example, in our Titanic dataset, the majority class represents 62% of the samples, and the minority class represents 38% of the samples. There‚Äôs even more imbalance in the census dataset, with a split of 76% to 24%.\nA small amount of class imbalance (like in the Titanic dataset) tends not to matter, and you should just use all of the usual techniques that you‚Äôve learned in this book. But as the amount of class imbalance increases, more specialized techniques need to be applied to the problem, and those techniques are the focus of this chapter.\ny.value_counts(normalize=True)\n\n0    0.616162\n1    0.383838\nName: Survived, dtype: float64\ncensus_y.value_counts(normalize=True)\n\n &lt;=50K    0.760718\n &gt;50K     0.239282\nName: class, dtype: float64",
    "crumbs": [
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Class imbalance</span>"
    ]
  },
  {
    "objectID": "ch18.html#introduction-to-class-imbalance",
    "href": "ch18.html#introduction-to-class-imbalance",
    "title": "18¬† Class imbalance",
    "section": "",
    "text": "Overview of class imbalance:\n\n‚ÄúClass imbalance‚Äù is when classes are not equally represented\nInherent to many domains\nCan occur in binary and multiclass problems\nBinary problems have a ‚Äúmajority class‚Äù and a ‚Äúminority class‚Äù\nMakes it harder for the model to learn patterns in the minority class\nMost datasets have some class imbalance\nGreater class imbalance requires more specialized techniques",
    "crumbs": [
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Class imbalance</span>"
    ]
  },
  {
    "objectID": "ch18.html#preparing-the-mammography-dataset",
    "href": "ch18.html#preparing-the-mammography-dataset",
    "title": "18¬† Class imbalance",
    "section": "18.2 Preparing the mammography dataset",
    "text": "18.2 Preparing the mammography dataset\nFor this chapter, we‚Äôre going to use a dataset of mammography scans that were designed to detect the presence of cancer.\nWe‚Äôll read in the dataset using pandas. You can see that there are 6 features as well as a target column called ‚Äúclass‚Äù. Each sample represents an object of interest that was extracted from a scan, and each object was translated into these features using a computer vision algorithm.\nThe class column has two possible values, -1 and 1. -1 means the object did not indicate the presence of cancer, and 1 means the object did indicate the presence of cancer. These labels were assigned by a human expert and thus represent the ‚Äúground truth‚Äù labels for the dataset.\n\nscan = pd.read_csv('http://bit.ly/scanrecords')\nscan.head()\n\n\n\n\n\n\n\n\nattr1\nattr2\nattr3\nattr4\nattr5\nattr6\nclass\n\n\n\n\n0\n0.230020\n5.072578\n-0.276061\n0.832444\n-0.377866\n0.480322\n'-1'\n\n\n1\n0.155491\n-0.169390\n0.670652\n-0.859553\n-0.377866\n-0.945723\n'-1'\n\n\n2\n-0.784415\n-0.443654\n5.674705\n-0.859553\n-0.377866\n-0.945723\n'-1'\n\n\n3\n0.546088\n0.131415\n-0.456387\n-0.859553\n-0.377866\n-0.945723\n'-1'\n\n\n4\n-0.102987\n-0.394994\n-0.140816\n0.979703\n-0.377866\n1.013566\n'-1'\n\n\n\n\n\n\n\nAs you can see, -1 (meaning ‚Äúnon-cancerous‚Äù) is the majority class, and 1 (meaning ‚Äúcancerous‚Äù) is the minority class. You might call this severe class imbalance because the minority class makes up only 2% of the dataset.\n\nscan['class'].value_counts(normalize=True)\n\n'-1'    0.97675\n'1'     0.02325\nName: class, dtype: float64\n\n\nAlthough scikit-learn doesn‚Äôt require it, we‚Äôre going to map the strings in the class column to the integers 0 and 1, in which 0 means ‚Äúnon-cancerous‚Äù and 1 means ‚Äúcancerous‚Äù.\n\nscan['class'] = scan['class'].map({\"'-1'\":0, \"'1'\":1})\nscan.head()\n\n\n\n\n\n\n\n\nattr1\nattr2\nattr3\nattr4\nattr5\nattr6\nclass\n\n\n\n\n0\n0.230020\n5.072578\n-0.276061\n0.832444\n-0.377866\n0.480322\n0\n\n\n1\n0.155491\n-0.169390\n0.670652\n-0.859553\n-0.377866\n-0.945723\n0\n\n\n2\n-0.784415\n-0.443654\n5.674705\n-0.859553\n-0.377866\n-0.945723\n0\n\n\n3\n0.546088\n0.131415\n-0.456387\n-0.859553\n-0.377866\n-0.945723\n0\n\n\n4\n-0.102987\n-0.394994\n-0.140816\n0.979703\n-0.377866\n1.013566\n0\n\n\n\n\n\n\n\nNow that the dataset is ready, we‚Äôre going to define two objects, scan_X and scan_y, to represent the feature matrix and the target.\n\nscan_X = scan.drop('class', axis='columns')\nscan_y = scan['class']",
    "crumbs": [
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Class imbalance</span>"
    ]
  },
  {
    "objectID": "ch18.html#sec-18-3",
    "href": "ch18.html#sec-18-3",
    "title": "18¬† Class imbalance",
    "section": "18.3 Evaluating a model with train/test split",
    "text": "18.3 Evaluating a model with train/test split\nThroughout the book, we‚Äôve been using cross-validation as the model evaluation procedure. But in this chapter, we‚Äôre going to use train/test split instead, since that will help you to better understand the metrics and techniques I‚Äôll be outlining. However, I‚Äôll return to cross-validation in chapter 19.\nBefore I get into the code, here‚Äôs a quick overview of how we use train/test split for model evaluation:\n\nYou split the dataset into training and testing sets.\nYou fit the model on the training set.\nYou use the fitted model to make predictions on the testing set and evaluate those predictions.\n\n\n\n\n\n\n\nSteps of train/test split:\n\nSplit rows into training and testing sets\nTrain model on training set\nMake predictions on testing set\nEvaluate predictions\n\n\n\n\nWhen we use K-fold cross-validation, we‚Äôre really just running train/test split K times in a very systematic way and then averaging the results. Cross-validation is superior because it outputs a lower variance estimate of model performance, but again, we‚Äôre going to start with train/test split to help you better understand the concepts in this chapter.\n\n\n\n\n\n\nCross-validation vs train/test split:\n\nCross-validation is just repeated train/test split\nCross-validation outputs a lower variance estimate of performance\n\n\n\n\nAnyway, let‚Äôs start by splitting the dataset into training and testing sets. We‚Äôll use a split of 75% of the rows for training and 25% of the rows for testing, and we‚Äôll set a random_state for reproducibility.\n\nX_train, X_test, y_train, y_test = train_test_split(scan_X, scan_y,\n                                                    test_size=0.25,\n                                                    random_state=1,\n                                                    stratify=scan_y)\n\nWe‚Äôll use stratified sampling so that the class proportions will be approximately equal in the training and testing sets. This is especially important when you have severe class imbalance, otherwise you might have insufficient examples of the minority class in either the training or testing set.\n\n\n\n\n\n\nStratified sampling:\n\nEnsures that each set is representative of the dataset\nEspecially important when there is severe class imbalance\n\n\n\n\nNow we‚Äôll fit our logistic regression model on the training set, and use the fitted model to make predictions on the testing set. The predict method outputs class predictions of 0 or 1, which we‚Äôll store as y_pred.\nAs an aside, we‚Äôre not using a Pipeline here because the features are entirely numeric and don‚Äôt need any preprocessing. However, if you were using a Pipeline, you would just use pipe.fit and pipe.predict instead.\n\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test)\ny_pred\n\narray([0, 0, 0, ..., 0, 0, 0])\n\n\nFinally, let‚Äôs evaluate the model‚Äôs predictions. We‚Äôve used accuracy as our evaluation metric throughout the book, so we‚Äôll try that here. We have to import the accuracy_score function from the metrics module, and then we pass it the true values followed by the predicted values.\nIt outputs an accuracy of 98%, which sounds great. But as you might recall, about 98% of the target values are 0, so an uninformed model could achieve 98% accuracy by always predicting class 0.\nBecause the accuracy value isn‚Äôt telling us much about how the model is actually performing, it‚Äôs not a particularly useful evaluation metric in cases of severe class imbalance.\n\nfrom sklearn.metrics import accuracy_score\naccuracy_score(y_test, y_pred)\n\n0.9831902718168812",
    "crumbs": [
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Class imbalance</span>"
    ]
  },
  {
    "objectID": "ch18.html#exploring-the-results-with-a-confusion-matrix",
    "href": "ch18.html#exploring-the-results-with-a-confusion-matrix",
    "title": "18¬† Class imbalance",
    "section": "18.4 Exploring the results with a confusion matrix",
    "text": "18.4 Exploring the results with a confusion matrix\nBefore we choose an evaluation metric other than accuracy, let‚Äôs first use a confusion matrix to get a better understanding of our results.\nWe import it from the metrics module, and then pass it the true values followed by the predicted values. Note that the ordering is important, because your confusion matrix will be incorrect if you pass the predicted values first and the true values second.\n\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix(y_test, y_pred)\n\narray([[2721,   10],\n       [  37,   28]])\n\n\nIt can be hard to remember what each of the four boxes represents, so a nice alternative is to use the plot_confusion_matrix function, which was introduced in version 0.22. After importing it, we pass it a fitted model, X_test, and y_test. It makes predictions for X_test, and outputs a confusion matrix by comparing the results to y_test. In other words, it duplicates some of the work we‚Äôve already done, though it ends up with the same result.\nAs an aside, scikit-learn 1.0 actually provides a different method for plotting a confusion matrix that allows you to directly pass in y_pred.\nAnyway, the confusion matrix is now labeled, with the true labels as the rows and the predicted labels as the columns. You can see that:\n\nIn 2721 cases, the model predicted 0 and that was correct. These are called True Negatives.\nIn 28 cases, it predicted 1 and that was correct. These are True Positives.\nIn 37 cases, it predicted 0 and that was incorrect. These are False Negatives.\nAnd in 10 cases, it predicted 1 and that was incorrect. These are False Positives.\n\n\nfrom sklearn.metrics import plot_confusion_matrix\ndisp = plot_confusion_matrix(logreg, X_test, y_test, cmap='Blues')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n¬†\nPredicted 0\nPredicted 1\n\n\n\n\nTrue label 0\nTrue Negatives\nFalse Positives\n\n\nTrue label 1\nFalse Negatives\nTrue Positives\n\n\n\n\n\n\nYou can see that the confusion matrix helps us understand the performance of our classifier much better than the accuracy score.\nIn particular, it highlights a troubling issue, which we can see by examining the bottom row. There were 65 samples in which cancer was present in the testing set, and it was only detected in 28 of those cases. Given the context, in which the model is attempting to detect the presence of cancer, you could imagine that it would be highly problematic to miss 37 out of 65 cases. This is an issue that we will address by the end of this chapter.",
    "crumbs": [
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Class imbalance</span>"
    ]
  },
  {
    "objectID": "ch18.html#calculating-rates-from-a-confusion-matrix",
    "href": "ch18.html#calculating-rates-from-a-confusion-matrix",
    "title": "18¬† Class imbalance",
    "section": "18.5 Calculating rates from a confusion matrix",
    "text": "18.5 Calculating rates from a confusion matrix\nBefore we discuss a solution to the problem from the previous lesson, let‚Äôs first calculate a few rates from the confusion matrix to help us quantify what we want to improve and what tradeoffs we‚Äôll be making. Here‚Äôs our confusion matrix from the previous lesson.\n\nconfusion_matrix(y_test, y_pred)\n\narray([[2721,   10],\n       [  37,   28]])\n\n\nTrue Positive Rate answers the question: When cancer is present, how often is that correctly predicted? We divide the True Positives by the entire bottom row, and we get 43.1%. True Positive Rate is also known as recall.\n\n28 / (37 + 28)\n\n0.4307692307692308\n\n\nTrue Negative Rate answers the question: When cancer is not present, how often is that correctly predicted? We divide the True Negatives by the entire top row, and we get 99.6%.\n\n2721 / (2721 + 10)\n\n0.9963383376052728\n\n\nFalse Positive Rate answers the question: When cancer is not present, how often is that incorrectly predicted? We divide the False Positives by the entire top row, and we get 0.4%. As you might have figured out, the False Positive Rate is 1 minus the True Negative Rate.\n\n10 / (2721 + 10)\n\n0.003661662394727206\n\n\n\n\n\n\n\n\nCalculated rates:\n\nTrue Positive Rate: 0.431\n\nRecall for class 1\n\nTrue Negative Rate: 0.996\n\nRecall for class 0\n\nFalse Positive Rate: 0.004\n\n1 minus True Negative Rate\n\n\n\n\n\nAnother way to see these results without doing the calculations yourself is with the classification report. We import it and pass it the true and predicted values.\n\nThe True Positive Rate is the recall for class 1, which is 43%.\nThe True Negative Rate is the recall for class 0, which has been rounded to 100%.\nThe False Positive Rate is not directly shown, but it‚Äôs 1 minus the True Negative Rate, thus it‚Äôs close to 0%.\nThe overall accuracy is also shown, which is 98%, though that‚Äôs not our focus here.\n\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))\n\n              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      2731\n           1       0.74      0.43      0.54        65\n\n    accuracy                           0.98      2796\n   macro avg       0.86      0.71      0.77      2796\nweighted avg       0.98      0.98      0.98      2796\n\n\n\nBecause the True Positive Rate of 43% seems quite problematic, you might think that the solution is just to work on maximizing True Positive Rate. However, the danger of only focusing on True Positive Rate is that you might end up with a confusion matrix similar to this.\n\nconfusion_matrix(y_test, np.ones_like(y_pred))\n\narray([[   0, 2731],\n       [   0,   65]])\n\n\nThe True Positive Rate in this case is 100%, but that‚Äôs because we‚Äôre always predicting the positive class. As such, the False Positive Rate would also be 100%. I think you would agree that this is not a useful solution.",
    "crumbs": [
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Class imbalance</span>"
    ]
  },
  {
    "objectID": "ch18.html#using-auc-as-the-evaluation-metric",
    "href": "ch18.html#using-auc-as-the-evaluation-metric",
    "title": "18¬† Class imbalance",
    "section": "18.6 Using AUC as the evaluation metric",
    "text": "18.6 Using AUC as the evaluation metric\nNow that I‚Äôve gone through the nature of the problem, let‚Äôs start talking about a solution.\nThe first part of the solution is to choose a different evaluation metric to focus on when tuning our model. We‚Äôre going to use AUC, which is short for Area Under the ROC Curve. At a high level, AUC is a measure of how well the model separates the classes by predicting higher probabilities for the class 1 samples than for the class 0 samples.\nTo explain this further, let‚Äôs have the fitted model output predicted probabilities (rather than class predictions) by using the predict_proba method. We‚Äôll store these in y_score and print them out.\nThis first number tells us that for the first sample in the test set, the model predicts a 0.18% likelihood that it belongs to the positive class. The second number tells us that for the second sample in the test set, the model predicts a 0.26% likelihood that it belongs to the positive class. And so on.\n\ny_score = logreg.predict_proba(X_test)[:, 1]\ny_score\n\narray([0.00179856, 0.00255211, 0.00181612, ..., 0.0015053 , 0.02641707,\n       0.00073432])\n\n\nAUC is a measure of how good a job the model is doing at assigning higher probabilities to class 1 samples than class 0 samples. In other words, AUC doesn‚Äôt care about the actual predicted probability values, rather it cares only about the rank ordering of the values. As such, it can be used with any classifier that outputs predicted probabilities, regardless of whether those probabilities are well-calibrated.\n\n\n\n\n\n\nArea Under the ROC Curve (AUC):\n\nMeasures how well the model separates the classes\nWants the model to assign higher probabilites to class 1 samples than class 0 samples\nCan be used with any classifier that outputs predicted probabilities\n\n\n\n\nLet‚Äôs actually calculate the AUC for our model. We import the roc_auc_score function, and then we pass it the true values followed by the predicted probabilities, not the class predictions. It outputs an AUC of 0.93, which is quite good.\n\nfrom sklearn.metrics import roc_auc_score\nroc_auc_score(y_test, y_score)\n\n0.9339323437455991\n\n\nTo put this in context, a perfect model would achieve an AUC of 1.0, whereas a completely uninformed model would achieve an AUC of 0.5.\nA more formal way to interpret this value is as follows: If you randomly chose a class 1 sample and you randomly chose a class 0 sample, there‚Äôs a 93% probability that the model assigned a higher predicted probability to the class 1 sample than the class 0 sample.\n\n\n\n\n\n\nAUC scores:\n\nPerfect model: 1.0\nUninformed model: 0.5\n\n\n\n\nA natural follow-up question might be: Why is the AUC so high, when the True Positive Rate is so low?\nTo answer that question, we have to talk about the decision threshold. The decision threshold is the predicted probability value above which a model will predict the positive class. By default, the threshold is 0.5, which means that if a model predicts a probability of 0.7, it predicts class 1. Whereas if a model predicts a probability of 0.2, it predicts class 0.\n\n\n\n\n\n\nDecision threshold:\n\nPredicted probability value above which a model will predict the positive class\nDefault threshold is 0.5:\n\nProbability of 0.7 ‚Üí predicts class 1\nProbability of 0.2 ‚Üí predicts class 0\n\n\n\n\n\nFrom looking at the confusion matrix again, we can see that there were only 38 samples in the testing set for which the model predicted class 1.\n\nconfusion_matrix(y_test, y_pred)\n\narray([[2721,   10],\n       [  37,   28]])\n\n\nEquivalently, there were only 38 samples for which the model predicted a probability greater than 0.5.\nIf you‚Äôre wondering how this code works, the part within the parentheses is creating a boolean array, and summing that array converts each True to one and each False to zero.\n\nsum(y_score &gt; 0.5)\n\n38\n\n\nWith all of that in mind, we can infer two things from the fact that the AUC is high but the True Positive Rate is low:\n\nFirst, we know from the high AUC that the model is already doing a good job separating the classes.\nSecond, we know from the low True Positive Rate that the default decision threshold is not serving us well.\n\n\n\n\n\n\n\nWhat can we infer so far?\n\nHigh AUC: Model is already doing a good job separating the classes\nLow TPR: Default decision threshold is not serving us well\n\n\n\n\nTo understand this second issue better, we‚Äôll plot the ROC curve using the plot_roc_curve function, which was introduced in version 0.22. It uses an API similar to plot_confusion_matrix, in that we pass it the fitted model, X_test, and y_test, though the API has changed starting in scikit-learn 1.0.\nSo what are we looking at? The ROC curve is a plot of the True Positive Rate (on the y-axis) versus the False Positive Rate (on the x-axis) for all possible decision thresholds.\nSo for example, with the default decision threshold of 0.5, our True Positive Rate was 43%, and our False Positive Rate was nearly 0, which is represented by a single point on the curve.\nWe could move to another point on the curve simply by changing the decision threshold. For example, we could move to a point that has a True Positive Rate of about 90% and a False Positive Rate of about 10% by changing the threshold. To be clear, the threshold itself is not shown on this plot, rather the plot only shows the True Positive Rate and False Positive Rate pairs that result from all possible thresholds.\nOne other thing I want to note about this plot is that the Area Under the Curve is literally the percentage of the plot that is underneath the curve. So because the AUC is 0.93, we know that 93% of this plot is underneath this curve. If we can increase the AUC, that means we are moving the curve further into the upper left corner, which means that higher True Positive Rates and lower False Positive Rates will be available to us.\n\nfrom sklearn.metrics import plot_roc_curve\ndisp = plot_roc_curve(logreg, X_test, y_test)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpreting the ROC curve:\n\nPlot of the TPR vs FPR for all possible decision thresholds\nMove to another point on the curve by changing the threshold\n\nThreshold is not shown on this plot\n\nAUC is the percentage of the plot underneath the curve\n\n\n\n\nGiven what we‚Äôve learned so far, I want to chart our path forward. In lesson 18.7, we‚Äôre going to try to improve the AUC of our model. Then in lesson 18.8, we‚Äôre going to explore alternative decision thresholds in order to balance the True Positive Rate and the False Positive Rate to our liking.\n\n\n\n\n\n\nNext steps:\n\nImprove the model‚Äôs AUC\nExplore alternative decision thresholds",
    "crumbs": [
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Class imbalance</span>"
    ]
  },
  {
    "objectID": "ch18.html#cost-sensitive-learning",
    "href": "ch18.html#cost-sensitive-learning",
    "title": "18¬† Class imbalance",
    "section": "18.7 Cost-sensitive learning",
    "text": "18.7 Cost-sensitive learning\nNow that we know that our next step is to improve our model‚Äôs AUC, how do we actually do that? The good news is that we can use any technique I‚Äôve covered in this book, such as hyperparameter tuning, feature selection, trying non-linear models, and so on. All of those techniques have the potential to improve the model‚Äôs AUC.\nHowever in this lesson, I want to focus on one particular technique that we haven‚Äôt covered in this book that is particularly useful in cases of class imbalance. That technique is called cost-sensitive learning.\nThe insight behind cost-sensitive learning is that not all prediction errors have the same cost. This can refer to an actual dollar cost of one type of error versus another, or in our case, the real-world implications of a certain type of error.\nWhen there‚Äôs severe class imbalance, it‚Äôs usually the case that False Negatives, in which positive samples are identified as negative, have a higher cost than False Positives, in which negative samples are identified as positive. This makes sense because the positive samples are rare occurrences, and thus we‚Äôre more interested in locating the positive samples than the negative samples. In simple terms, we would prefer a False Positive to a False Negative.\n\n\n\n\n\n\nHow to improve the model‚Äôs AUC?\n\nAny technique covered in this book:\n\nHyperparameter tuning\nFeature selection\nTrying non-linear models\nEtc.\n\nCost-sensitive learning:\n\nParticularly useful when there‚Äôs class imbalance\nInsight: Not all prediction errors have the same ‚Äúcost‚Äù\nIf positive samples are rare:\n\nFalse Negatives have a higher cost\nFalse Positives have a lower cost\n\n\n\n\n\n\nSo how does cost-sensitive learning actually work? In scikit-learn, this is implemented using the class_weight parameter for some models, such as logistic regression and Random Forests.\nBy setting class_weight to 'balanced', scikit-learn will give more weight to the samples from the minority class than samples from the majority class. More specifically, the model is penalized more for making mistakes on the minority class (meaning False Negatives) than it is for making mistakes on the majority class (meaning False Positives). Because the model‚Äôs goal is to minimize the total cost, the model may show increased bias toward predicting the minority class.\n\n\n\n\n\n\nHow does cost-sensitive learning work?\n\nGives more weight to samples from the minority class (positive class)\n\nModel is penalized more for False Negatives than False Positives\n\nModel‚Äôs goal is to minimize total cost\n\nModel may be biased toward predicting the minority class\n\n\n\n\n\nLet‚Äôs try this out by creating a logistic regression instance that uses class_weight='balanced'. This specifies a class weighting that is inversely proportional to the class frequencies in the input data, though you can specify custom weights for each class if you like.\n\nlogreg_cost = LogisticRegression(solver='liblinear',\n                                 class_weight='balanced', random_state=1)\n\nWe‚Äôll fit our logistic regression model on the training set, and use the fitted model to make class predictions as well as predicted probabilities on the testing set. Then we‚Äôll calculate the AUC, and it has increased from 0.93 to 0.94 simply by setting class weights.\nKeep in mind that class weighting is not guaranteed to improve your AUC, and thus it should be tuned like any other parameter, as we‚Äôll see in the next chapter.\n\nlogreg_cost.fit(X_train, y_train)\ny_pred = logreg_cost.predict(X_test)\ny_score = logreg_cost.predict_proba(X_test)[:, 1]\nroc_auc_score(y_test, y_score)\n\n0.9404163028476467\n\n\nLet‚Äôs take a look at the classification report to see how our rates have changed:\n\nThe True Positive Rate, which was 43%, is up to 88%.\nThe True Negative Rate, which was nearly 100%, is down to 89%, which means that the False Positive Rate has increased from around 0% to 11%.\n\nI do want to point out that even though this model might match our priorities better, its accuracy is down from 98% to 89%. This illustrates how sometimes a useful classifier has a lower accuracy than null accuracy.\n\nprint(classification_report(y_test, y_pred))\n\n              precision    recall  f1-score   support\n\n           0       1.00      0.89      0.94      2731\n           1       0.16      0.88      0.27        65\n\n    accuracy                           0.89      2796\n   macro avg       0.58      0.88      0.61      2796\nweighted avg       0.98      0.89      0.93      2796\n\n\n\n\n\n\n\n\n\nChanges due to cost-sensitive learning:\n\nTPR: 0.43 ‚Üí 0.88\nFPR: 0.00 ‚Üí 0.11",
    "crumbs": [
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Class imbalance</span>"
    ]
  },
  {
    "objectID": "ch18.html#sec-18-8",
    "href": "ch18.html#sec-18-8",
    "title": "18¬† Class imbalance",
    "section": "18.8 Tuning the decision threshold",
    "text": "18.8 Tuning the decision threshold\nAt this point, we could continue to tune various aspects of our model to increase the AUC, but instead we‚Äôre just going to move on to tuning the decision threshold.\nLet‚Äôs take a look at the ROC curve for our class-weighted logistic regression model. Using the default threshold of 0.5, the model achieved a True Positive Rate of 88% and a False Positive Rate of 11%, which is represented by a single point on the curve. If we want to move somewhere else on this curve, in order to better match our priorities, we simply change the threshold.\n\ndisp = plot_roc_curve(logreg_cost, X_test, y_test)\n\n\n\n\n\n\n\n\nBefore tuning the threshold, it‚Äôs useful to examine the confusion matrix. This is our current confusion matrix, which results from the default threshold of 0.5. You might notice that there are a lot more True Positives and False Positives than before.\n\nconfusion_matrix(y_test, y_pred)\n\narray([[2436,  295],\n       [   8,   57]])\n\n\nMore specifically, the model is predicting the positive class 352 times, whereas previously it predicted the positive class only 38 times.\n\nsum(y_score &gt; 0.5)\n\n352\n\n\nLet‚Äôs pretend that we‚Äôre still uncomfortable with having 8 False Negatives, and thus we want to reduce those even further. By decreasing the threshold to 0.25, we find out that the model will predict the positive class 870 times.\n\nsum(y_score &gt; 0.25)\n\n870\n\n\nThe boolean array created by this condition can be converted to class predictions simply by multiplying it by 1.\n\n(y_score &gt; 0.25) * 1\n\narray([0, 0, 0, ..., 0, 1, 0])\n\n\nAnd in fact, the boolean array can be passed directly to the confusion matrix function, and it will do the conversion automatically.\nThus we can see that by decreasing the threshold to 0.25, the number of False Negatives has been reduced from 8 to 4, but the number of False Positives has increased from 295 to 809. More generally, decreasing the threshold moves samples from the left column of the confusion matrix to the right column.\n\nconfusion_matrix(y_test, y_score &gt; 0.25)\n\narray([[1922,  809],\n       [   4,   61]])\n\n\nLooking at the classification report, the True Positive Rate has increased to 94%, but the False Positive Rate has increased to 30%. That moves us to a new spot on the ROC curve.\n\nprint(classification_report(y_test, y_score &gt; 0.25))\n\n              precision    recall  f1-score   support\n\n           0       1.00      0.70      0.83      2731\n           1       0.07      0.94      0.13        65\n\n    accuracy                           0.71      2796\n   macro avg       0.53      0.82      0.48      2796\nweighted avg       0.98      0.71      0.81      2796\n\n\n\n\n\n\n\n\n\nChanges due to decreasing the threshold:\n\nTPR: 0.88 ‚Üí 0.94\nFPR: 0.11 ‚Üí 0.30\n\n\n\n\nAlternatively, let‚Äôs pretend that we actually think the original threshold resulted in too many False Positives. In that case, we could increase the threshold to 0.75, which moves samples from the right column to the left column.\n\nconfusion_matrix(y_test, y_score &gt; 0.75)\n\narray([[2632,   99],\n       [  15,   50]])\n\n\nWe can see from the classification report that the True Positive Rate has decreased to 77%, but the False Positive Rate has decreased to 4%. Again, that moves us to a new spot on the ROC curve.\n\nprint(classification_report(y_test, y_score &gt; 0.75))\n\n              precision    recall  f1-score   support\n\n           0       0.99      0.96      0.98      2731\n           1       0.34      0.77      0.47        65\n\n    accuracy                           0.96      2796\n   macro avg       0.66      0.87      0.72      2796\nweighted avg       0.98      0.96      0.97      2796\n\n\n\n\n\n\n\n\n\nChanges due to increasing the threshold:\n\nTPR: 0.88 ‚Üí 0.77\nFPR: 0.11 ‚Üí 0.04\n\n\n\n\nKeep in mind that as we change the threshold, it‚Äôs not actually changing the model itself. Instead, changing the threshold is just a way to make tradeoffs between two different types of errors, namely False Positives and False Negatives.\nAnd to be clear, there is no correct threshold that we are trying to find. Instead, the threshold you should choose is the one that best matches your priorities. A method does exist for finding the point on the ROC curve that is closest to the upper left corner and calling that the correct threshold, but again, that threshold is only useful if it matches your priorities.",
    "crumbs": [
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Class imbalance</span>"
    ]
  },
  {
    "objectID": "ch19.html",
    "href": "ch19.html",
    "title": "19¬† Class imbalance walkthrough",
    "section": "",
    "text": "19.1 Best practices for class imbalance\nWe covered a lot of concepts in the previous chapter that may have been new to you: class imbalance, the confusion matrix and all the rates you can calculate from it, the classification report, ROC curves and AUC, the decision threshold, and cost-sensitive learning.\nIn order to properly demonstrate these concepts, I modified my actual workflow in two important ways that are slightly less than optimal. In this chapter, I‚Äôm going to walk through my real workflow for class imbalance problems so that you can see the best practices.\nSo what are the two modifications that I made?\nIn this chapter, I‚Äôm going to walk through my entire workflow, start to finish, while making these two improvements.",
    "crumbs": [
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Class imbalance walkthrough</span>"
    ]
  },
  {
    "objectID": "ch19.html#best-practices-for-class-imbalance",
    "href": "ch19.html#best-practices-for-class-imbalance",
    "title": "19¬† Class imbalance walkthrough",
    "section": "",
    "text": "New concepts from chapter 18:\n\nClass imbalance\nConfusion matrix, TPR, TNR, FPR\nClassification report\nROC curve and AUC\nDecision threshold\nCost-sensitive learning\n\n\n\n\n\n\n\nIn the previous chapter, I used train/test split for model evaluation instead of cross-validation. Cross-validation is better than train/test split because it‚Äôs easier to use and it provides more reliable estimates of model performance.\nIn the previous chapter, I tuned the decision threshold using the same dataset that I used to optimize the model. It‚Äôs actually better to tune the threshold using different data than you used to optimize the model, because it has been shown experimentally to lead to more reliable estimates of True Positive Rate and False Positive Rate.\n\n\n\n\n\n\n\nWorkflow improvements in chapter 19:\n\nUse cross-validation instead of train/test split\n\nEasier to use\nMore reliable performance estimates\n\nUse new data to tune the decision threshold\n\nMore reliable TPR/FPR estimates",
    "crumbs": [
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Class imbalance walkthrough</span>"
    ]
  },
  {
    "objectID": "ch19.html#step-1-splitting-the-dataset",
    "href": "ch19.html#step-1-splitting-the-dataset",
    "title": "19¬† Class imbalance walkthrough",
    "section": "19.2 Step 1: Splitting the dataset",
    "text": "19.2 Step 1: Splitting the dataset\nStep 1 of my real workflow is to split the dataset into training and testing sets. This might be a confusing way to start, because I just said that we‚Äôre going to use cross-validation in this chapter rather than train/test split.\nThat is absolutely true, but we‚Äôre actually using the train_test_split function differently than we did in chapter 18:\n\nIn chapter 18, we used train/test split for model evaluation.\nIn this chapter, we‚Äôre using it to set aside independent data for tuning the decision threshold.\n\nFor a longer discussion of this concept, you can review lesson¬†10.12.\n\n\n\n\n\n\nDifferent uses of train/test split:\n\nChapter 18: Set aside data for model evaluation\nChapter 19: Set aside data for tuning the decision threshold\n\n\n\n\nAnyway, our train_test_split code is identical to the code that we used in lesson¬†18.3, including the use of stratified sampling.\n\nX_train, X_test, y_train, y_test = train_test_split(scan_X, scan_y,\n                                                    test_size=0.25,\n                                                    random_state=1,\n                                                    stratify=scan_y)",
    "crumbs": [
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Class imbalance walkthrough</span>"
    ]
  },
  {
    "objectID": "ch19.html#step-2-optimizing-the-model-on-the-training-set",
    "href": "ch19.html#step-2-optimizing-the-model-on-the-training-set",
    "title": "19¬† Class imbalance walkthrough",
    "section": "19.3 Step 2: Optimizing the model on the training set",
    "text": "19.3 Step 2: Optimizing the model on the training set\nStep 2 is to optimize your model (or Pipeline) using the training set only.\nAs you can see, I‚Äôm using cross-validation as the evaluation procedure, and AUC as the evaluation metric that I want to optimize. But notice that I‚Äôm passing the training set only to cross_val_score. I‚Äôm not touching the testing set during this step so that it can function as an independent dataset for the next step.\n\ncross_val_score(logreg, X_train, y_train, cv=5, scoring='roc_auc').mean()\n\n0.9137689754365026\n\n\nTo optimize the model, we‚Äôll use a grid search. Normally this would be a grid search of all pipeline steps, but in this case it will be a grid search of just the model.\nSince cost-sensitive learning is useful when there‚Äôs class imbalance, we‚Äôre going to include the class_weight parameter in our search. There are four options I‚Äôll try:\n\nNone is the default, and it means don‚Äôt use cost-sensitive learning.\n'balanced' is what we used previously, and it specifies class weights that are inversely proportional to the class frequencies in the input data. Since our input data is about 98% class 0 and 2% class 1, it would apply a weight of 2 to class 0 and a weight of 98 to class 1.\nThe third option is custom weights, which we specify with a dictionary. Using {0:1, 1:99} means for class 0, I want a weight of 1, and for class 1, I want a weight of 99. In other words, I‚Äôm applying an even higher weight to class 1 than the 'balanced' option.\nThe fourth option is another set of custom weights, namely {0:3, 1:97}, in which I‚Äôm applying a slightly lower weight to class 1 than the 'balanced' option. Note that the weights can be any numbers and don‚Äôt actually have to add up to 100 like I‚Äôm doing here.\n\n\nim_params = {}\nim_params['penalty'] = ['l1', 'l2']\nim_params['C'] = [0.1, 1, 10]\nim_params['class_weight'] = [None, 'balanced', {0:1, 1:99}, {0:3, 1:97}]\n\nNow that we‚Äôve set up the parameters to be searched, we can pass them to GridSearchCV and use AUC as the metric. When doing the search, again we only pass it the training set.\nThe search results in an AUC of 0.923, which is higher than the unoptimized model we passed to cross_val_score.\n\ntraining_grid = GridSearchCV(logreg, im_params, cv=5, scoring='roc_auc',\n                             n_jobs=-1)\ntraining_grid.fit(X_train, y_train)\ntraining_grid.best_score_\n\n0.9227187648199522\n\n\nHere‚Äôs the best set of parameters it found, which actually uses one of the custom class weights.\n\ntraining_grid.best_params_\n\n{'C': 1, 'class_weight': {0: 1, 1: 99}, 'penalty': 'l1'}\n\n\nNow that we‚Äôve found the best set of parameters, we need to save the model with those parameters as an object.\nWe could try to further optimize the model by creating more features, using feature selection, trying a different type of model, and so on, but instead we‚Äôre just going to move on to the next step.\n\nbest_model = training_grid.best_estimator_",
    "crumbs": [
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Class imbalance walkthrough</span>"
    ]
  },
  {
    "objectID": "ch19.html#step-3-evaluating-the-model-on-the-testing-set",
    "href": "ch19.html#step-3-evaluating-the-model-on-the-testing-set",
    "title": "19¬† Class imbalance walkthrough",
    "section": "19.4 Step 3: Evaluating the model on the testing set",
    "text": "19.4 Step 3: Evaluating the model on the testing set\nStep 3 is to use our best model to make predictions for the testing set and evaluate those predictions. Again, we did not touch the testing set during step 2, so the model has never seen this data.\nWe‚Äôll use the predict method to generate class predictions, and the predict_proba method to generate predicted probabilities.\n\ny_pred = best_model.predict(X_test)\ny_score = best_model.predict_proba(X_test)[:, 1]\n\nWe‚Äôll evaluate the predicted probabilities using AUC and the ROC curve. The AUC is 0.94, which is our best estimate of how well the trained model will perform on truly new data.\n\nroc_auc_score(y_test, y_score)\n\n0.9375151395656705\n\n\n\ndisp = plot_roc_curve(best_model, X_test, y_test)\n\n\n\n\n\n\n\n\nWe‚Äôll evaluate the class predictions using a confusion matrix and the classification report. Notice that the True Positive Rate (class 1 recall) is 95%, and the False Positive Rate (1 - class 0 recall) is 24%.\n\nconfusion_matrix(y_test, y_pred)\n\narray([[2082,  649],\n       [   3,   62]])\n\n\n\nprint(classification_report(y_test, y_pred))\n\n              precision    recall  f1-score   support\n\n           0       1.00      0.76      0.86      2731\n           1       0.09      0.95      0.16        65\n\n    accuracy                           0.77      2796\n   macro avg       0.54      0.86      0.51      2796\nweighted avg       0.98      0.77      0.85      2796",
    "crumbs": [
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Class imbalance walkthrough</span>"
    ]
  },
  {
    "objectID": "ch19.html#step-4-tuning-the-decision-threshold",
    "href": "ch19.html#step-4-tuning-the-decision-threshold",
    "title": "19¬† Class imbalance walkthrough",
    "section": "19.5 Step 4: Tuning the decision threshold",
    "text": "19.5 Step 4: Tuning the decision threshold\nStep 4 is to tune the decision threshold based on our priorities, meaning our tolerance of False Negatives versus False Positives.\nThis is the same process you saw in lesson¬†18.8, except this time, we‚Äôre tuning the threshold using a testing set that the model never saw when it was being optimized. Again, this is important because it will lead to a more reliable estimate of the True Positive and False Positive Rates.\nAnyway, let‚Äôs pretend that we would prefer a slightly lower False Positive Rate, and are willing to tolerate a lower True Positive Rate in order to achieve it. As such, we‚Äôll increase the decision threshold slightly to 0.55.\n\nconfusion_matrix(y_test, y_score &gt; 0.55)\n\narray([[2179,  552],\n       [   5,   60]])\n\n\n\nprint(classification_report(y_test, y_score &gt; 0.55))\n\n              precision    recall  f1-score   support\n\n           0       1.00      0.80      0.89      2731\n           1       0.10      0.92      0.18        65\n\n    accuracy                           0.80      2796\n   macro avg       0.55      0.86      0.53      2796\nweighted avg       0.98      0.80      0.87      2796\n\n\n\nThe True Positive Rate has decreased from 95% to 92%, and the False Positive Rate has decreased from 24% to 20%. Let‚Äôs assume we‚Äôre happy with these numbers, and we‚Äôll move on to the final step.\n\n\n\n\n\n\nChanges due to increasing the threshold:\n\nTPR: 0.95 ‚Üí 0.92\nFPR: 0.24 ‚Üí 0.20",
    "crumbs": [
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Class imbalance walkthrough</span>"
    ]
  },
  {
    "objectID": "ch19.html#step-5-retraining-the-model-and-making-predictions",
    "href": "ch19.html#step-5-retraining-the-model-and-making-predictions",
    "title": "19¬† Class imbalance walkthrough",
    "section": "19.6 Step 5: Retraining the model and making predictions",
    "text": "19.6 Step 5: Retraining the model and making predictions\nStep 5 is to use the decision threshold we chose when making predictions on new data.\nBefore making predictions, it‚Äôs critical that we train our best model on all of our data, meaning the entirety of scan_X and scan_y, otherwise we‚Äôre throwing away valuable data. In other words, we‚Äôre using the hyperparameters that were chosen from the training data during step 2, and fitting that model with all of the data.\n\nbest_model.fit(scan_X, scan_y)\n\nLogisticRegressionLogisticRegression(C=1, class_weight={0: 1, 1: 99}, penalty='l1',\n                   random_state=1, solver='liblinear')\n\n\nWe‚Äôll use that model to make predictions on new data, meaning data for which you don‚Äôt know the actual class labels. I don‚Äôt have access to any new data, so I‚Äôm just going to create some random data for demonstration purposes.\nI‚Äôll set NumPy‚Äôs random seed for reproducibility, and then use the randint function to create a 4 by 6 array with random integers between 0 and 2. In other words, this is a simulation of 4 samples of new data, each of which has 6 features.\n\nnp.random.seed(1)\nscan_X_new = np.random.randint(0, 3, (4, 6))\nscan_X_new\n\narray([[1, 0, 0, 1, 1, 0],\n       [0, 1, 0, 1, 0, 2],\n       [1, 2, 0, 2, 1, 2],\n       [0, 0, 2, 0, 1, 2]])\n\n\nTo make predictions, we‚Äôll pass the new data to the predict_proba method, and save the output.\n\nscan_y_new_score = best_model.predict_proba(scan_X_new)[:, 1]\n\nFinally, we‚Äôll predict class 1 any time the predicted probability is greater than our decision threshold of 0.55, otherwise we‚Äôll predict class 0. These are our class predictions for the 4 new samples.\n\n(scan_y_new_score &gt; 0.55) * 1\n\narray([1, 0, 1, 0])",
    "crumbs": [
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Class imbalance walkthrough</span>"
    ]
  },
  {
    "objectID": "ch19.html#qa-should-i-use-an-roc-curve-or-a-precision-recall-curve",
    "href": "ch19.html#qa-should-i-use-an-roc-curve-or-a-precision-recall-curve",
    "title": "19¬† Class imbalance walkthrough",
    "section": "19.7 Q&A: Should I use an ROC curve or a precision-recall curve?",
    "text": "19.7 Q&A: Should I use an ROC curve or a precision-recall curve?\nOne alternative to ROC curves that you might have heard about is the precision-recall curve. In this lesson, I‚Äôll explain the precision-recall curve and then compare it to the ROC curve.\nTo start, I want to look at the confusion matrix for our best model from earlier in the chapter. I‚Äôm going to fit the model on X_train and y_train, and then generate class predictions and predicted probabilities for X_test.\n\nbest_model.fit(X_train, y_train)\ny_pred = best_model.predict(X_test)\ny_score = best_model.predict_proba(X_test)[:, 1]\n\nThen, we can create the confusion matrix by comparing the true values with the predicted values. There are two rates that we‚Äôll calculate from the confusion matrix this time.\n\nconfusion_matrix(y_test, y_pred)\n\narray([[2082,  649],\n       [   3,   62]])\n\n\nThe first rate is recall, which is just another name for True Positive Rate. It answers the question: When cancer is present, how often is that correctly predicted? We divide the True Positives by the entire bottom row, and we get 95%.\n\n62 / (3 + 62)\n\n0.9538461538461539\n\n\nThe second rate we‚Äôll calculate is called precision. It answers the question: When cancer is predicted, how often is that prediction correct? We divide the True Positives by the entire right column, and we get 9%. Notice that unlike all of our other calculations, the denominator for precision is a column total rather than a row total.\n\n62 / (649 + 62)\n\n0.08720112517580872\n\n\n\n\n\n\n\n\nCalculated rates:\n\nRecall: 0.95\nPrecision: 0.09\n\n\n\n\nBoth precision and recall are listed in the classification report. We just calculated the precision and recall for class 1, as you can see here.\n\nprint(classification_report(y_test, y_pred))\n\n              precision    recall  f1-score   support\n\n           0       1.00      0.76      0.86      2731\n           1       0.09      0.95      0.16        65\n\n    accuracy                           0.77      2796\n   macro avg       0.54      0.86      0.51      2796\nweighted avg       0.98      0.77      0.85      2796\n\n\n\nNow that we understand precision and recall, let‚Äôs plot the precision-recall curve using the plot_precision_recall_curve function, which was introduced in version 0.22. It uses an API similar to plot_roc_curve in that we pass it the fitted model, X_test, and y_test, though the API has changed starting in scikit-learn 1.0.\nSo what are we looking at? The precision-recall curve is a plot of precision (on the y-axis) versus recall (on the x-axis) for all possible decision thresholds. This is very similar to an ROC curve in that it can help you to tune the decision threshold of your model based on your priorities.\nAnd just like you can summarize an ROC curve by calculating the area underneath it, you can summarize a precision-recall curve by calculating the area underneath it. There are multiple ways to do this calculation, but the metric used in scikit-learn is called average precision.\n\nfrom sklearn.metrics import plot_precision_recall_curve\ndisp = plot_precision_recall_curve(best_model, X_test, y_test)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpreting the precision-recall curve:\n\nPlot of precision vs recall for all possible decision thresholds\nMove to another point on the curve by changing the threshold\nAverage precision is the percentage of the plot underneath the curve\n\n\n\n\nTo calculate average precision, we import average_precision_score and pass it the true values and the predicted probabilities. It outputs a score of 0.55.\n\nfrom sklearn.metrics import average_precision_score\naverage_precision_score(y_test, y_score)\n\n0.5475069395983533\n\n\nTo put this in context, a perfect model would achieve a score of 1.0, whereas a completely uninformed model would achieve a score equivalent to the fraction of positive samples, which in this case is about 0.02.\nAs a reminder, the AUC of an uninformed model is 0.5, which means that AUC and average precision usually have very different baseline scores for the same problem.\n\n\n\n\n\n\nPrecision-recall scores:\n\nPerfect model: 1.0\nUninformed model: 0.02 in this case (fraction of positive samples)\n\n\n\n\nIn this chapter, I recommended using AUC as your primary evaluation metric in cases of class imbalance. However, there are many people who recommend using average precision instead. Which metric should you use?\nFirst, let me share the most common critique of AUC, and then I‚Äôll share my response. Looking at our confusion matrix might be helpful to you during this discussion, so I‚Äôll display it again.\n\nconfusion_matrix(y_test, y_pred)\n\narray([[2082,  649],\n       [   3,   62]])\n\n\nLet‚Äôs quickly run through the rates again:\n\nTrue Positive Rate (or recall) is 62 out of 65, which is 95%.\nFalse Positive Rate is 649 out of 2731, which is 24%.\nPrecision is 62 out of 711, which is 9%.\n\n\n\n\n\n\n\nCalculated rates:\n\nTPR / Recall: 0.95\nFPR: 0.24\nPrecision: 0.09\n\n\n\n\nWith that in mind, I‚Äôll do my best to summarize the most common critique of AUC in cases of class imbalance, which is as follows:\nIn cases of severe class imbalance, there will be a very large number of True Negatives. As such, the False Positive Rate will be artificially low, and thus the AUC will be artificially high, and so AUC will no longer provide a realistic estimate of the model‚Äôs performance.\n\n\n\n\n\n\nCritique of AUC in cases of class imbalance:\n\nResults of severe class imbalance:\n\nVery large number of True Negatives\nFPR will be artificially low\nAUC will be artificially high and is no longer realistic\n\nExample:\n\nIncrease True Negatives from 2082 to 200000\nFPR would drop from 0.24 to 0.003\nAUC would increase\nPrecision would still be 0.09\n\nProposed solution:\n\nUse precision-recall curve and average precision\nMore realistic because it ignores the number of True Negatives\n\n\n\n\n\nAs an example of this, imagine that we increased the number of True Negatives from 2082 to 200000 but left all the other values the same, as shown below. The False Positive Rate would drop from 24% to 0.3%, and the AUC would increase (though we can‚Äôt say by how much, since you can‚Äôt calculate AUC from a confusion matrix). Thus the model would look great based on the AUC, even though the model‚Äôs precision is still only 9%.\nThe solution, according to this critique, is to use the precision-recall curve and average precision, since it will provide a more realistic estimate of the model‚Äôs performance by ignoring the number of True Negatives.\n\n\n\n\n\n\n\n\n\n¬†\nPredicted 0\nPredicted 1\n\n\n\n\nTrue label 0\n200000\n649\n\n\nTrue label 1\n3\n62\n\n\n\n\n\n\nI have three responses to this critique.\nMy first response is that neither AUC nor average precision is inherently better, rather it depends on what you‚Äôre trying to measure:\n\nAUC focuses on both classes. In the context of this problem, AUC measures how skillfully the model finds cancer when it‚Äôs present, balanced against how skillfully the model doesn‚Äôt find cancer when it‚Äôs not present.\nAverage precision, on the other hand, focuses only on the positive class. Because both precision and recall ignore the number of True Negatives, the average precision will not change regardless of whether there are two thousand or two million True Negatives.\n\nUltimately, the choice between AUC and average precision is as follows:\n\nIf you‚Äôre interested in a model‚Äôs performance across both classes, then AUC is the better choice.\nIf you‚Äôre only interested in how well the model locates the positive class, then average precision is the better choice.\n\nIn our particular case of detecting cancer, my judgment is that the performance on both classes is relevant, and thus AUC is the better choice.\nMy second response to this critique is that in cases of severe class imbalance, even if you think the False Positive Rate is artificially low, which makes the model look really good, it‚Äôs just as fair to call the precision artificially low, which makes the model look really bad.\nFor example, I would judge the model shown in this latest confusion matrix to be very good:\n\nIf someone doesn‚Äôt have cancer, they only have a 0.3% chance of being told incorrectly that they do. (649 / 200649 = 0.003)\nIf someone does have cancer, they only have a 5% chance of being told incorrectly that they don‚Äôt. (3 / 65 = 0.05)\n\nDespite these characteristics, the model still has a precision of just 9%. Even if we moved all 3 False Negatives to the True Positive box, resulting in a True Positive Rate of 100%, the precision would still be 9%, making it sound like a very poor model.\nMy third response to this critique is that the actual AUC score is irrelevant, and thus it doesn‚Äôt matter if the AUC is artificially high.\nThe only reason we‚Äôre using any evaluation metric during model tuning is that we need some relevant metric to maximize in order to choose between models. AUC fits this purpose well because it measures how skillfully the model is separating the classes. Once you‚Äôve maximized AUC, then you can change the decision threshold to balance the True Positive Rate and False Positive Rate according to your priorities. But the AUC score itself is never your business objective, so it doesn‚Äôt matter if the AUC is artificially high.\n\n\n\n\n\n\nMy responses to this crititque:\n\nNeither metric is inherently better:\n\nAUC focuses on both classes\nAverage precision focuses on positive class only\nBoth classes are relevant in this case\n\nFPR and precision are both artificially low:\n\nExcellent model can still have a low precision\nExample: Model can have FPR of 0.003 and TPR of 1.0, but precision of 0.09\n\nAUC score itself is irrelevant:\n\nOur goal is to choose between models\nMaximizing AUC helps you choose the most skillful model\nBalance TPR and FPR based on your priorities\nAUC score itself is never your business objective\n\n\n\n\n\nThe bottom line is as follows:\n\nBoth AUC and average precision are reasonable metrics to maximize even in cases of class imbalance.\nNeither metric is a perfect representation of a model‚Äôs performance.\nChoose AUC if you‚Äôre interested in the model‚Äôs performance across both classes, and choose average precision if you‚Äôre only interested in how well the model locates the positive class.\n\n\n\n\n\n\n\nSummary of AUC vs average precision:\n\nBoth are reasonable metrics to maximize (even with class imbalance)\nNeither metric perfectly captures model performance\nAUC focuses on both classes, average precision focuses on positive class only",
    "crumbs": [
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Class imbalance walkthrough</span>"
    ]
  },
  {
    "objectID": "ch19.html#qa-can-i-use-a-different-metric-such-as-f1-score",
    "href": "ch19.html#qa-can-i-use-a-different-metric-such-as-f1-score",
    "title": "19¬† Class imbalance walkthrough",
    "section": "19.8 Q&A: Can I use a different metric such as F1 score?",
    "text": "19.8 Q&A: Can I use a different metric such as F1 score?\nThere are many other metrics that are popular to use in cases of class imbalance, such as the F1 score or F-beta score, balanced accuracy, Cohen‚Äôs kappa, and Matthews correlation coefficient.\n\n\n\n\n\n\nAlternative metrics in cases of class imbalance:\n\nF1 score or F-beta score\nBalanced accuracy\nCohen‚Äôs kappa\nMatthews correlation coefficient\n\n\n\n\nHowever, all of these metrics require you to choose a decision threshold, whereas AUC and average precision capture the performance of a classifier across all possible thresholds. Thus by using AUC or average precision as your evaluation metric, you can first maximize the overall performance of your classifier during model tuning, and then you can alter the decision threshold to match your priorities.\n\n\n\n\n\n\nAdvantage of AUC and average precision:\n\nThey don‚Äôt require you to choose a decision threshold\nYou can first maximize classifier performance, then alter the decision threshold\n\n\n\n\nIf you were instead trying to maximize F1 score (for example) during model tuning, you‚Äôll be optimizing the model‚Äôs hyperparameters based on the default decision threshold of 0.5, but that threshold might be far from optimal for your given problem. In other words, you might miss out on a more optimal model because you were tuning it based on a non-optimal decision threshold.\n\n\n\n\n\n\nDisadvantage of F1 score (and others) during model tuning:\n\nYou‚Äôre optimizing hyperparameters based on a decision threshold of 0.5\nAn alternative decision threshold might lead to a more optimal model\n\n\n\n\nIf you do want to use F1 score (or any of these other metrics), I would recommend using them only to help you choose between different decision thresholds, after your model has already been optimized for either AUC or average precision.",
    "crumbs": [
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Class imbalance walkthrough</span>"
    ]
  },
  {
    "objectID": "ch19.html#qa-should-i-use-resampling-to-fix-class-imbalance",
    "href": "ch19.html#qa-should-i-use-resampling-to-fix-class-imbalance",
    "title": "19¬† Class imbalance walkthrough",
    "section": "19.9 Q&A: Should I use resampling to fix class imbalance?",
    "text": "19.9 Q&A: Should I use resampling to fix class imbalance?\nIn cases of class imbalance, there is a set of techniques collectively known as ‚Äúresampling‚Äù that is often used. Resampling refers to any technique that transforms the training data in order to achieve more balance between the classes. In other words, resampling attempts to fix the class imbalance at the dataset level rather than working around it, which is what we‚Äôve done in this chapter.\n\n\n\n\n\n\nWhat is resampling?\n\nTransforming the training data in order to balance the classes\nFixes class imbalance at the dataset level\n\n\n\n\nHere are the two most common resampling approaches:\n\nUndersampling (or downsampling) is the process of deleting samples from the majority class.\nOversampling (or upsampling) is the process of creating new samples from the minority class, either by duplicating existing samples or by simulating new samples. One popular oversampling method that simulates new samples is SMOTE.\n\nBoth of these approaches can be done in either a directed, strategic fashion or in a random fashion. Or they can be done together, in which you both undersample and oversample.\nRegardless of the specific approach, keep in mind that the act of resampling risks deleting important samples and/or adding meaningless new samples.\n\n\n\n\n\n\nCommon resampling approaches:\n\nUndersampling (downsampling):\n\nDeleting samples from the majority class\nRisk of deleting important samples\n\nOversampling (upsampling):\n\nCreating new samples of the minority class\nDone through duplication or simulation (SMOTE)\nRisk of adding meaningless new samples\n\n\n\n\n\nAll of that being said, is resampling actually helpful? Experimental results show that resampling methods can be helpful, but are not always helpful. And while there are dozens of different resampling algorithms, no one algorithm works best across all datasets and models, meaning that it‚Äôs hard to give practical advice for which one to use.\n\n\n\n\n\n\nIs resampling helpful?\n\nSometimes helpful, sometimes not\nNo one algorithm works best across all datasets and models\n\n\n\n\nIf you decide to pursue resampling, keep in mind that it‚Äôs not yet supported by scikit-learn, though it may eventually be available. In the meantime, you can use the imbalanced-learn library, which is supposed to be fully compatible with scikit-learn. Personally, I tend not to use this approach in order to avoid adding additional complexity or project dependencies.\n\n\n\n\n\n\nHow to implement resampling:\n\nNot yet supported by scikit-learn\nUse imbalanced-learn library (compatible with scikit-learn)\n\n\n\n\nHere are two guidelines for the proper use of resampling:\n\nFirst, you should treat resampling like any other preprocessing technique. Namely, it should be included in a Pipeline in order to avoid data leakage.\nSecond, the resampling technique should only ever be applied to training data, and not testing data. The model should always be tested on the natural, imbalanced data so that it can output a realistic estimate of model performance.\n\n\n\n\n\n\n\nAdvice for proper resampling:\n\nTreat like any other preprocessing technique\n\nInclude in a Pipeline to avoid data leakage\n\nOnly apply to training data\n\nModel should be tested on natural, imbalanced data",
    "crumbs": [
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Class imbalance walkthrough</span>"
    ]
  },
  {
    "objectID": "ch20.html",
    "href": "ch20.html",
    "title": "20¬† Going further",
    "section": "",
    "text": "20.1 Q&A: How do I read the scikit-learn documentation?\nIn order to become truly proficient with scikit-learn and go beyond what I‚Äôve covered in this book, you need to be able to read the documentation. In this lesson, I‚Äôm going to walk through the five main pages and page types that you need to be familiar with.\nThe first page is the API reference, which you can get to by clicking on API from the top navigation bar. The API Reference gives you a high-level view of everything available in scikit-learn. Namely, it lists all of the classes and functions, organized by module.\nFor example, this is the compose module. There‚Äôs a brief description of the module, a link to the relevant section of the User Guide, and descriptions of the two classes and two functions contained in the module. If we wanted to learn more about ColumnTransformer, we could click on it and be taken to the class documentation.\nThe class documentation is the second type of page you need to be familiar with. It gives you a detailed view of a class, in this case the ColumnTransformer class. At the top is the class signature, which lists the parameters and their default values. Starting in version 0.23, everything after the * needs to be passed as a keyword argument, also known as a named argument, rather than by position. There‚Äôs also a link to the source code for the class.\nNext is the class description, a link to the relevant section of the User Guide, and sometimes the version in which the class was introduced. Next you‚Äôll see the same parameters from the class signature, except here it provides a detailed description of each parameter and the expected data type. Below the parameters are the attributes. As a reminder, attributes will end with an underscore if they are learned or estimated from the data during the fit step.\nNext are links to any related functions or classes, any important notes, and some simple usage examples. The next big section is a list of the methods. It starts with a description of each method, and below that you can see the parameters and return values for each method. Again, you can click on ‚Äúsource‚Äù to view the source code for any method. Finally, there are links to examples that use this class.\nLet‚Äôs scroll back to the top of the page, and then click over to the User Guide for ColumnTransformer. The User Guide is the third type of page you need to be familiar with, and you‚Äôll know you‚Äôre in the User Guide when you see the numbered sections. The User Guide is more like a tutorial, because it explains why you might want to use a particular class and advice for how to use that class properly. It often discusses related functions and classes, and includes additional usage examples.\nLet‚Äôs go back to the class documentation for ColumnTransformer, scroll to the bottom, and click on an example. Examples are the fourth type of page you need to be familiar with. Examples vary in structure and length, but their distinguishing feature is that they demonstrate how to solve a particular problem from start to finish. You‚Äôll always see imports at the top, and buttons on the right to run the example online or download the example to your computer.\nLet‚Äôs go back to the class documentation for ColumnTransformer one more time, and then scroll up to parameters. Sometimes in the documentation, you‚Äôll see highlighted terms that are not classes or functions, such as ‚Äúfit‚Äù and ‚Äútransform‚Äù in this case. If you click on it, you‚Äôll be taken to the Glossary, which is the fifth page you need to be familiar with. The Glossary defines almost all of the important terms used in the scikit-learn documentation. If you just want to browse through it, you can access it through the More menu in the top navigation bar.\nLet me now summarize the five pages and page types we walked through, and describe how I use them:\nIf you‚Äôre ever unsure where to start, you can click on User Guide in the top navigation bar and just start browsing the section that seems most relevant, or you can do a search in the search box.",
    "crumbs": [
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Going further</span>"
    ]
  },
  {
    "objectID": "ch20.html#qa-how-do-i-read-the-scikit-learn-documentation",
    "href": "ch20.html#qa-how-do-i-read-the-scikit-learn-documentation",
    "title": "20¬† Going further",
    "section": "",
    "text": "The first page is the API reference. I go here when I want to see what classes or functions are in a particular module. For example, if I wanted to review all of the classes and functions that are available for preprocessing, the API reference is the fastest way to do this.\nThe second page type is the class documentation. I go here any time I need to understand a particular class in-depth, especially all of its parameters and attributes.\nThe third page type is the User Guide. I go here when I need more context about a particular class or function, or advice about how to use it properly.\nThe fourth page type is Examples. I go here when I need to see a more complex usage example of a particular class, since the examples in the class documentation and User Guide are purposefully simple.\nThe fifth page is the Glossary. I go here when I need to understand a particular term and it‚Äôs not covered in the User Guide.\n\n\n\n\n\n\n\n\nKey pages in the documentation:\n\nAPI reference: List of classes and functions in each module\nClass documentation: Detailed view of a class\nUser Guide: Advice for proper usage of a class or function\nExamples: More complex usage examples\nGlossary: Definitions of important terms",
    "crumbs": [
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Going further</span>"
    ]
  },
  {
    "objectID": "ch20.html#qa-how-do-i-stay-up-to-date-with-new-scikit-learn-features",
    "href": "ch20.html#qa-how-do-i-stay-up-to-date-with-new-scikit-learn-features",
    "title": "20¬† Going further",
    "section": "20.2 Q&A: How do I stay up-to-date with new scikit-learn features?",
    "text": "20.2 Q&A: How do I stay up-to-date with new scikit-learn features?\nAfter every major release of scikit-learn, I recommend reviewing the Release Highlights, which are linked from the top of the home page. This page summarizes a small subset of updates that the scikit-learn developers have judged to be especially important or exciting.\nI also recommend that you review the detailed release notes, which are linked from the Release Highlights and are also linked from the More menu under ‚ÄúRelease History‚Äù. This will give you the most comprehensive look at all of the new features, enhancements, and API changes. Even if you aren‚Äôt upgrading right away, it can help you to decide whether to upgrade and warn you about future API changes so that you can start preparing your code now.\nReading this page might seem like an overwhelming task given its length, but my recommendation is to skip past any modules that you never import and only read through the modules that you actually use.\nWhile reading about a particular change, you can click through to the class documentation for more details. If you still have questions after reading the class documentation, you can click on the number on the right to read through the GitHub pull request that introduced this feature or change. Often, the pull request will include a long conversation about the reasons for the change and why it‚Äôs designed in a certain way. These kinds of details are not always captured in the documentation, and they can help you to build a more in-depth knowledge of scikit-learn.\n\n\n\n\n\n\nPages to review after each release:\n\nRelease Highlights: Most important or exciting changes\nDetailed release notes: All new features, enhancements, and API changes\n\nOnly review the modules you use\nRead the class documentation for more details\nRead the pull request for further context",
    "crumbs": [
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Going further</span>"
    ]
  },
  {
    "objectID": "ch20.html#qa-how-do-i-improve-my-machine-learning-skills",
    "href": "ch20.html#qa-how-do-i-improve-my-machine-learning-skills",
    "title": "20¬† Going further",
    "section": "20.3 Q&A: How do I improve my Machine Learning skills?",
    "text": "20.3 Q&A: How do I improve my Machine Learning skills?\nIf you want to get better at Machine Learning, my top recommendation is to practice what you‚Äôve learned in this book as much as possible. I recommend choosing different types of problems and datasets so that you can learn how to adapt these skills to all different types of situations. During that process, you are likely to learn about other topics and scikit-learn modules that we didn‚Äôt cover in this book, which will expand your Machine Learning and scikit-learn fluency.\n\n\n\n\n\n\nPractice what you‚Äôve learned:\n\nChoose different types of problems and datasets\nLearn about other topics and modules we didn‚Äôt cover\n\n\n\n\nOne particular area that might benefit you is to study Machine Learning models in more depth. This will help you in a number of ways:\n\nIt will help you to choose which models are worth trying for a given problem.\nIt will help you to efficiently and properly tune those models.\nIt will help you to better interpret the results of those models.\n\nThere are many resources on this topic, but my favorite resource is the book, An Introduction to Statistical Learning. It will help you to gain a practical understanding of many of the most important and widely used Machine Learning models. It‚Äôs available in hardcover, or you can download it as a free PDF. Although initially the book was written for the R language, there is now a second version that uses Python.\n\n\n\n\n\n\nStudy Machine Learning models:\n\nBenefits:\n\nLearn which models are worth trying\nLearn how to tune those models\nLearn how to interpret those models\n\nResource:\n\nAn Introduction to Statistical Learning (free book)",
    "crumbs": [
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Going further</span>"
    ]
  },
  {
    "objectID": "ch20.html#qa-how-do-i-learn-deep-learning",
    "href": "ch20.html#qa-how-do-i-learn-deep-learning",
    "title": "20¬† Going further",
    "section": "20.4 Q&A: How do I learn Deep Learning?",
    "text": "20.4 Q&A: How do I learn Deep Learning?\nAs I mentioned at the beginning of this book, there are some specialized problems for which a Deep Learning library will provide results that are superior to what you could achieve with scikit-learn. As such, you may decide that you‚Äôd like to spend your time learning how to use Deep Learning.\nDeep Learning is a fast-moving field, but one course that has stayed up-to-date and is taught in an approachable way is Practical Deep Learning for Coders. It‚Äôs available for free and is complemented by a book, a deep learning library called fastai, and a huge online community.\nAlthough Deep Learning has a higher learning curve than Machine Learning, it is based on many of the same fundamental principles that you‚Äôve learned in this book, and so you‚Äôre well-positioned to begin learning Deep Learning.\n\n\n\n\n\n\nLearning Deep Learning:\n\nWhy?\n\nDeep Learning will provide superior results for some specialized problems\n\nHow?\n\nPractical Deep Learning for Coders (free course)\nHigher learning curve than Machine Learning, but based on many of the same principles",
    "crumbs": [
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Going further</span>"
    ]
  }
]